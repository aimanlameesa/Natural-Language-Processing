## PPT: Pre-trained Prompt Tuning for Few-shot Learning

Paper link: https://arxiv.org/pdf/2109.04332.pdf

| Aim | Gu et al. proposed pre-trained prompts by adding soft prompts during the pre-training stage in order to achieve a better initialization for downstream tasks. | 
| ------- | --- | 
| Background | Prompts for pre-trained language models (PLMs) has been proved very effective since they worked as the connection between pre-training tasks and various downstream tasks. However, prompt tuning freezes PLMs and tunes soft prompts only, thus it is able to provide an efficient solution for adapting large-scale PLMs to downstream tasks. Though prompt tuning performs very well with sufficient downstream data, it is much worse under few-shot learning settings which may hinder the performance of prompt tuning in different applications. In this paper, Gu et al. proposed pre-trained prompt tuning (PPT) to address this limitation. | 
| Datasets | They mostly experimented with English and Chinese datasets, where they presented the results of the T5 model (from small size to XXL) for full-model tuning (FT) and the results of PPT and other baselines under prompt tuning (PT). To be more spefic, they used SST-2, RACE-m, BoolQ, CB datasets for English tasks and CCPM, C<sup>3</sup>, LCQMC etc datasets for Chinese tasks. | 
| Methods | For this work, they pre-trained prompts and used the pre-trained prompts for specific tasks. Given an input sentence and its label, a pattern mapping is first applied to convert the input into a new sequence, whereas the new sequence adds some prompt tokens as hints and preserves the masked tokens at the same time so that the PLMs can predict tokens at the masked positions. Then a verbalizer is used to map the true labels to some label tokens. The classification task can be represented by a pattern-verbalizer pair of new sequences and the label tokens. <br> Moreover, they assumed that the downstream tasks can be divided into some groups, whereas each set in a group can be represented as a pattern-verbalizer pair. After pre-training soft prompts on these tasks with fixed model parameters, some pre-trained prompts were obtained and soft prompts initialization was used to optimize the whole process for each task. <br> Then the typical classification tasks are grouped into three formats, i.e, sentence-pair classification, multiple-choice classification, and single-text classification to ensure the generalization of pre-trained prompts. In addition, multiple-choice classification has been found to be more general among these formats and all classification tasks can be unified to this format. |  
| Results and Findings| In their research, they tested two variants of PPT: (1) Hybrid PPT, where carefully designed hard prompts are combined with pre-trained soft prompt and (2) Unified PPT, that takes unified tasks in the multiple-choice classification format. <br> However, the major findings from their experiments are: <br> 1. Since larger models achieve better overall performance under the few-shot setting, PT was studied on the large-scale pre-trained model. The experimental results showed that CPM-2 was able to outperform mT5-XXL across all tasks on Chinese datasets though they share the same parameter scale. <br> 2. PPT was capable of performing better than Vanilla PT and LM Adaption on most datasets significantly. Though it performed worse than Hybrid PT on BoolQ, using Hybrid PPT by combining PPT and hard prompts could outperform all baselines. Experimenting on RACE-m, LCQMC and C3 datasets also provided similar observation. <br> 3. PPT could outperform FT on all Chinese datasets and most English datasets, indicating the existing gap between masked language modeling and downstream tasks. Based on this observation, they intended to extend the proposed method in future. <br> 4. PPT resulted in lower variances across all the datasets with the help of additional hard prompts or further pre-training with language modeling whereas Vanilla PT or Hybrid PT increased the variances for some datasets which is not different from random guesses. <br> Moreover, they tested Unified PPT on datasets with more than 5 labels, whereas it outperformed FT and PT baselines by a large margin. Futhermore, they observed that PPT was able to speed up the convergence of Vanilla PT on RACE-m and CB datasets though it converged a bit slower than FT. | 
| Limitations | There is still an existing gap between masked language modeling and downstream tasks though prompt pre-training was capable to bridge this gap to some extent. Moreover, PPT converges slower than FT. Therefore, how to further accelerate the convergence needs to explored in future according to this research. |  
| Future Work | The significant future research directions based on this paper are: (1) Designing unified task formats with their corresponding pre-training objectives for different tasks such as, language generation or relation extraction (2) Evaluating the performance of few-shot learning techniques considering other parameter tuning approaches and adapting unified task pre-training to them (3) Studying the performance unified task pre-training on pre-trained language models apart the soft-prompt based approaches | 
