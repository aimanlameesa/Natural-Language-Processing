## 

Paper link: 

| Aim | Zhou et al. presented an entity-aware model for abstractive multi-document summarization using a novel decoding mechanism that aims to deal with saliency and redundancy issues explicitly. | 
| ------- | --- | 
| Background | Previous research efforts had highlighted the importance of modeling cross-document relations in multi-document summarization. Those relations were also proved to be effective in identifying the salient and redundant information, which can further help us to process summary generation from long documents. However, previous research in this domain did not focus on modeling the underlying semantic information across multiple documents explicitly. Since entity mentions convey rich semantic and contribute towards the coherence and conciseness of the text, they can be very significant in summary generation process. Considering entities as the indicator of saliency and reducing redundancy, Zhou et al. proposed an entity-aware abstractive multi-document summarization model to emphasize the relations of the entities across multiple documents. | 
| Datasets | The authors experimented on two major datasets, namely WikiSum (Liu et al., 2018) and MultiNews (Fabbri et al., 2019). For the WikiSum dataset, each paragraph has 70.1 tokens and target sumamry has 139.4 tokens on average. After performing entity cluster extraction on the top-20 WikiSum dataset, each instance has 23.7 clusters and each cluster has 10.2 tokens on average. For the MultiNews dataset, each source article contains 2.8 paragraphs and 21.6 sentences on average. Focusing on sentence level rather than paragraph level, this work used 13.3 clusters for each instance and each cluster has 9.9 tokens on average. | 
| Methods | Firstly, entity clusters are used as more advanced semantic units and the co-reference resolution tool is applied from AllenNLP (Gardner et al., 2018) to extract entity clusters. Secondly, a source document cluster was divided into smaller semantic units, such as paragraphs and sentences, depending on the characteristics of datasets and a heterogeneous graph was used to model the important relationships between entity clusters and paragraphs. Futhermore, they utilized a paragraph encoder to encode contextual information within each paragraph, multi-head pooling to obtain fixed-length paragraph representations, entity cluster encoder to get entity clustersâ€™ representation without shared parameters between two encoders and graph encoder to update the representations of semantic nodes. 

 |  
| Results and Findings|  | 
| Limitations | The limitation of this research is that the model heavily relies on the quality of entity annotations and may not generalize well to new domains with limited annotated data. |  
| Future Work | The future research direction includes exploring other approaches, i.e, reinforcement learning based methods to further improve the summary generation process for multiple documents. and also applying the proposed method to different tasks such as multi-document question answering.

