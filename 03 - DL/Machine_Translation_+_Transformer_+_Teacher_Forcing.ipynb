{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aADpsQl9Rqa",
        "outputId": "17027fcb-e0a5-41f1-82b2-5e32a85ff8f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.25.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.26.14)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Installing collected packages: portalocker, torchdata\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wxtqfzB_Axo",
        "outputId": "df88aa8a-469f-4003-a267-e1c166afdc74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-07 14:58:03.316887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-07 14:58:04.959051: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 14:58:04.959199: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-07 14:58:04.959224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from de-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.22.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (23.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.10.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.1.2)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1tlyp-d8-i1",
        "outputId": "624eec03-22ad-432b-dc6f-b09fb480aa97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch, torchdata, torchtext\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random, math, time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dogyLOrv9L9P",
        "outputId": "b0f45374-26af-4c4e-aa21-894968ea4007"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "htoeDGERFv70",
        "outputId": "07ff78d8-50a7-40df-edbe-b06f64039ca6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchtext.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7CrDAsDf9MLr",
        "outputId": "3cfc0ac4-e036-4ab2-f344-885c2e8e67d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.14.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ETL: Loading the Dataset"
      ],
      "metadata": {
        "id": "XRkCou129iRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this if you are not using our department puffer\n",
        "# import os\n",
        "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
        "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "SRC_LANGUAGE = 'en'\n",
        "TRG_LANGUAGE = 'de'\n",
        "\n",
        "train = Multi30k(split=('train'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))"
      ],
      "metadata": {
        "id": "BnyL7mJC9pVK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzsHyMVM9prT",
        "outputId": "c231a636-8d9d-4bfc-c882-e926bf1667c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardingFilterIterDataPipe"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EDA - Simple Investigation"
      ],
      "metadata": {
        "id": "gINkT5i_-Eav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look at one example of train\n",
        "sample = next(iter(train))\n",
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u_8i8QN9y-b",
        "outputId": "73319a2b-61ef-40af-db73-bded697d4c06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Two young, White males are outside near many bushes.',\n",
              " 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(list(iter(train)))\n",
        "train_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbIshbmZ90c_",
        "outputId": "a69cf13e-4a5d-425d-ed26-c7a90d6fd1f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29001"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = train.random_split(total_length=train_size, weights = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}, seed=999)"
      ],
      "metadata": {
        "id": "2lYtU1y690OT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(list(iter(train)))\n",
        "train_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRe4uOR-PDA",
        "outputId": "f9cbb3aa-d5ed-464f-f458-60246d654d0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20301"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = len(list(iter(val)))\n",
        "val_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfEZ7TLr-O9H",
        "outputId": "461d7622-b146-484e-9e5b-ffeaa4bbe53f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5800"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = len(list(iter(test)))\n",
        "test_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q6aJDW4-WbT",
        "outputId": "03f503d6-b515-4215-9100-9a99fa1edc5d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2900"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocessing"
      ],
      "metadata": {
        "id": "HDwbaoqR-aOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing"
      ],
      "metadata": {
        "id": "YLd2y8ph-ee6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "metadata": {
        "id": "nFFj0X4J-itX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')"
      ],
      "metadata": {
        "id": "H77SIPUW-kH_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of tokenization of the english part\n",
        "print(\"Sentence: \", sample[0])\n",
        "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD-50liO-kwa",
        "outputId": "8d876106-80f5-4a5b-8158-134fe98ec818"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  Two young, White males are outside near many bushes.\n",
            "Tokenization:  ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to yield list of tokens\n",
        "# here data can be `train` or `val` or `test`\n",
        "def yield_tokens(data, language):\n",
        "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data:\n",
        "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
      ],
      "metadata": {
        "id": "_Z3O6bbd-km8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining special symbols and indices\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# making sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
      ],
      "metadata": {
        "id": "H-dzZDNu-qLs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text to Integers (Numericalization)"
      ],
      "metadata": {
        "id": "3XlbgvV4AWb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
        "    # creating torchtext's Vocab object \n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
        "                                                    min_freq=2,   # if not, everything will be treated as UNK\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True) # indicates whether to insert symbols at the beginning or at the end                                            \n",
        "# Setting UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "4HOgftfJASvy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking some example\n",
        "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmMlyXvqAS4L",
        "outputId": "3d5978ce-f79e-42fb-dcf2-199a25f12b3a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1891, 10, 4, 0, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can reverse it....\n",
        "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
        "\n",
        "# printing 1816, for example\n",
        "mapping[1891]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "52R-h83iAotT",
        "outputId": "9027e697-4dd8-447d-f5a0-e2d83563d21f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'here'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try unknown vocab\n",
        "mapping[0]\n",
        "# they will all map to <unk> which has 0 as integer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mCDvMTvvAokR",
        "outputId": "06fec3c3-46c2-4972-9444-45a429768779"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<unk>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try special symbols\n",
        "mapping[1], mapping[2], mapping[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdBp3UoTAofC",
        "outputId": "41b88a6a-c2d3-46fc-ba9e-7de04d63dc27"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<pad>', '<sos>', '<eos>')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking unique vocabularies\n",
        "len(mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv2mi-u6Aoaf",
        "outputId": "cacc9493-1350-45d2-fc38-2fcdd00c9408"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5174"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preparing the Dataloader"
      ],
      "metadata": {
        "id": "w_JVbgtaCnjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids):\n",
        "    return torch.cat((torch.tensor([SOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and trg language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], # Tokenization\n",
        "                                               vocab_transform[ln], # Numericalization\n",
        "                                               tensor_transform) # Adding BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_batch(batch):\n",
        "    src_batch, src_len_batch, trg_batch = [], [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
        "        src_batch.append(processed_text)\n",
        "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
        "        src_len_batch.append(processed_text.size(0))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
        "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
      ],
      "metadata": {
        "id": "mFbY9pS4A84I"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(val, batch_size=batch_size,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader  = DataLoader(test, batch_size=batch_size,\n",
        "                             shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "a11tCxC6A8yq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for en, _, de in train_loader:\n",
        "    break"
      ],
      "metadata": {
        "id": "WoupDwptA8s_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
        "print(\"German shape: \", de.shape)   # (batch_size, seq len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lp3QjmyA8nX",
        "outputId": "1b3340b4-8048-4606-f992-ae4c6a0690ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English shape:  torch.Size([64, 27])\n",
            "German shape:  torch.Size([64, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Designing the Model"
      ],
      "metadata": {
        "id": "5QyN2eUEC-pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "EBPIu2pRDE_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, \n",
        "                 pf_dim, dropout, device, max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        # src = [batch size, src len]\n",
        "        # src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        # src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        # src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "metadata": {
        "id": "A1h_Xh6pDLsi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "1sGk0OsADewx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim,  \n",
        "                 dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        # src = [batch size, src len, hid dim]\n",
        "        # src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        # self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        # dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        # src = [batch size, src len, hid dim]\n",
        "        \n",
        "        # positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        # dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        # src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "metadata": {
        "id": "KQth9RqBDLkf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mutli-head Attention Layer"
      ],
      "metadata": {
        "id": "_V3JLqUvDosU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        # query = [batch size, query len, hid dim]\n",
        "        # key = [batch size, key len, hid dim]\n",
        "        # value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        # Q = [batch size, query len, hid dim]\n",
        "        # K = [batch size, key len, hid dim]\n",
        "        # V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # Q = [batch size, n heads, query len, head dim]\n",
        "        # K = [batch size, n heads, key len, head dim]\n",
        "        # V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        # energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "        # attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        # x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        # x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        # x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        # x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "mkfSH1ZdDLOq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feedforward Layer"
      ],
      "metadata": {
        "id": "Z2qU4WzDEH9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        # x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        # x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "cs1DLnYgEJYw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "q9nXZ5FrESsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
        "                 pf_dim, dropout, device,max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        # trg = [batch size, trg len]\n",
        "        # enc_src = [batch size, src len, hid dim]\n",
        "        # trg_mask = [batch size, 1, trg len, trg len]\n",
        "        # src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
        "        # pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        # trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        # trg = [batch size, trg len, hid dim]\n",
        "        # attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        # output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "pVUsxihCEUOQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer"
      ],
      "metadata": {
        "id": "Re2P7Fa7EdIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        # trg = [batch size, trg len, hid dim]\n",
        "        # enc_src = [batch size, src len, hid dim]\n",
        "        # trg_mask = [batch size, 1, trg len, trg len]\n",
        "        # src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        # self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        # trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        # encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))            \n",
        "        # trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        # positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        # dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        # trg = [batch size, trg len, hid dim]\n",
        "        # attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "metadata": {
        "id": "Bi7JMrJ-Eeib"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting them together (become Seq2Seq!)"
      ],
      "metadata": {
        "id": "ASRWCqbOEu8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        # src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        # trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        # trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        # trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        # src = [batch size, src len]\n",
        "        # trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        # src_mask = [batch size, 1, 1, src len]\n",
        "        # trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        # enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        # output = [batch size, trg len, output dim]\n",
        "        # attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention   "
      ],
      "metadata": {
        "id": "KVfKswMSE9UP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training"
      ],
      "metadata": {
        "id": "udvTyogyFWpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "metadata": {
        "id": "Oxqe8ib0E9PG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
        "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "SRC_PAD_IDX = PAD_IDX\n",
        "TRG_PAD_IDX = PAD_IDX"
      ],
      "metadata": {
        "id": "XfZbjKv6Fbla"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
        "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
        "hid_dim = 256\n",
        "enc_layers = 3\n",
        "dec_layers = 3\n",
        "enc_heads = 8\n",
        "dec_heads = 8\n",
        "enc_pf_dim = 512\n",
        "dec_pf_dim = 512\n",
        "enc_dropout = 0.1\n",
        "dec_dropout = 0.1\n",
        "\n",
        "SRC_PAD_IDX = PAD_IDX\n",
        "TRG_PAD_IDX = PAD_IDX\n",
        "\n",
        "enc = Encoder(input_dim, \n",
        "              hid_dim, \n",
        "              enc_layers, \n",
        "              enc_heads, \n",
        "              enc_pf_dim, \n",
        "              enc_dropout, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(output_dim, \n",
        "              hid_dim, \n",
        "              dec_layers, \n",
        "              dec_heads, \n",
        "              dec_pf_dim, \n",
        "              enc_dropout, \n",
        "              device)\n",
        "\n",
        "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP8WqqrHFbgD",
        "outputId": "2a9449a2-24e9-474c-b5df-707eeac2f25a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqTransformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(5174, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(6433, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=6433, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can print the complexity by the number of parameters\n",
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(f'{item:>6}')\n",
        "    print(f'______\\n{sum(params):>6}')\n",
        "    \n",
        "count_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLxyQbr5lHkH",
        "outputId": "f4fe76d2-1d65-4a1a-bc90-12256dcd4fbb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1324544\n",
            " 25600\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "1646848\n",
            " 25600\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            " 65536\n",
            "   256\n",
            "131072\n",
            "   512\n",
            "131072\n",
            "   256\n",
            "1646848\n",
            "  6433\n",
            "______\n",
            "8629537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0005\n",
        "\n",
        "# training hyperparameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) # combining softmax with cross entropy"
      ],
      "metadata": {
        "id": "UmBBWJx6lNE9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for src, src_len, trg in loader:\n",
        "        \n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        # output = [batch size, trg len - 1, output dim]\n",
        "        # trg    = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.reshape(-1, output_dim)\n",
        "        trg = trg[:,1:].reshape(-1) # trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
        "                \n",
        "        # output = [batch size * trg len - 1, output dim]\n",
        "        # trg    = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / loader_length"
      ],
      "metadata": {
        "id": "_yOil2talM4K"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion, loader_length):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for src, src_len, trg in loader:\n",
        "        \n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            # output = [batch size, trg len - 1, output dim]\n",
        "            # trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            # output = [batch size * trg len - 1, output dim]\n",
        "            # trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / loader_length"
      ],
      "metadata": {
        "id": "SDuUYP3ylbV2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting everything together"
      ],
      "metadata": {
        "id": "18y8v6v6lrk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_length = len(list(iter(train_loader)))\n",
        "val_loader_length   = len(list(iter(valid_loader)))\n",
        "test_loader_length  = len(list(iter(test_loader)))"
      ],
      "metadata": {
        "id": "zEQgvAS_lojC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "bb2PKy63lpHh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "num_epochs = 10\n",
        "clip       = 1\n",
        "\n",
        "save_path = f'{model.__class__.__name__}.pt'\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
        "    \n",
        "    # for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    \n",
        "    # lower perplexity is better"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b56ikuL_m06j",
        "outputId": "9fb8ab27-a706-4949-9884-241099fad24f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 25s\n",
            "\tTrain Loss: 4.150 | Train PPL:  63.402\n",
            "\t Val. Loss: 3.080 |  Val. PPL:  21.764\n",
            "Epoch: 02 | Time: 0m 22s\n",
            "\tTrain Loss: 2.865 | Train PPL:  17.554\n",
            "\t Val. Loss: 2.360 |  Val. PPL:  10.588\n",
            "Epoch: 03 | Time: 0m 21s\n",
            "\tTrain Loss: 2.305 | Train PPL:  10.019\n",
            "\t Val. Loss: 1.896 |  Val. PPL:   6.658\n",
            "Epoch: 04 | Time: 0m 21s\n",
            "\tTrain Loss: 1.956 | Train PPL:   7.074\n",
            "\t Val. Loss: 1.585 |  Val. PPL:   4.878\n",
            "Epoch: 05 | Time: 0m 21s\n",
            "\tTrain Loss: 1.702 | Train PPL:   5.485\n",
            "\t Val. Loss: 1.385 |  Val. PPL:   3.993\n",
            "Epoch: 06 | Time: 0m 20s\n",
            "\tTrain Loss: 1.521 | Train PPL:   4.577\n",
            "\t Val. Loss: 1.204 |  Val. PPL:   3.333\n",
            "Epoch: 07 | Time: 0m 21s\n",
            "\tTrain Loss: 1.368 | Train PPL:   3.926\n",
            "\t Val. Loss: 1.054 |  Val. PPL:   2.868\n",
            "Epoch: 08 | Time: 0m 21s\n",
            "\tTrain Loss: 1.246 | Train PPL:   3.477\n",
            "\t Val. Loss: 0.943 |  Val. PPL:   2.567\n",
            "Epoch: 09 | Time: 0m 21s\n",
            "\tTrain Loss: 1.141 | Train PPL:   3.129\n",
            "\t Val. Loss: 0.850 |  Val. PPL:   2.339\n",
            "Epoch: 10 | Time: 0m 21s\n",
            "\tTrain Loss: 1.055 | Train PPL:   2.873\n",
            "\t Val. Loss: 0.776 |  Val. PPL:   2.173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_losses, label = 'train loss')\n",
        "ax.plot(valid_losses, label = 'valid loss')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "1BIccFsim0wi",
        "outputId": "05009f0e-26c8-46fd-c132-459239aa5dd4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(save_path))\n",
        "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI5Bs5ZhoW_I",
        "outputId": "da4b5e71-9e20-4231-d4fc-9e2093266a33"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 0.773 | Test PPL:   2.165 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Testing on Some Random News"
      ],
      "metadata": {
        "id": "mpoGLSixoboz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QAHe9kgOohbl",
        "outputId": "d13f7850-86ce-4bac-803b-c8739c7e7ebf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two young, White males are outside near many bushes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Jdl7_Cfgoh3J",
        "outputId": "63e319d8-87c9-49b2-f169-9759626add4a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
        "src_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYq5xGQnorVv",
        "outputId": "1abf1a33-5d3b-4bf0-fc42-639ec6f30cac"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,   19,   25,   15, 1069,  842,   17,   56,   84,  331, 1623,    5,\n",
              "           3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
        "trg_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXHoJZCxov5G",
        "outputId": "53a6d6a6-5f28-451a-f65d-8797271533ca"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,   21,   83,  262,   32,   89,   22,   91,    7,   16,  115,    0,\n",
              "        2893,    4,    3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_text = src_text.reshape(1, -1)  # because batch_size is 1"
      ],
      "metadata": {
        "id": "YBz49EK-ox5s"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trg_text = trg_text.reshape(1, -1)"
      ],
      "metadata": {
        "id": "UD4jglVzoxzn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_text.shape, trg_text.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07auIuIeo4kT",
        "outputId": "70330ee2-13b5-4b36-f947-f02d42addf43"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 13]), torch.Size([1, 15]))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
      ],
      "metadata": {
        "id": "gwoASsRNo4S3"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(save_path))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output, attentions = model(src_text, trg_text) # turning off teacher forcing"
      ],
      "metadata": {
        "id": "HiOAb1qfpyrE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape # batch_size, trg_len, trg_output_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fdVeNZBpye2",
        "outputId": "3893eeec-ce06-45f1-c44f-820244d302b8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 15, 6433])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.squeeze(0)"
      ],
      "metadata": {
        "id": "fCiJW5c0qJ8H"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNZ7WBn1qJ0_",
        "outputId": "43abaef6-4bbc-44b3-8178-d0e5fe2272b9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 6433])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = output[1:]\n",
        "output.shape # trg_len, trg_output_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cSwCAnhqYPG",
        "outputId": "52422bb0-4f5d-4a0b-c32a-514d42381314"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14, 6433])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_max = output.argmax(1) # returns max indices"
      ],
      "metadata": {
        "id": "_TTsVOZmqfPH"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_max"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUAt8ijAqhav",
        "outputId": "74c7c65c-5f56-47a6-b2d7-acbdf70087b4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 83,  32,  32,   9,  22,  91,   7,  16, 115,  24,   4,   4,   3,   4],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
      ],
      "metadata": {
        "id": "TP4CC9jjqhNF"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in output_max:\n",
        "    print(mapping[token.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHaUdzmaqph3",
        "outputId": "07dff8da-c6c1-4ec3-d07c-6958e121cb9f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junge\n",
            "Männer\n",
            "Männer\n",
            ",\n",
            "im\n",
            "Freien\n",
            "in\n",
            "der\n",
            "Nähe\n",
            "von\n",
            ".\n",
            ".\n",
            "<eos>\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Attention"
      ],
      "metadata": {
        "id": "AzyZvgydrgN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attentions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6jcao42rko8",
        "outputId": "bc5f51a5-3b82-47da-ce22-ee4949547976"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 15, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = attentions[0, 0, :, :]\n",
        "attention.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_8iFPkKrmLa",
        "outputId": "d51af65b-7ac6-4549-e86e-a491c3538c45"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
        "src_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfSo9aCorl7b",
        "outputId": "45fe2dbb-caf6-43c1-d585-0e6dc9f2869d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>',\n",
              " 'Two',\n",
              " 'young',\n",
              " ',',\n",
              " 'White',\n",
              " 'males',\n",
              " 'are',\n",
              " 'outside',\n",
              " 'near',\n",
              " 'many',\n",
              " 'bushes',\n",
              " '.',\n",
              " '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
        "trg_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNROqiJGrvlU",
        "outputId": "c9df50f4-b695-4525-e5ec-27d664d3417b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>',\n",
              " 'junge',\n",
              " 'Männer',\n",
              " 'Männer',\n",
              " ',',\n",
              " 'im',\n",
              " 'Freien',\n",
              " 'in',\n",
              " 'der',\n",
              " 'Nähe',\n",
              " 'von',\n",
              " '.',\n",
              " '.',\n",
              " '<eos>',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    ax.tick_params(labelsize=10)\n",
        "    \n",
        "    y_ticks =  [''] + translation\n",
        "    x_ticks =  [''] + sentence \n",
        "     \n",
        "    ax.set_xticklabels(x_ticks, rotation=45)\n",
        "    ax.set_yticklabels(y_ticks)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "JuXOOFgyrvhg"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_attention(src_tokens, trg_tokens, attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "Ppqu6Yo2r3nv",
        "outputId": "a49f0729-8c6c-4284-d422-e1e80a74c63a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-b23710e830bc>:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(x_ticks, rotation=45)\n",
            "<ipython-input-70-b23710e830bc>:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(y_ticks)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAJZCAYAAAD8otwQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3ZUlEQVR4nO3debxdVX338c83CZMgoII4MVQfBxAFClrF2uI8iwoITlVUUpyrtVZbq/jUqShttdpqtM6tIg7VKlpaKw5FZRDBuVatIo8VEEGgTAm/54+1A+cmNyRZuffuk9zP+/XKK+fufe7ZvzPcs797rbXXTlUhSZLUY8nYBUiSpM2XQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKStB5JcmM/S4uZQUKSbkSS1DAFcJLdkmxfTgk89Qx7Cyf+PUjS7NYIEX8E3BPYHvgH4BNVdfmY9Wl2a7xvTwOuqqoPjVvVlssWCUlah4md0WOAB1fVEcC2wP0NEdNrjfB3DPDNcSuaXnPRcmOQ6LT6xbf5TNryJPmtJC+fWLQj8J4kLwSuAZ413O92Y9Sn9RvemwdU1X2Anyd5xBrv6aKWZAncELo2xbJNL2dxSbKkqq6jhbBVtKbOr41b1fRIcl/gYVX1J2PXMs0mm16tYypdCRyR5Nqq+gvgAuDPgF8Dj6yqlUleDNw9yTOq6toxi9Wsn+UrgNsm+Ufad/UlwMOSLKuq40YocapU1XVJbkHbhz0F+FFVdQUtg8RGGl78PYFnJ7kD8IAk/6eqfjl2bWNL8rvA0cDvJdm1qo4Zu6ZpseaX3ETT62g78jX6kY8BfgycWlUrx6hnGqx+Tarq3CT/F3hNkl8B7wCeCPwUeGSSm9O+fJ9oiGiS7FlVPxlp25Of5ccCl9HC34OBpwEfr6rvJTkc+K0hTCzmz/ndgFsBrwVOAu4OnNL7eHZtbIQkRyV5PvBB4LvAj4AP0z60i1qSewDvAf4GOJD2x/r2UYuaEmt8yR2R5PeTPCjJzcdsDZio6bm0pvr/XsxfrjDjNXkxcATwVeCFwLHDv0uBRwC/TQsR3x6p1KkyHNm+LMkdx9j+xPv2AuBFwF7APwG7V9XrhhDxfOAVwHsW6+c8yZIkzwTeDdwPeD3wMVqjwhm9j2uLxAZIsgy4B/CnwGuAFwNfB74E/ElVXTNiedNia+Dfq+osgCT3An6cZGVVPWfc0sY18SX3IuBRwCeAVwN/BYw6kjzJbYEjgaOAnyQ5DNgJOKuqzhmztoWUZHfgoqq6cnhNjgIOBX4O7Au8izby/w3D/betqqtGK3j6BLglcBDwg1EKaGMi7l9V903yStoAy7OSbAdsQ9txLurwN7So/4AWkn9WVdcOAestm/K62CKxYbaqqq8AB1fVh6rqNGA/4JSq+teRa5sWlwB7JNkLoKr+F3gjrZ/5VSPWNaqJQbk3B/arqvsBBfwKOCnJtkm2Wuh6JlwMnAu8EngfcDjwMODghappbBPBYdlw0HAprX/9ymE81LeBjwN/OXzpAlw9SrFTJskeSW5RVRcBbwGOTfJ/Fmjbs+2/LkvyWlqr6FFVtQo4DNgKeHxVfWshaptGSZ6c5JCq+kJV/XgIEUuA36W1rnczSKzH8MXxriRbVdVkF8YrgEV9+tcwCvqvh6bxHwGfAd6X5NFJnkD7gD4b2Gshd5bTIMmu0Fojkty9qi4Grkvyz8CDaAP2VtF23AvSHLxGF8tDkxwK7Az8I61/9BVV9QRaS9vBSZYuRF1jq6rzgbcBtwd+fzit82zg40m2Ht6nX9C67T41/M5oXVJJ9h9r25OGz/gLgX9J8ju01+hk2mdqXTv6OTOEPJLsMfz8M1o383JaiLgyyVOBFwBLF/NYlrTTYJ8NXLjGqpcAVNXJm/L4dm3ciGEH+QTgqUN627aqrkqyH20gzxvGrXA8SfYG/i/wUdqO8L3AU2mvy/2AuwJ/DNwauA2wFFhMf8j3H8LUZ4BHJXkSrb/9qcBLh1H/T6X9IT9sIQtL8mzgycAnae/b/avqvcO6pwLPBI4cdqBbrMlgVVWXJbk7sH+SJwN/AryO1jT+WdpR7QOq6sfjVQxJngEcluQJVXXpiHXsVFUXAi8cPjOH0ybqOhS4d5LD5uvzk+Q3gdtW1T8neR5wdJLvAG+ndRdeC3w+yeeAhwK/V1X/Mx+1bA6S3B54aFUdnGSHJA8E9qyqvwf+hTZGYpMGfhsk1mE4gr49bcTvNkmOBZ4xjOT+DG2066oxR92PJcnBwMuA46vqxCS70VL/e4HnVNUHkmwDHELr3jhysfQnJ9kXOH94Xf4I+GvgTlV1aZKvALcA/iLJD4H9ac2tP53nmnavqvOG1pG9aS0iD6SdhfB14BvD/fagnQr2hKr67nzWNLY1WmeeRBv/8P6h5+dewLKqekGSh9L6198xBSHiPrQd9nOHz9PSMcJekkfSAsRlwMuq6r1JPgbsBqwc/j8E+Nxcfz8OrWT7AM8cDuj2o70mRw//f5422PJQWvfU31fVD+dq+5uplcDuSf6aNv4ptIObJVX1jqE7b5Na2Zwi+0Yk+VPah/N8WpPmjrTR2kcNYwAWspZZ/yBzw7wWC1XHMlorw78DX66qo4flu9JaIO4KPJY2DuABtDMBvrNQ9Y1pGH/w58Df0saMHEN7DXYB7ltVq4Yvwj1pn6ULh2b1+azpFrSBgp+rqjcn2Zb2Rbs7LSg/cmhtew5tjMTVi2nwcJI/pA08e8bqwWZJHkcLW+cC76uqK0YscXUXwda0boSjaIH9r4ZguKAHMkkOBI6nDTw/nPY5eitw+tBaexPgpcBlqwemzuG2MzznnYGH0M6i+UZVvXD4u/oD4Ddo302frqpFPY4lbU6fS4DzaGexHAWcWFVnp00bvifwf+fk81NV/pv4R0u2rwJeTpsKd0/g5sO6+wH/Btxs5PpeSRv1v+uwbMkCbXufYdu3AG5HG4T2son1u9KOvkd/H0f+DN0T+Dtgt+Hnk4Azh9uPBg5bwFq2pZ2ueBLwrGHZ+2gj2rcbfj4SOId2qtzor988vx5LJm7fjnYEG1qwexRtnMjq1+R4YOcpqHnn4f+ltNN03wQcOrE+C1THbWnXGPnAxLKX0U6Hvx+w7bDsD2mtttvMVW2TjzN8zyyldcF9F3jMxLqX01pBdxj7fRv5M/M82umc7wa+MPmdM3yGvgXsPVfbs2tjQpLlwNNpo4/vS9tR3qeqLk47r/yJwNOq6ldTUN+9gTOS3Kuq/meBjkx2oCX+Y2lHIY8APjGMHXlltT7TNQfzbPFmaRX6FnBz4NVJXlpVRyT5RJIzaUeWRy5ATasnVroqyb/TZvZ7TpKLgefSxkf8zdAFdVfgSVV13nzXNaahdWZ74KdJHgCcRmv2PYU20dTVwH5Jdq6qFyX5TFX9eryKYWgpetgwBuA/aOMAngsckmSbqvrwAvzdr7YSOB14QpIjquqkqnpd2llZz6V1kV1FOwr+45rDFoHVzzFtivJH00Lfh4btPXP4vH+8ql6dNj/Loh0In+Q3gN+jvUa/pE1d8KYk/0s7YDiC1qo+d92XYyenafjHDV08fws8YmL5a2lfMqHtwO88Ul3rq2+bea7jrtxw9HoQLUS8AtgOuAPwfeAOY7+PI312tpu4/SDgfsPtrWgTdL2ToQUL+B3aILEF+dwMt7dlOAqnDer8NPC4Yfnv0AYR7jXya/gg2umm+8zzdu5N6+b5K9pcB9vQ+vNfufrzCzwSeOPYn6uhluW0o8k70U4//TrwzGHdS2mTCc37kffwN7//6u8/4PdpM31OHuXO+9//8B38VYaWM1or0o60HeMXJ78bF/M/WnfTKWsseyk3tEhuP9fb9PTPZvLCO/tO3D4BOK+ad1XV9xeyqBreddrgL4DraIOLJuv7ybB8Xgx9ns8DViTZrqrOpPXRPozW9HsxbX6ERTegKcldgPcmuWmSJ9IGVr4uyRtoLTfH0ELo3yXZraq+WPM8JgJmHL09n9a0+Y4kD6iqz9Bas55KGxPwxar6aFX993zXtC7DgNQ/ofW3/0WSe8/DNgJQbS6Yy2gtai+rqqur6hdV9aqq+uHwer2a9vkeVZKb0k7PfwxtPMB2tGt9HJPk6Kp6PfAXNc9H3kkewXBmD/ChJL9bVW+ntUwcluTxAAv0978z7eBp72Fsy+nAM2hN+G+ljWlZtJLcKe1smvOAC5KcNLF69UEfwJyP71v0QSLJs4BXDl827wZekuSZwwCnh9E+tDuv/jJaoJqWDP8vHZqe3zt0a5xAGy399In67kprrp0X1QaVnkBrIntzku2r6nRaS8jtaEfbi+KMjElp11l5DG1k+LuBI6rqrrQjWmhfcHvRdlq/ZIH/1oYm8cfRdtK3AP4+yeOGMPEu4D5JbraQn+tZarwjbZK3+9GuqLkK+NowIHTOTASr/Winu72Edj2Y+60esZ5kF2Bv4ClVNfolp6vqsqp6G+2I+2G0puhPAxcBjx92GPPaxZrkzrRTvFc3kW9NuwLqQ6vqHbT5Rub97J4kj0w7o+j7tLkQXgT8jPbZPhi4pqpOrC28a+7GDH/vK4BXJHkH7Xtn2ySfT3IcbQD8O2B+5kBZ1GMkkhxNay47bHhxz0g7tfG9tLMz9qXNIXHJAta0R91wOuBdqurbaRMHvYw2gOkhtH7S36ZdaGVe6kvykGEb29IGDr6PNqfGh5O8jdYc/YdVtUkzom3GtqIdpZ1Fa35+Q5I7VdV/Jnkr8BzaKbF/XQswRfjkOI0hfK6e0e9ptBarlwBvTHJdVf1Tkn+rEc9GSHJP2pk9lyY5AbgL8LhqU/g+IMlXaw4vhJd2Lv0xtLOI3pjkKtrf1K/TJlPaiXZa5ahzZ6TNXfMbwM1oXRe/oO3Ab5V22uUlwAtqnueQGD5DV9DmYdibtvP+LdpAvROTHFVVfzefNUzUcV/aGIwnMAyOraorkjyI1oy/qE89TPIwWoveY2lzn9xiaKl6VNocHytpBzrz16I+Vj/OmP9oR4fLaGMODqN9GF9Iuxz4C4b77MhwVsQC1vVIWt/tHsABtB3A62l/QM+ihQZoV23bab7qo31h/Jh2xH0SbZT4Q2hHtifQLlT2qLHfx5E+O78B7DTcvh/tSO0JwHHDa7XXsO4OtGbyWy5wfYfSrnh4X9oYgFO54eyef6OdrTHqiPbhs/QV2oDUt9H6/m81rDsGOBPYZR62ezit++kFw8/LgQ/QmsgPmILP1rOG9+j2tID6N8Py19IuQHUOrRtxvut4LPDPwJuHz9HDaIMnoZ1C+Pe0ScwW6nXZZfj7+vTE39ezh8/N3cZ+30b8vKwe+3R/2jiRY2ktxVsPy+/BAp3RsyjnkUiyb1V9K8mjaAPivkRLul+nXZTrqKr6fwtc00Nog8AOraofDMveSrsmA7SQ8Uval+C8zRefdon0ZwKrquq4YdlLgHtV1eOGn7evdkSwqCbjSnIn4ERaE+sfVDtb5mjaVODvon3p7gv8aVX9KAtwqeLJ9yDJUbQd5Xto81f8Le1o8gO0U1L3ofWr/2I+a7oxabNG/hHtc3xqksfQXre70QbSPY42gdmcXFhp6MO/bVX91fDzY2hB61xaU+92tMmnLpmL7XXWuHp+hFfS+vqfSts5PA64tlorzXa0HcR8t0TsTPv8nEg7S+s5tIObXwD/TQs7h1ebi2CuJ5uabFU7Eti/ql42/Hxz2sHePrQQcRvg0lq8LaIkuWu1Fut70LrsflhV9xjWPRO4D/C8WoAzWBbdGIkkT6cNHLxJVf0zrY/tiVX1Jloz/jIW+BoaSR5M6zr4Hm2K3tPTrkb4ZVoafwWtFWA/2oC0eemSSpuh8lm0L47bZLj4TlUdD9ws7Rr21NAkvphCxOBHDKcE096Hh9MGLv2E1hT9HtoX7p8N79G8NpOvESL2oDXx3qeqXkoLpS+k9W8fTmvGf8+YIWLwBVoL4OpBev9EO/J9H+21fcymhIhMXN9h2Pn+DHjq8MW6ensX0lo+fh+4YswQMbjj8Hm5PfAR2pHkodVOn3xO2qy6Vy1AiPgtWuvaWVX1wWrjIF491HV7WrfKC6vqbJj7v/+JELErreXsyWlTYFPtWjX/RmsRPAE4Z5GHiKfTBlFvX1Vn0CYDvDbJYWmDhp8F/OVChAhYRGMkJtLurWhHZf8LUEO/0TBY5Vja+fQLdu542vnsb6F96d+KNoDxAlqXxsdp4eEPq+q1Sb4M/Ggej3Ivoh3B3om20zwkyU60vtJbAKOeUz+WYSe9bbXxD39AO4tlCW2HeEfaEfUdaUcFfzbcdyFbIp4PPAm4Ke0KledX1T8muZx2hHs28IaaedG5BTX0/e9Lm2Pj4bQLPf2kqv6iqn5CC2ObbGJndCztbKef0XY8L02yqqreTQvstwY+MnYYHl6XF9Dm9fgxreXxQ9WuxfI02g7h0Pmucxgb9m7gv4BbDt81X66qj6RdLuDPgI9X1S/noSXiYGCPqvrQEByW0y7+dTotlF9XVW+ltUKcAry5FnA232myxn7s9XXDOKf30s6gO5TWiv2UWsAZhRdNkBiaB29PGyT4L6uXp03ecR7wHdqAlO8tcGm/pk1ydVqSfWiTFX2e1qy4He3o6YVJ/qWqvjgfBaRdRnmHqvr+8If8ItoEPbcf6tkKOG74wl9UkmxP+xLdOsnHqw1U/BFtIpxTaF9ue9P6ji+pqmfTTjGcVxMh4jG08/yfQuuSuhtwryRfrqpPpp0Bcc7IIeLZtD7cJ9G6FN5GGwT6lrRTio+b4+0dRhuc92TaTLC3Aj4LHD+0Ih1Am3Pggrnc7sZK8mjagOmH0rpbdqQFij9Ou2bLAbRuhB/Mcx2/RZvN9/Cq+maSP6d1q1yX5LSq+mCSL9Yw+HUeQs3NaKdN35U2tugxw///TZvF8thhcO79aBefWtBu52myrv0YbW6Nj9IuorjgFsUYiSShhaY302az+0faKPHX0Y6E3jj2TnJ10kw75eqJtIGWX6Jd5+PtwJOrXSZ3rre7Pa35cj/aTHFfofVBvq+qvpI2G+BNquq8xTYmYrUkt6L1WR9PG0PzX7TWq9dW1VlDl9By4INV9V8LWNdtae/Xv1bVM4bQ8Ke08+0/CXx+vltG1ifJjsBf0sLYEbTZUH8JXEnbub+e1pT/q7n6bCX5E9opgW9MsjUttOxL+/u/M/DtGnHuDJjx3v1bVT097eyE1QO/d6QNcL56vrszhloeTGsBeElV/eXQAvHyoZb3V9XnF6CGB9G6486pqicNr8ftaWfVnQL8J228yKINEevZj/2U9v103hjf0YtijEQ119Kafm8DfI725XIOre949POPVzfVDV0t/zAsPox2fv395yNEDNu7gvYavJg2QO/RtD71N6ddNfKXNZyfvRhDBEBV/U9V/SPttXkc7UhxGXBCkjsM4w5es5AhYqjrfNqFih6Wdlnpq2hHltfSzozYeiHrmc3QTfgc4JbAY6vqIbSWicNp43/2r6qL5/iz9R3gvkn2qaprqmoF7ci/qurTY4cImPHePTTtVMqraUH+Qtr38jULESKGWk6hfdc8I8kTh+/KPwf+h9bNuhA1/CstBD88yZHVJgv7Lq2bdceq+sliDhGw3v3YHwM/G+s7etF0bQxH+kfQZho8nnYUd+24Vc1u6Is/kbbTumK++wOHHdDX0ya92ob2RbY/bbzGZtMSkWSr+XxPq+rraWdpPITW9/4o4NFJ3sRI57JX1ceSXE1rGmZohn4JbaKwBb1C7bpU1dVp8/wvGwbs7klrjTh5nrpcTqV19zwpyam0LsIdmLIxPrO8dx9K8h7aFMYL2hVVVZ9Ici3w50m2rqr30CZ8WugankI7iNmbdu2O2w//i+ndjy2Kro3VhmbWlZNfsNO8k5zvHeN6tv2nwJ5VtXyM7W+sYbT+PwJfq+FUv3nc1la0M3xOAE6oBZ46fTZpk9KsAF5UVSet7/4LbWiq/gPggbSjqSPmczBYktvQgvijaWdhvaqqzpmv7W2KiffuhVX1kZFreTStu+mBwC9qhAm6hnE/HwU+RXtNNruzM4Ywds08PfbU7ccWVZDQ+q3+QKbNSXA07XS8K8eua0MMR7vvAn6/qr4+dj0Lbehn/uG0fvEOAexWwHW1ANccGbZ5E4aZEBdie72m6b1Lsmu1K/mOWcPvAj+Zhm6ojTWMVXoXcHTN4RVQp5lBQmsZBvU8EvhxzePkV/Mhyf1pA7bmbHplSdoYGSbtG7uOhWKQ0BZl7CY+SVpsDBKSJKnbojj9U5IkzQ+DhCRJ6maQWIdhToWpMW31wPTVZD03btrqgemryXpunPWs37TVtBD1GCTWbao+DExfPTB9NVnPjZu2emD6arKeG2c96zdtNRkkJEnS9NriztpIMlVP6MADD5yTx7nwwgvZdddd5+SxzjrrrDl5HEnS4lFVmW25QWKerVy14DPMrteypUvHLkGStJlZV5Cwa0OSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElStzkLEkm2TrL9HD3W9km2movHkiRJ82eTg0SSvZOcAHwfuNOw7PVJvpPk3CRvHJbtleTfh2WfS7LHsPyIJN9Kck6SLw4PeyfgP5O8Mcnem1qjJEmaH6mqjf+l1vLweOAZw6J3Ax+uqsuS3AI4DbhLVVWSnavqkiT/DHykqt6b5OnAo6vqMUm+CTy0qs5ffd9hGzcFjgSOBgr4+2EbV6ynto1/QvNo5apVY5ewlmVLl45dgiRpM1NVmW15b5D4NXAu8Myq+t4a65YBZw3/PgV8qqquSXIRcOuqunbotvh5Ve2S5G3AHYAPAx+rql/Osr29aUFi36racZb1y4Hlw48HbvQTmkcGCUnSlmBdQaK3a+Nw4HzgY0lekWTPiQ2tBO4JfAR4JPDZ9RR2LPByYHfgrKFFA7i+O+SVwMeB84btzvYYK6rqoKo6qPP5SJKkDl0tEtf/ctvpP5nW/XAR8Mzh/5tU1QVJdgJ+VFW3SPJJ4KSqen+SpwGHVtVjk9yhqn44PN4ZwDHAJcA7gV1o3SYfmK2lYh012bWxHrZISJI21px2bcz6QMk9gZ8DK4FPANsCAd44jIvYkxYKdgEuBI6uqp8m+Rhwx+G+nwP+ALgdrRvk9I46DBLrYZCQJG2seQ8S08IgsX4GCUnSxprrMRKSJEkGCUmS1M8gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSt2VjF7Clm8YLZE3bhdqSWa8DI0naDNgiIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG5zGiSSnDaXjydJkqZbqmrsGuZUki3rCc2DaXvPk4xdgiRpPapq1i/ruW6RuDzJIUk+NbHsLUmeNtz+7ySvSvL1JN9Mcpdh+a5J/jXJt5O8M8lPkuwyrHtyktOTfCPJ25MsncuaJUlSvzHGSFxUVb8J/B3w4mHZK4F/r6q7Ah8B9gBIsjdwJHCfqtofWAU8ac0HTLI8yZlJzlyA+iVJ0mDZCNv82PD/WcDjhtu/DTwWoKo+m+RXw/IHAAcCZwzN39sBF6z5gFW1AlgBdm1IkrSQ5iNIrGRmS8e2a6y/evh/1QZsP8B7q+plc1SbJEmaQ/PRtfETYJ8k2yTZmdaqsD7/ATweIMmDgZsNyz8HHJ7klsO6myfZc+5LliRJPea6RaKq6rwkHwa+BfwYOHsDfu9VwAeTPAX4CvA/wGVVdVGSlwOnJFkCXAs8hxZWJEnSyObs9M8ktwC+XlUb3WKQZBtgVVWtTHJv4O+GwZU9dThGYj08/VOStLHWdfrnnLRIJLkNcCrwxs6H2AP48NDqcA1wzFzUJUmS5pcTUi1C0/ae2yIhSdNvQSakkiRJi4tBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqdt8XEZcU+4z55wzdgkzbLP1dmOXMMOOO+06dgkzXHvt1WOXsJZLLvnF2CVImhK2SEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKnbeoNEkkrygYmflyW5MMmnJpadnGTnJKfNV6GSJGn6LNuA+1wB7Jtku6q6EngQcP7kHarq4cPNg+e4vrUkWVZVK+d7O5Ikaf02tGvjZOARw+0nAB9cvSLJvZJ8JcnZSU5Lcudh+dOSfCzJZ5P8IMnxE79zeZLXJDknyVeT7DYs3zXJR5OcMfy7z7D8uCTvT/IfwPvn4HlLkqQ5sKFB4kPAUUm2Be4OfG1i3XeB+1bVAcArgNdOrNsfOBK4G3Bkkt2H5dsDX62q/YAvAscMy98E/FVV3QM4DHjnxGPtAzywqp6wZnFJlic5M8mZG/h8JEnSHNiQrg2q6twke9FaI05eY/WOwHuS3BEoYKuJdZ+rqksBknwH2BM4D7gGWD3G4ixadwnAA4F9klz/2El2GG5/cuhama2+FcCKYTu1Ic9JkiRtug0KEoNPAm8EDgFuMbH8z4HPV9Vjh7Bx6sS6qydur5rY3rVVVbMsXwLcq6qumtzwECyu2IhaJUnSAtiY0z/fBbyqqr65xvKduGHw5dM2sZ5TgOet/iHJ/pv4eJIkaR5tcJCoqp9V1ZtnWXU88LokZ7NxLRyzeT5wUJJzh66QYzfx8SRJ0jzKDT0MWwbHSKzfyd/4xtglzPDYe9577BJm2HGnXccuYYZrr716/XdaYJdc8ouxS5C0wKoqsy13ZktJktTNICFJkroZJCRJUjeDhCRJ6maQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkroZJCRJUjeDhCRJ6uZFuxah7bffaewSZnjWS14zdgkzvOnVfzh2CTM89rAXjF3CWj78oePHLkHa4i1ZsnTsEq533XWrvGiXJEmaewYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd3WGySSVJIPTPy8LMmFST41sezkJDsnOW2+CpUkSdNn2Qbc5wpg3yTbVdWVwIOA8yfvUFUPH24ePMf1rSXJsqpaOd/bkSRJ67ehXRsnA48Ybj8B+ODqFUnuleQrSc5OclqSOw/Ln5bkY0k+m+QHSY6f+J3Lk7wmyTlJvppkt2H5rkk+muSM4d99huXHJXl/kv8A3j8Hz1uSJM2BDQ0SHwKOSrItcHfgaxPrvgvct6oOAF4BvHZi3f7AkcDdgCOT7D4s3x74alXtB3wROGZY/ibgr6rqHsBhwDsnHmsf4IFV9YQNrFmSJM2zDenaoKrOTbIXrTXi5DVW7wi8J8kdgQK2mlj3uaq6FCDJd4A9gfOAa4DVYyzOonWXADwQ2CfJ9Y+dZIfh9ieHrpW1JFkOLN+Q5yJJkubOBgWJwSeBNwKHALeYWP7nwOer6rFD2Dh1Yt3VE7dXTWzv2qqqWZYvAe5VVVdNbngIFlesq7CqWgGsGO5b67qfJEmaWxtz+ue7gFdV1TfXWL4TNwy+fNom1nMK8LzVPyTZfxMfT5IkzaMNDhJV9bOqevMsq44HXpfkbDauhWM2zwcOSnLu0BVy7CY+niRJmkfr3fFX1Q6zLDuVoQujqr4C3Gli9cuH5e8B3jPxO4+c7TGr6iPAR4bbF9EGZ665vePWV6ckSVp4zmwpSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd1SVWPXMKeSbFlPaBH40QW/GLuEGe5++7uMXcIMV1xx6dglaCNVXTd2CdKcq6rMttwWCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSui0bu4C5kGQ5sHzsOiRJWmy2iCBRVSuAFQBJauRyJElaNDa7ro0kn0ty27HrkCRJm1mQSLIE+D/AxWPXIkmSNrMgAewDfLSqrhy7EEmStJmNkaiqbwEvGrsOSZLUbG4tEpIkaYoYJCRJUjeDhCRJ6maQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkroZJCRJUjeDhCRJ6paqGruGOZVky3pCWvSWLp2+S+KsWrVq7BJmWP7814xdwgzvfMufjV3CWp794r8Yu4QZ3nL8H41dwhrcdaxPVWW25bZISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqdtUBIkkp41dgyRJ2nhTESSq6uCxa5AkSRtvKoJEksuH/w9J8oUkn0jyoySvT/KkJKcn+WaSO4xdqyRJusFUBIk17AccC+wNPAW4U1XdE3gn8LzZfiHJ8iRnJjlz4cqUJEnTGCTOqKqfV9XVwA+BU4bl3wT2mu0XqmpFVR1UVQctUI2SJInpDBJXT9y+buLn64BlC1+OJElal2kMEpIkaTNhkJAkSd2moqugqnYY/j8VOHVi+SETt2eskyRJ47NFQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6parGrmFOJdmynpCk9br48svHLmGG2+xyq7FLWMtVV03Xa6TNT1VltuW2SEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndNjlIJFmV5BsT//baiN89bVO3L0mSxrNsDh7jyqraf7YVSQKkqq6bbX1VHTwH25ckSSOZ866NJHsl+X6S9wHfAnZP8kdJzkhybpJXTdz38onba91neKzvJnlHkm8nOSXJdnNdsyRJ6jMXQWK7iW6Njw/L7gj8bVXdFbjz8PM9gf2BA5P8zuQDJHnwjdznjsBbh8e6BDhszQKSLE9yZpIz5+D5SJKkDTTnXRvDGImfVNVXh0UPHv6dPfy8Ay0cfHHiMdZ1n58CP66qbwzLzwL2WrOAqloBrBi2X5v4fCRJ0gaaiyAxmysmbgd4XVW9/UbuP+t9hlBy9cSiVYBdG5IkTYmFOP3zX4CnJ9kBIMltk9yy4z6SJGnKzFeLxPWq6pQkewNfaSdxcDnwZOCCDbjPqvmuT5Ik9UvVljWkwDES0uJz8eWXr/9OC+g2u9xq7BLWctVV0/UaafNTVZltuTNbSpKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVK3Le6iXcuWbVU77HCzscu43u677z12CWv51re+OHYJM9zhDgeMXcIMv/jFf49dwgzXXTd9F8FdufKasUuYYdq+x3bb7TfGLmEt22yz3dglzPDDH549dgkzbLfdTccuYS3Llm01dgnXu+KKS1m1aqUX7ZIkSXPLICFJkroZJCRJUjeDhCRJ6maQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkroZJCRJUjeDhCRJ6maQkCRJ3QwSkiSp21QGiSSnjV2DJElav6kMElV18Ng1SJKk9ZvKIJHk8uH/Q5KcmuQjSb6X5B+SZOz6JElSs2zsAjbAAcBdgf8H/AdwH+DLk3dIshxY3m5PZTaSJGmLtDnsdU+vqp9V1XXAN4C91rxDVa2oqoOq6qAlSzaHpyRJ0pZhc9jrXj1xexWbRyuKJEmLwuYQJCRJ0pQySEiSpG5T2U1QVTsM/58KnDqx/LkjlSRJkmZhi4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdUtVjV3DnEqyZT0hSeu1dOl0XX/wzB/+19glrOUh9/zdsUuY4cILzxu7hBmmc184XTVVVWZbbouEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1G0qgkSS45K8eOw6JEnSxpmKILExkiwbuwZJktSMFiSS/GmS/0zyZeDOw7I7JPlskrOSfCnJXYbl70nytiRfA44fq2ZJkjTTKEf3SQ4EjgL2H2r4OnAWsAI4tqp+kOS3gL8F7j/82u2Ag6tq1cJXLEmSZjNWN8F9gY9X1f8CJPkksC1wMHBSktX322bid05aV4hIshxYPn/lSpKk2UzTeIMlwCVVtf861l+xrl+sqhW01gyS1NyXJkmSZjPWGIkvAo9Jsl2SmwKPAv4X+HGSIwDS7DdSfZIkaQOMEiSq6uvAicA5wGeAM4ZVTwKekeQc4NvAoWPUJ0mSNkyqtqyeALs2pMVn6dJp6qWFM3/4X2OXsJaH3PN3xy5hhgsvPG/sEmaYzn3hdNVUVZlt+WY3j4QkSZoeBglJktTNICFJkroZJCRJUjeDhCRJ6maQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkroZJCRJUjcv2iVJc2zVddeNXcJali7xuFGbxot2SZKkOWeQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkroZJCRJUjeDhCRJ6maQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkrrNeZBIUklOmPj5xUmOm/j5tCQ7Jzl5YtlxSV4817VIkqT5NR8tElcDj0uyy2wrq+rgqrqkqh4+D9uWJEkLaD6CxEpgBfDCNVckOTTJ15KcneRfk+w2sXqfJKcm+VGS50/8zpOTnJ7kG0nenmTpPNQsSZI6zNcYibcCT0qy0xrLvwjcq6oOAE4EXjKx7i7AQ4B7Aq9MslWSvYEjgftU1f7AKuBJa24syfIkZyY5c+6fiiRJWpdl8/GgVfXrJO8Dng9cObHqdsCJSW4NbA38eGLdp6vqauDqJBcAuwEPAA4EzkgCsB1wwSzbW0FrBSFJzf0zkiRJs5nPszb+GngGsP3Esr8B3lJVdwN+H9h2Yt3VE7dX0UJOgPdW1f7DvztX1XHzWLMkSdoI8xYkqupi4MO0MLHaTsD5w+2nbsDDfA44PMktAZLcPMmec1qoJEnqNt/zSJwATJ69cRxwUpKzgIvW98tV9R3g5cApSc4F/hW49TzUKUmSOqRqyxpS4BgJSWNbdd11Y5ewlqVLnH9Qm6aqMttyP1mSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1G3Z2AXMtSVLlrL99juNXcb1Lr/8krFLWMs229xk7BJmuOqqy8cuYartttteY5ewliuvnK737Ne/Xu/FhBfUNF4g6+nHvmrsEmb4+Il/N3YJM0zb9yLA1Vf/79glXO/G/sam79MuSZI2GwYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdVu2kBtL8nrgvKp66/DzccAVwC2BhwEFvLqqTkxyCHAccBGwL3AW8OSqqoWsWZIkrdtCt0icCDx+4ufHAxcA+wP7AQ8E3pDk1sP6A4A/APYBbg/cZ7YHTbI8yZlJzjRnSJK0cBY0SFTV2cAtk9wmyX7Ar2gh4oNVtaqqfgF8AbjH8CunV9XPquo64BvAXut43BVVdVBVHZRkvp+GJEkaLGjXxuAk4HDgVrQWit+4kftePXF7FePUK0mS1mGMwZYnAkfRwsRJwJeAI5MsTbIr8DvA6SPUJUmSNtKCH+FX1beT3BQ4v6p+nuTjwL2Bc2iDLV9SVf+T5C4LXZskSdo4o3QVVNXdJm4X8EfDv8n7nAqcOvHzcxeoPEmStIGcR0KSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuo1y0a75tGTJUrbddoexy7jeZZddPHYJa7nqqsvHLmGGZcu2HruEGVauvGbsEmbYaaddxy5hLdetWjV2CdpIJ77/L8cuYYa73e2QsUuY4eKL/9/YJazl8sun51j/8st/tc5101OlJEna7BgkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSui0bu4C5kGQ5sBxgyZKlI1cjSdLisUW0SFTViqo6qKoOMkhIkrRwtoggIUmSxmGQkCRJ3TarIJHk5CS3GbsOSZLUbFaDLavq4WPXIEmSbrBZtUhIkqTpYpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHXbrC7atWEKqsYuQhth5cprxy5hqp1//n+OXcJadtnldmOXMEN+OV3HRFXXjV3CWlZee83YJcxw7bVXjV3CDHe5y73GLmEtp576wbFLuN6qVavWuW66/vokSdJmxSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqZtBQpIkdTNISJKkbgYJSZLUzSAhSZK6LRu7gLmQZDmwHGDJkqUjVyNJ0uKxRbRIVNWKqjqoqg5asmSLeEqSJG0W3OtKkqRum1WQSHJyktuMXYckSWo2qzESVfXwsWuQJEk32KxaJCRJ0nQxSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuqWqxq5hTi1duqy2226Hscu43oe+9PmxS1jLow88aOwSZli2bKuxS5hh5cprxy5hhiRjl7CWafveqLpu7BLWMH3v2bR9jqbtPdtqq23GLmEtW2+97dglXO/KKy9n1aqVs36IbJGQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqNidBIsnWSbafi8caHm+nJIYcSZKm3CbtrJPsneQE4PvAnYZlByb5QpKzkvxLklsPy/dP8tUk5yb5eJKbDcufn+Q7w/IPDQ/928D3kxyXZI9NqVGSJM2fjQ4SSbZPcnSSLwPvAL4D3L2qzk6yFfA3wOFVdSDwLuA1w6++D/jjqro78E3glcPylwIHDMuPBaiqTwP3Bi4FPpnks0mOSLL1OmpanuTMJGdWXbexT0mSJHVKVW3cLyS/Bs4FnllV31tj3b7AacCPhkVLgZ8DRwDfrKo9hvvdATipqn4zyWeBy4F/Av6pqi6fZZv3poWSa4fAsU5Lly6r7bbbYaOe03z60Jc+P3YJa3n0gQeNXcIMy5ZtNXYJM6xcee3YJcyQZOwS1rKx3xvzbfoOIKbvPZu2z9G0vWdbbbXN2CWsZeuttx27hOtdeeXlrFq1ctYPUU/XxuHA+cDHkrwiyZ4T6wJ8u6r2H/7draoevJ7HewTwVuA3gTOSLLv+wZJ9kryB1prxH8AxHfVKkqR5stFBoqpOqaojgfvSuh4+keTfkuxFGyux69CCQJKtkty1qi4FfpXkvsPDPAX4wjCgcveq+jzwx8BOwA5JfjPJV4F3At+jdX08s6q+tmlPV5IkzaVl67/L7Krql8CbgDcluSewqqquSXI48OYkOw2P/9fAt4GnAm9LchNa18fRtK6PDwz3DfDmqrokyZXA0VX13U14bpIkaZ51B4lJVXX6xO1vAL8zy32+Adxrll//7Vnua4CQJGkz4FwNkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkroZJCRJUjeDhCRJ6maQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktQtVTV2DXNq6623rV133X3sMq53wQU/HbuEtaxcee3YJaxhy/oMzrVttrnJ2CWsZeuttx27hBkuu+zisUuYevvtd7+xS5jhwgvPG7uEGQ444IFjl7CWr3zlE2OXcL1LL72IlSuvyWzrbJGQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndDBKSJKmbQUKSJHUzSEiSpG4GCUmS1M0gIUmSuhkkJElSN4OEJEnqZpCQJEndlo1dwFxIshxYDrB06RbxlCRJ2ixsES0SVbWiqg6qqoOWLFk6djmSJC0aW0SQkCRJ4zBISJKkbptVkEhycpLbjF2HJElqNquRiVX18LFrkCRJN9isWiQkSdJ0MUhIkqRuBglJktTNICFJkroZJCRJUjeDhCRJ6maQkCRJ3QwSkiSpm0FCkiR1M0hIkqRuBglJktTNICFJkrqlqsauYU4luRD4yRw81C7ARXPwOHNl2uqB6avJem7ctNUD01eT9dw461m/aatprurZs6p2nW3FFhck5kqSM6vqoLHrWG3a6oHpq8l6bty01QPTV5P13DjrWb9pq2kh6rFrQ5IkdTNISJKkbgaJdVsxdgFrmLZ6YPpqsp4bN231wPTVZD03znrWb9pqmvd6HCMhSZK62SIhSZK6GSQkSVI3g4QkSepmkJAkSd0MEpIkqdv/B5qe/aIzbIGnAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}