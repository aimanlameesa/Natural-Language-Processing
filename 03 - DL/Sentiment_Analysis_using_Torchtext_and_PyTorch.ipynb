{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cPmKTuAUMx",
        "outputId": "543e330c-9978-4582-f01a-538f1594b2aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.13.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchdata) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (4.0.0)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1 urllib3-1.26.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (0.14.1)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchtext) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbn8fH3z_mn3",
        "outputId": "8ea36b35-eeee-46b1-8523-a55fcbe9407e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch, torchdata, torchtext\n",
        "from torch import nn\n",
        "\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# reproducibility \n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1VoqS-f8AGDZ",
        "outputId": "1302ada8-4806-478b-f49e-1f8096536d21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchtext.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "H1RxJG3SAHCx",
        "outputId": "8d6eaae0-45a0-4a49-f113-81bc6b7e902b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.14.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchdata.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0u3oJirKAh26",
        "outputId": "083699fe-1042-439c-9826-4334585d1a39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.5.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "XNO4HVGLAhxX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Loading the Dataset"
      ],
      "metadata": {
        "id": "6kXHI2DuA19A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for puffer\n",
        "# import os\n",
        "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
        "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
        "\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train, test = AG_NEWS()"
      ],
      "metadata": {
        "id": "FooIHfjpA48u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train  # a new object by torchdata.....streaming data (yield ....)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnNn7HADA8Uv",
        "outputId": "7440e41c-fb49-48f3-98ef-eae1c022d3a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardingFilterIterDataPipe"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EDA - Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "BNpYqZB2BB78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train))  # generator\n",
        "# (“World”, “Sports”, “Business”, “Sci/Tech”)\n",
        "#  1,        2,        3,          4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgJfU2YxBG8l",
        "outputId": "dc00e38b-5604-49df-be04-688064aec9b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(iter(train))[100]  # generator\n",
        "# (“World”, “Sports”, “Business”, “Sci/Tech”)\n",
        "#  1,        2,        3,          4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UsJPO8ZBHWU",
        "outputId": "cc32a5df-b092-4a9c-8f17-fcc040174ca7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/datapipes/iter/combining.py:262: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " 'Comets, Asteroids and Planets around a Nearby Star (SPACE.com) SPACE.com - A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest.')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set([y for y, x in list(iter(train))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ_u3tT6BHQp",
        "outputId": "ac709e5c-4482-40bd-dad6-12b86b9897bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1, 2, 3, 4}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(list(iter(train)))\n",
        "train_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyHv-BKWBHJd",
        "outputId": "e0cfbefc-57a5-4729-fc33-718709654562"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1L6EsdgDzAM",
        "outputId": "66213b61-e353-412a-d8b5-57dea7b62afc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardingFilterIterDataPipe"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data\n",
        "too_much, train, valid = train.random_split(total_length=train_size, \n",
        "                                            weights = {\"too_much\": 0.7, \n",
        "                                                       \"smaller_train\": 0.2,\n",
        "                                                       \"valid\": 0.1},\n",
        "                                            seed = SEED)"
      ],
      "metadata": {
        "id": "Gnn2Vb-gDzb-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(list(iter(train)))\n",
        "val_size   = len(list(iter(valid)))\n",
        "test_size  = len(list(iter(test)))"
      ],
      "metadata": {
        "id": "W7jO8Kd9D5U8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size, val_size, test_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1PzLDVUD5Pq",
        "outputId": "cead00ba-10cc-4e6a-ca85-350aababfde8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 12000, 7600)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocessing"
      ],
      "metadata": {
        "id": "3nsYQ-NLEKL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 3.1 Tokenizing\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# checking whether the tokenizer works.....\n",
        "tokens    = tokenizer(\"Chaky likes deep learning very much and wants his student to be number 1 in Asia....\")\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKxcGbl4D5Kd",
        "outputId": "66abc74f-521a-49b1-c5e1-6602c629a8e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chaky',\n",
              " 'likes',\n",
              " 'deep',\n",
              " 'learning',\n",
              " 'very',\n",
              " 'much',\n",
              " 'and',\n",
              " 'wants',\n",
              " 'his',\n",
              " 'student',\n",
              " 'to',\n",
              " 'be',\n",
              " 'number',\n",
              " '1',\n",
              " 'in',\n",
              " 'Asia',\n",
              " '....']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXU5NTWREEM8",
        "outputId": "c298d255-978b-4429-d217-3d1270079dac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " 'Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3.2 Numericalization\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(data_iter):  # data_iter, e.g., train\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "        \n",
        "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>',\n",
        "                                                                 '<bos>', '<eos>'])"
      ],
      "metadata": {
        "id": "h99D5S07ED46"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.set_default_index(vocab[\"<unk>\"]) # if we don't the id of this word, we can set it unk"
      ],
      "metadata": {
        "id": "IdReeZwgEDuq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(['Chaky', 'wants', 'his', 'student', 'to', 'be', 'number', '1', '.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7m9uFosFJRb",
        "outputId": "ffdec990-a99b-4b43-cf8a-e02ed4fdfb72"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 944, 38, 3956, 8, 43, 498, 109, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = vocab.get_itos()"
      ],
      "metadata": {
        "id": "9TPgnTPgFJMJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HFfty_AJFJFV",
        "outputId": "e58644c6-79c1-4974-fbe8-0e8943ea03c3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<unk>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(['<pad>', '<bos>', '<eos>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIYoDv8rFUDm",
        "outputId": "74461b17-f04f-47f6-f0a0-5c7161db80cb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)  # 52k unique words....."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NKoswtvFT-8",
        "outputId": "174038dd-75d2-49c8-feb6-ab7776e626f9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52686"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. FastText Embedding"
      ],
      "metadata": {
        "id": "yyyVz6XoFbJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import FastText\n",
        "fast_vectors = FastText(language='simple')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojaadlpQFT6f",
        "outputId": "18014d6c-1938-4916-c00a-20760d315263"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/wiki.simple.vec: 293MB [00:57, 5.08MB/s]                           \n",
            "  0%|          | 0/111051 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 111051/111051 [00:14<00:00, 7879.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
      ],
      "metadata": {
        "id": "x-RwxJsmFT1t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast_embedding.shape # (vocab size, 300) == (52k, 300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0d0y1BcFvL6",
        "outputId": "4cc50d87-cc4f-4fc6-f789-bc13f760ee4a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([52686, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looking up the fasttext embedding of id 100\n",
        "fast_embedding[100][:10] #size of 300 dim of this word id 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8g0rhQ6FvHK",
        "outputId": "07ced02f-7600-4c59-818b-430909be2b8d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0935,  0.0915,  0.2640,  0.0387,  0.0843,  0.3809, -0.1776,  0.1745,\n",
              "        -0.0362, -0.0278])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preparing Dataloader"
      ],
      "metadata": {
        "id": "tuU1cruyyAgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1  # 1, 2, 3, 4 ---> 0, 1, 2, 3"
      ],
      "metadata": {
        "id": "DTO1NtcbFvBr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "why padding????\n",
        "\n",
        "in the same batch, e.g., batch size = 2\n",
        "\n",
        "\"chaky eat sushi\", ==> \"chaky\", \"eat\", \"sushi\" ==> 0, 22, 11, 1, 1\n",
        "\"chaky sleep\" ==> \"chaky\", \"sleep\" ==> 0, 99, 1, 1, 1\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "YyiotSsIyGoz",
        "outputId": "9582220a-882c-4c9f-8be1-b49294f45eb6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwhy padding????\\n\\nin the same batch, e.g., batch size = 2\\n\\n\"chaky eat sushi\", ==> \"chaky\", \"eat\", \"sushi\" ==> 0, 22, 11, 1, 1\\n\"chaky sleep\" ==> \"chaky\", \"sleep\" ==> 0, 99, 1, 1, 1\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence # making each batch same length\n",
        "\n",
        "pad_ix = vocab['<pad>']\n",
        "\n",
        "# this function gonna be called by DataLoader\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, length_list = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))  # [3, 1, 0, 2, ]\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) # [0, 44, 21, 2]\n",
        "        text_list.append(processed_text)\n",
        "        length_list.append(processed_text.size(0)) # for padding\n",
        "        \n",
        "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
        "        pad_sequence(text_list, padding_value=pad_ix, batch_first=True), \\\n",
        "        torch.tensor(length_list, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "tuwbQ9E-yGj3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train, batch_size = batch_size, \n",
        "                          shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "val_loader   = DataLoader(valid, batch_size = batch_size,\n",
        "                          shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "test_loader  = DataLoader(test, batch_size = batch_size,\n",
        "                          shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "9bOc5g2KyGew"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for label, text, length in train_loader:\n",
        "#     break\n",
        "\n",
        "# label: [batch size, ]\n",
        "# text : [batch size, longest length of this batch] ==> [batch size, seq len] ==> [b, l]\n",
        "# length:[batch size, ]\n",
        "\n",
        "# label, text, length  #why we need length --> we can later ignore padding...."
      ],
      "metadata": {
        "id": "U3nZhU3qyyM-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Designing the Model"
      ],
      "metadata": {
        "id": "QIBYgQIszEWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, \n",
        "                 dropout):\n",
        "        # input dim = how many vocab you have\n",
        "        # emb dim = 300 --> we use fasttext\n",
        "        # padding_idx tells this lookup table to ignore, and just randomize....\n",
        "        # <unk>, <bos>, <eos>\n",
        "        self.embedding_layer = nn.Embedding(input_dim, emb_dim, padding_idx=pad_ix)\n",
        "        self.lstm            = nn.LSTM(emb_dim,\n",
        "                                       hid_dim,\n",
        "                                       num_layers = num_layers,\n",
        "                                       bidrectional = bidirectional,\n",
        "                                       dropout = dropout,  # dropout is applied between layers....\n",
        "                                       batch_first=True)\n",
        "        \n",
        "        self.fc              = nn.Linear(hid_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, x, lengths):\n",
        "        # x: [batch size, seq len]\n",
        "        \n",
        "        embedded_x = self.embedding_layer(x)\n",
        "        # x: [batch size, seq len, emb dim]\n",
        "        \n",
        "        # packing this embedded_x in such a way that RNN knows to ignore padding....\n",
        "        # without batch_first = True; things will become [seq len, batch size, emb dim]\n",
        "        pack_embedded = nn.utils.rnn.pack_padded_sequence(embedded_x, lengths.to('cpu'),\n",
        "                                                          enforce_sorted=False,\n",
        "                                                          batch_first = True\n",
        "                                                          )\n",
        "        \n",
        "        # packed_outputs is basically all hidden states\n",
        "         #h is the last hidden state\n",
        "        # c is the last cell state\n",
        "        packed_outputs, (h, _) = self.lstm(pack_embedded)\n",
        "        \n",
        "        # h: [num_layers * num_directions, batch_size, hidden dim]\n",
        "        \n",
        "        # it happens that because packed_outputs is all hidden states....some hidden states near the end is\n",
        "        # hidden state for padding, pytorch guys help you\n",
        "        # by using this pad_packed_sequence, then all the hidden states will only be not padding....\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first = True)\n",
        "        # output: [batch size, seq len, direction * hidden sim]\n",
        "        \n",
        "        # last hidden state - concat last forward and backward states\n",
        "        last_hidden_state = torch.cat((h[-1, :, :], h[-2, :, :]), dim = 1)\n",
        "        # last_hidden_state: [batch_size, hidden_dim * 2]\n",
        "        \n",
        "        # for sentiment analysis...\n",
        "        return self.fc(last_hidden_state)  #[batch_size, output_dim]==> [batch_size, 4]        "
      ],
      "metadata": {
        "id": "QtYB2g-AyyEY"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}