{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3 \n",
        "\n",
        "Everything should be done ON MY code, no new code.\n",
        "\n",
        "\n",
        "1. Read https://aclanthology.org/D14-1082.pdf and maybe just write one paragraph summary in your README.md in your github\n",
        "\n",
        "2. Do something called ablation study (meaning try to delete something so we know the impact of that deleted thing - very common in NLP)\n",
        "Recall that we have 18 word + 18 pos + 12 dep features\n",
        "Try to delete only the 12 dep features and check UAS\n",
        "Try to delete only the 18 pos features and check UAS\n",
        "3. Do another comparison study testing the embedding\n",
        "Chaky uses some embedding\n",
        "Try to use (1) glove embedding (smallest), (2) nn.Embedding (train from scratch) and compare with Chaky's embedding\n",
        "4. Do some testing, compare 2-3 sentences with spaCy and see whether our neural network gives the same dependency.\n",
        "\n",
        "Criteria:\n",
        "\n",
        "0: not done\n",
        "\n",
        "1: ok\n",
        "\n",
        "2: with comments/explanation like how Chaky does his tutorial"
      ],
      "metadata": {
        "id": "LziqOpBMMDz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "\n",
        "The existing dependency parsers suffered from a number of problems, like, having millions of poorly estimated feature weights, relying on manually designed incomplete feature templates, and consuming most of the runtime during the feature extraction step. Later, Chen et al. proposed using dense features instead of the sparse indicator features to address all these problems. They used a neural network classifier to make parsing decisions in a transition-based dependency parser, whereas the network is capable of learning dense vector representations of different words, part-of-speech (POS) tags,dependency labels and modeling their interactions through a novel cube activation function. The classifier can achieve acceptable improvements in both parsing accuracy and speed by learning a small number of dense features on two languages (English and Chinese) and two different dependency representations (CoNLL and Stanford dependencies). It is also noteworthy that, the parser is able to parse more than 1000 sentences per second with 92.2% unlabeled attachment score on English Penn Treebank."
      ],
      "metadata": {
        "id": "Kx7oPe_rMaA1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9x1tnXPG-TLE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm  #gimmick for progressbar when you train\n",
        "import pickle # saving and loading models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing Function"
      ],
      "metadata": {
        "id": "zNobZ5iQP23I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['He', 'has', 'good', 'control', '.']\n",
        "something = sentence.pop(0)"
      ],
      "metadata": {
        "id": "n2BNBDsh-rx5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "something"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "72WVv25C-w9O",
        "outputId": "41617ac5-7024-41d4-b061-8eaa82b41055"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOomWBJB-0BB",
        "outputId": "51ea7147-f35d-466c-c352-6673dd7622f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['has', 'good', 'control', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basically, it takes the current state of the buffer, stack, dependencies\n",
        "# tells us how SHIFT, LA, RA changes these three objects\n",
        "\n",
        "class Parsing(object):\n",
        "    \n",
        "    # init stack, buffer, dep\n",
        "    def __init__(self, sentence):  \n",
        "        self.sentence = sentence      # ['The', 'cat', 'sat]  #conll format which is already in the tokenized form\n",
        "        self.stack    = ['ROOT']\n",
        "        self.buffer   = sentence[:]  # in the beginning, everything is inside the buffer\n",
        "        self.dep      = []           # maintains a list of tuples of dep\n",
        "    \n",
        "    # parse function that tells me how shift, la, ra changes these three objects\n",
        "    def parse_step(self, transition):     # transition could be either S, LA, RA\n",
        "        if transition == 'S':\n",
        "            # getting the top guy in the buffer and put in stack\n",
        "            head = self.buffer.pop(0)\n",
        "            self.stack.append(head)\n",
        "        elif transition == 'LA':  # stack = [ROOT, He, has] ==> append to dep (has, he) and then He is gone from the stack [ROOT, has]\n",
        "            dependent = self.stack.pop(-2)  # He\n",
        "            self.dep.append((self.stack[-1], dependent))  # (has, he)\n",
        "        elif transition == 'RA':\n",
        "            dependent = self.stack.pop()  # stack = [ROOT, has, control] ==> dep (has, control), control will be gone fromt he stack [ROOT, has]\n",
        "            self.dep.append((self.stack[-1], dependent))\n",
        "        else:\n",
        "            print(f\"Bad transition: {transition}\")\n",
        "    \n",
        "    # given some series of transition, it gonna for-loop the parse function\n",
        "    def parse(self, transitions):\n",
        "        for t in transitions:\n",
        "            self.parse_step(t)\n",
        "        return self.dep\n",
        "    \n",
        "    # checking whether things are finished - no need to do anymore functions....\n",
        "    def is_completed(self):\n",
        "        return (len(self.buffer) == 0) and (len(self.stack) == 1)  # so buffer is empty and ROOT is the only guy in stack"
      ],
      "metadata": {
        "id": "94D3aQ6q-2Qo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parse Step"
      ],
      "metadata": {
        "id": "VohyeO_AQ-Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a instance of Parsing\n",
        "parsing = Parsing(['He', 'has', 'good', 'control', '.'])"
      ],
      "metadata": {
        "id": "ULSkk9G0BOrK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRRebgdPRITY",
        "outputId": "d8328bcd-2a6e-4183-b5ac-2bb8a43c7119"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT'], ['He', 'has', 'good', 'control', '.'], [])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trying to do a shift\n",
        "parsing.parse_step(\"S\")\n",
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riYqxx8PRIEm",
        "outputId": "a32ccce6-4970-46af-fd60-9a25cf3b0560"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT', 'He'], ['has', 'good', 'control', '.'], [])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doing shift, and then left arc\n",
        "parsing.parse_step(\"S\")\n",
        "parsing.parse_step(\"LA\")\n",
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dERzS4hUyES",
        "outputId": "e078bc29-83c5-4337-c74a-5e2a61d052c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT', 'has'], ['good', 'control', '.'], [('has', 'He')])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parse"
      ],
      "metadata": {
        "id": "vOdIMqhbV4e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsing = Parsing(['He', 'has', 'good', 'control', '.'])\n",
        "\n",
        "# using the parse function, to tell it to do S, S, L, S, S, L, R\n",
        "# double checking whether we have three dep (has, he), (control, good), (has, control)\n",
        "\n",
        "parsing = Parsing([\"He\", \"has\", \"good\", \"control\",\".\"])\n",
        "parsing.parse([\"S\",\"S\", \"LA\", \"S\", \"S\", \"LA\", \"RA\"])\n",
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjOvvrHqUx4g",
        "outputId": "641ccc9d-8304-4165-ed1e-2d856fdf9a61"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT', 'has'],\n",
              " ['.'],\n",
              " [('has', 'He'), ('control', 'good'), ('has', 'control')])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minibatch Parsing"
      ],
      "metadata": {
        "id": "4AtGFN4JWpx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    dep = []  # all the resulting dep\n",
        "    \n",
        "    # init Parsing instance for each sentence in the batch\n",
        "    partial_parses = [Parsing(sentence) for sentence in sentences]  # in tokenized form\n",
        "    # Parsing(['The', 'cat', 'sat']), Parsing(['Chaky', 'is', 'mad'])\n",
        "    \n",
        "    unfinished_parses = partial_parses[:]\n",
        "    \n",
        "    # while we still have sentence\n",
        "    while unfinished_parses:  # if there are still a Parsing object\n",
        "    \n",
        "        # taking a certain batch of sentence\n",
        "        minibatch = unfinished_parses[:batch_size] # number of Parsing object\n",
        "        \n",
        "        # creating a dummy model to tell us what's the next transition for each sentence\n",
        "        transitions = model.predict(minibatch) \n",
        "        # transitions = [S, S, .....]\n",
        "        # minibatch   = [Parsing(sentence1), Parsing(sentence2)]\n",
        "        \n",
        "                \n",
        "        # for transition predicted this dummy model\n",
        "        for transition, partial_parse in zip(transitions, minibatch):\n",
        "            # parse step\n",
        "            # transition: S\n",
        "            # partial_parse: Parsing(sentence)\n",
        "            partial_parse.parse_step(transition)\n",
        "            \n",
        "        # removing any sentence is finish\n",
        "        unfinished_parses[:] = [p for p in unfinished_parses if not p.is_completed()]\n",
        "    \n",
        "    dep = [parse.dep for parse in partial_parses]\n",
        "    \n",
        "    return dep"
      ],
      "metadata": {
        "id": "wrXMHYQNU6SU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyModel(object):\n",
        "    def predict(self, partial_parses):\n",
        "        # partial_parses: list of Parsing instances\n",
        "        # first shifting everything onto the stack, and then just doing RA if the first word\n",
        "        # of the sentence is \"right\", otherwise, is \"left\"\n",
        "        return [(\"RA\" if pp.stack[1] == \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]"
      ],
      "metadata": {
        "id": "wDPCTWLZU6OT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "             [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "             [\"left\", \"arcs\", \"only\"],\n",
        "             [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "\n",
        "minibatch_parse(sentences, DummyModel(), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQqLKxyrW_9r",
        "outputId": "4c7f6b59-2d08-420b-bac3-941c85fb1a20"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
              " [('only', 'again'), ('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
              " [('only', 'arcs'), ('only', 'left'), ('only', 'ROOT')],\n",
              " [('again', 'only'), ('again', 'arcs'), ('again', 'left'), ('again', 'ROOT')]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "_LrJpeRKXa3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conll(filename):\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    with open(filename) as f:\n",
        "        i = 0\n",
        "        word, pos, head, dep = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            i = i+1\n",
        "            wa = line.strip().split('\\t')  # ['1', 'In', '_', 'ADP', 'IN', '_', '5', 'case', '_', '_']\n",
        "            # In <--------  5th guy  # case\n",
        "            \n",
        "            if len(wa) == 10:  # if all the columns are there\n",
        "                word.append(wa[1].lower())\n",
        "                pos.append(wa[4])\n",
        "                head.append(int(wa[6]))\n",
        "                dep.append(wa[7])\n",
        "            \n",
        "            # the row is not exactly 10, it means new sentence\n",
        "            elif len(word) > 0:  # if there is somethign inside the word\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
        "                word, pos, head, dep = [], [], [], [] # clear word, pos, head, dep\n",
        "        \n",
        "        if len(word) > 0:  # if there is somethign inside the word\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  # in the sentence level\n",
        "\n",
        "    return examples  "
      ],
      "metadata": {
        "id": "Q0iFWeTFW_0K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    print(\"1. Loading data\")\n",
        "    train_set = read_conll(\"/content/train.conll\")\n",
        "    dev_set   = read_conll(\"/content/dev.conll\")\n",
        "    test_set   = read_conll(\"/content/test.conll\")\n",
        "    \n",
        "    # making my dataset smaller because my mac cannot handle it\n",
        "    train_set = train_set[:1000]\n",
        "    dev_set   = dev_set[:500]\n",
        "    test_set  = test_set[:500]\n",
        "    \n",
        "    return train_set, dev_set, test_set"
      ],
      "metadata": {
        "id": "Wvpepi3tI71l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Load Function"
      ],
      "metadata": {
        "id": "lhwNVhjXZCAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, dev_set, test_set = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Ipa75cPeGi",
        "outputId": "238ce2e9-9383-4257-c174-0be732dc8e25"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set), len(dev_set), len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwqXoq71PgV7",
        "outputId": "45a53535-9b6e-437e-f72c-dec2318b71ef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parser"
      ],
      "metadata": {
        "id": "_ThTL4R1bCcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_PREFIX = '<p>:' # indicating pos tags\n",
        "D_PREFIX = '<d>:' # indicating dependency tags\n",
        "UNK      = '<UNK>'\n",
        "NULL     = '<NULL>'\n",
        "ROOT     = '<ROOT>'\n",
        "\n",
        "class Parser(object):\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        \n",
        "        # setting the root dep\n",
        "        self.root_dep = 'root'\n",
        "                \n",
        "        # getting all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
        "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
        "                                               for w in ex['dep']\n",
        "                                               if w != self.root_dep]))\n",
        "        \n",
        "        # print(all_dep)\n",
        "        # 1. putting dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
        "        # {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
        "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
        "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
        "        \n",
        "        # we are using \"unlabeled\" where we do not label with the dependency\n",
        "        # thus the number of dependency relation is 1\n",
        "        trans = ['L', 'R', 'S']\n",
        "        self.n_deprel = 1   # because we are not predicting the relations, we are only predicting S, L, R\n",
        "        \n",
        "        # creating a simple lookup table mapping action and id\n",
        "        # e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
        "        # e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
        "        self.n_trans = len(trans)\n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}  # using for easy coding\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "               \n",
        "        # 2. putting pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  # also remember the pos tags of unknown\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "        \n",
        "        # now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
        "        \n",
        "        # 3. putting word into tok2id lookup table\n",
        "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
        "        tok2id[NULL] = self.NULL = len(tok2id)\n",
        "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "        \n",
        "        # now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
        "        \n",
        "        # creating id2tok\n",
        "        self.tok2id = tok2id\n",
        "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "        \n",
        "        self.n_features = 18 + 18 + 12\n",
        "        self.n_tokens = len(tok2id)\n",
        "\n",
        "                \n",
        "    # utility function, in case we want to convert token to id\n",
        "    # function to turn train set with words to train set with id instead using tok2id\n",
        "    def numericalize(self, examples):\n",
        "        numer_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "            pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "            dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
        "                            else -1 for w in ex['dep']]\n",
        "            numer_examples.append({'word': word, 'pos': pos,\n",
        "                                 'head': head, 'dep': dep})\n",
        "        return numer_examples\n",
        "            \n",
        "    # function to extract features to form a feature embedding matrix\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "             \n",
        "        # ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
        "        # ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
        "        # ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
        "        # ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
        "\n",
        "        # stack     :  [0]\n",
        "        # buffer    :  [1, 2, 3, 4, 5]\n",
        "        \n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0  # starting the stack with [ROOT]\n",
        "            \n",
        "        p_features = [] # pos features (2a, 2b, 2c) - 18\n",
        "        d_features = [] # dep features (3b, 3c) - 12\n",
        "        \n",
        "        # last 3 things on the stack as features\n",
        "        # if the stack is less than 3, then we simply append NULL from the left\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        \n",
        "        # next 3 things on the buffer as features\n",
        "        # if the buffer is less than 3, simply appending NULL\n",
        "        # the reason why NULL is appended on end because buffer is read left to right\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        \n",
        "        # corresponding pos tags\n",
        "        p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "        p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "             \n",
        "        # getting leftmost children based on the dependency arcs\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        # getting right most children based on the dependency arcs\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "        \n",
        "        # getting the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
        "        for i in range(2):\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1] # -1, -2 last two in the stack\n",
        "                \n",
        "                # the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
        "                lc = get_lc(k)  \n",
        "                rc = get_rc(k)\n",
        "                \n",
        "                # the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "\n",
        "                # leftmost of first word on stack, rightmost of first word, \n",
        "                # leftmost of the second word on stack, rightmost of second, \n",
        "                # leftmost of leftmost, rightmost of rightmost\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                # corresponding pos\n",
        "                p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "\n",
        "                        \n",
        "                # corresponding dep\n",
        "                d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
        "                \n",
        "            else:\n",
        "                # attaching NULL when they don't exist\n",
        "                features += [self.NULL] * 6\n",
        "                p_features += [self.P_NULL] * 6\n",
        "                d_features += [self.D_NULL] * 6\n",
        "                \n",
        "        features += p_features + d_features\n",
        "        assert len(features) == self.n_features  # asserting they are 18 + 18 + 12\n",
        "        \n",
        "        return features\n",
        "      \n",
        "    # generating training examples\n",
        "    # from the training sentences and their gold parse trees \n",
        "    def create_instances(self, examples):  # examples = word, pos, head, dep\n",
        "        all_instances = []\n",
        "        \n",
        "        for i, ex in enumerate(examples):\n",
        "            # Ms. Haag plays Elianti .\n",
        "            # e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
        "            # here 344 stands for ROOT\n",
        "            # Chaky - I cheated and take a look\n",
        "            n_words = len(ex['word']) - 1  #excluding the root\n",
        "            \n",
        "            # arcs = {(head, tail, dependency label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]  # [1, 2, 3, 4, 5]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            \n",
        "            # because that's the maximum number of shift, leftarcs, rightarcs we can have\n",
        "            # this will determine the sample size of each training example\n",
        "            # if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
        "            # but this for loop can be break if there is nothing left....\n",
        "            for i in range(n_words * 2):  # maximum times we can do either S, L, R\n",
        "                \n",
        "                # getting the gold transition based on the parse trees\n",
        "                # gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                \n",
        "                # if gold_t is None, no need to extract features.....\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                \n",
        "                # making sure when the model predicts, we inform the current state of stack and buffer, so\n",
        "                # the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
        "                legal_labels = self.legal_labels(stack, buf)                \n",
        "                assert legal_labels[gold_t] == 1\n",
        "                \n",
        "                # extracting all the 48 features \n",
        "                features = self.extract_features(stack, buf, arcs, ex)\n",
        "                instances.append((features, legal_labels, gold_t))\n",
        "                \n",
        "                # shift \n",
        "                if gold_t == 2:\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                # left arc \n",
        "                elif gold_t == 0:\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                # right arc\n",
        "                else:\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "                    \n",
        "            else:\n",
        "                all_instances += instances\n",
        "\n",
        "        return all_instances\n",
        "\n",
        "    # providing an one hot encoding of the labels\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel  # left arc but you cannot do ROOT <--- He\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel  # right arc because ROOT --> He\n",
        "        labels += [1] if len(buf) > 0 else [0]  #shift\n",
        "        return labels\n",
        "    \n",
        "    # a simple function to check punctuation POS tags\n",
        "    def punct(self, pos):\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    \n",
        "    # deciding whether to shift, leftarc, or rightarc, based on gold parse trees\n",
        "    # this is needed to create training examples which contain samples and ground truth\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        \n",
        "        # leaving if the stack is only 1, thus nothing to predict....\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "        \n",
        "        # predicting based on the last two words on the stack\n",
        "        # stack: [ROOT, he, has]\n",
        "        i0 = stack[-1] # has\n",
        "        i1 = stack[-2] # he\n",
        "        \n",
        "        # getting the head and dependency\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "        d0 = ex['dep'][i0]\n",
        "        d1 = ex['dep'][i1]\n",
        "        \n",
        "        # either shift, left arc or right arc\n",
        "        # \"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
        "        # if head of the second last word is the last word, then leftarc\n",
        "        if (i1 > 0) and (h1 == i0):\n",
        "            return 0  # action is left arc ---> gold_t\n",
        "        # if head of the last word is the second last word, then rightarc\n",
        "        # making sure nothing in the buffer has head with the last word on the stack\n",
        "        # otherwise, we lose the last word.....\n",
        "        elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "            return 1  # right arc\n",
        "        # otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
        "        else:\n",
        "            return None if len(buf) == 0 else 2  # shift         \n",
        "        \n",
        "        \n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        \n",
        "        for i, example in enumerate(dataset):\n",
        "            \n",
        "            # example['word']=[188, 186, 186, ..., 59]\n",
        "            # n_words=37\n",
        "            # sentence=[1, 2, 3, 4, 5,.., 37]\n",
        "            \n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]            \n",
        "            sentences.append(sentence)\n",
        "            \n",
        "            # mapping the object unique id to the i            \n",
        "            # The id is the object's memory address\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "            \n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "        \n",
        "        UAS = all_tokens = 0.0\n",
        "        with tqdm(total=len(dataset)) as prog:\n",
        "            for i, ex in enumerate(dataset):\n",
        "                head = [-1] * len(ex['word'])\n",
        "                for h, t, in dependencies[i]:\n",
        "                    head[t] = h\n",
        "                for pred_h, gold_h, gold_l, pos in \\\n",
        "                        zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
        "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                        if (not self.punct(pos_str)):\n",
        "                            UAS += 1 if pred_h == gold_h else 0\n",
        "                            all_tokens += 1\n",
        "                prog.update(i + 1)\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies"
      ],
      "metadata": {
        "id": "7kgABE4UVVIp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        mb_x = torch.from_numpy(mb_x).long()\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "\n",
        "        pred = self.parser.model(mb_x)\n",
        "        pred = pred.detach().numpy()\n",
        "        \n",
        "        # we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
        "        # other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        \n",
        "        return pred"
      ],
      "metadata": {
        "id": "mv_IbBmdq7b8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple function to create ids.....\n",
        "def build_dict(keys, offset=0):\n",
        "    # keys = ['P_PREFIX:IN', 'P_PREFIX:DT', 'P_PREFIX:NNP', 'P_PREFIX:CD', so on...]\n",
        "    # offset is needed because this tok2id has something already inside....\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    \n",
        "    # most_common = [('P_PREFIX:NN', 70), ('P_PREFIX:IN', 57), ... , ('P_PREFIX:JJR', 1)]\n",
        "    # we use most_common in case we only want some maximum pos tags....\n",
        "    mc = count.most_common()\n",
        "    \n",
        "    # {'P_PREFIX:NN': 31, 'P_PREFIX:IN': 32, .., 'P_PREFIX:JJR': 62} \n",
        "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
      ],
      "metadata": {
        "id": "sslH0ztMrA5Y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parser"
      ],
      "metadata": {
        "id": "Ob1S1RrLddYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2. Building parser\")\n",
        "start = time.time()\n",
        "parser = Parser(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7fFOx40rFs6",
        "outputId": "c07cbd1b-ef47-4a34-f2b0-32048fb479b9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Building parser\n",
            "took 0.04 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# before numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyfSLed31CeT",
        "outputId": "de1826e4-1625-4ecf-fe2f-91ebd9af0360"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
            "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
            "Head:  [2, 3, 0, 3, 3]\n",
            "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = parser.numericalize(train_set)\n",
        "dev_set = parser.numericalize(dev_set)\n",
        "test_set = parser.numericalize(test_set)"
      ],
      "metadata": {
        "id": "1JjdQPMn49jB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj48GALk5CbD",
        "outputId": "2d941754-17d3-4fdc-bd34-e7dd8da30cc4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
            "Pos:  [84, 42, 42, 55, 42, 46]\n",
            "Head:  [-1, 2, 3, 0, 3, 3]\n",
            "Dep:  [-1, 30, 8, 0, 20, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding"
      ],
      "metadata": {
        "id": "b9FCaWA4dviw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4. Loading pretrained embeddings...\",)\n",
        "start = time.time()\n",
        "word_vectors = {}\n",
        "for line in open(\"/content/en-cw.txt\").readlines():\n",
        "    we = line.strip().split() # we = word embeddings - first column: word;  the rest is embedding\n",
        "    word_vectors[we[0]] = [float(x) for x in we[1:]] # {word: [list of 50 numbers], nextword: [another list], so on...}\n",
        "    \n",
        "# creating an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
        "# we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
        "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
        "\n",
        "for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token]\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
        "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHwrDBoY1LtK",
        "outputId": "bf422715-9eaa-4f84-fae3-1612cf36011d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Loading pretrained embeddings...\n",
            "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
            "took 2.60 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "WROj1Q4mebjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5. Preprocessing training data...\",)\n",
        "start = time.time()\n",
        "train_examples = parser.create_instances(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd7xfseQ1MJB",
        "outputId": "fa09a6e0-af32-43c0-d8dd-8250f0e13c84"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Preprocessing training data...\n",
            "took 1.73 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ThsK97ekhR",
        "outputId": "f737156d-43d4-4337-94dd-747fb55b3e9a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([5155,\n",
              "  5155,\n",
              "  5156,\n",
              "  91,\n",
              "  113,\n",
              "  806,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  83,\n",
              "  83,\n",
              "  84,\n",
              "  40,\n",
              "  41,\n",
              "  42,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38],\n",
              " [0, 0, 1],\n",
              " 2)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minibatch Loader"
      ],
      "metadata": {
        "id": "sY5PmcZWepzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    data_size = len(data[0])\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)"
      ],
      "metadata": {
        "id": "nudGhfoy3GhT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing your Minibatch Loader"
      ],
      "metadata": {
        "id": "YOQ6br-5ewzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (train_x, train_y) in enumerate(minibatches(train_examples, 1024)):\n",
        "#     print(train_x.shape)  #batch size, features\n",
        "#     print(train_y.shape)        #one hot encoding of 3 actions - shift, la, ra"
      ],
      "metadata": {
        "id": "4-s-6mbF3Gsk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "4WP41fdKe8dB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParserModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, n_features=48,\n",
        "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
        "\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        # t:  batch_size, n_features\n",
        "        batch_size = t.size()[0]\n",
        "                    \n",
        "        x = self.pretrained_embeddings(t)        \n",
        "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
        "        # x = (1024, 48 * 50)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t: (1024, 48)\n",
        "        embeddings = self.embedding_lookup(t)  \n",
        "    \n",
        "        # embeddings: (1024, 48 * 50)\n",
        "        hidden = self.embed_to_hidden(embeddings)\n",
        "    \n",
        "        # hidden: (1024, 200)\n",
        "        hidden_activations = F.relu(hidden)\n",
        "        # hidden_activations: (1024, 200)\n",
        "        thin_net = self.dropout(hidden_activations)\n",
        "        # thin_net: (1024, 200)\n",
        "        logits = self.hidden_to_logits(thin_net)\n",
        "        # logits: (1024, 3)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "-9BQqgbj5xDF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just a class to get the average.....\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "pRr0n-Ti9ntq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \n",
        "    best_dev_UAS = 0\n",
        "    \n",
        "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(\n",
        "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \n",
        "    parser.model.train()  # places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            \n",
        "            # train_x:  batch_size, n_features\n",
        "            # train_y:  batch_size, target(=3)\n",
        "            \n",
        "            optimizer.zero_grad() \n",
        "            loss = 0.\n",
        "            train_x = torch.from_numpy(train_x).long()  # long() for int so embedding works....\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  # getting the index with 1 because torch expects label to be single integer\n",
        "\n",
        "            # forward pass: computing predicted logits.\n",
        "            logits = parser.model(train_x)\n",
        "            # computing loss\n",
        "            loss = loss_func(logits, train_y)\n",
        "            # computing gradients of the loss w.r.t model parameters.\n",
        "            loss.backward()\n",
        "            # taking step with optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    parser.model.eval()  # places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "        \n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS"
      ],
      "metadata": {
        "id": "uz2vHZxR9pGU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Slt1OdUYgAB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating directory if it does not exist for saving the weights...\n",
        "output_dir = \"output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    \n",
        "print(80 * \"=\")\n",
        "print(\"TRAINING\")\n",
        "print(80 * \"=\")\n",
        "    \n",
        "model = ParserModel(embeddings_matrix)\n",
        "parser.model = model\n",
        "\n",
        "start = time.time()\n",
        "train(parser, train_examples, dev_set, output_path,\n",
        "      batch_size=1024, n_epochs=10, lr=0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-3RdF5U91cs",
        "outputId": "13286e1b-59fb-44e7-d4a3-cd662501c51a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  8.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.6064978316426277\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 9260295.72it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 56.44\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.3080985825508833\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7773435.97it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 61.63\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:06<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.24785119388252497\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 2474605.97it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 66.32\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:06<00:00,  7.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.2101665378237764\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7817275.91it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 68.22\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.18536188950141272\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7969667.55it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 71.69\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 6 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.16693965066224337\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7457400.47it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 72.31\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.14806008618324995\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7439868.80it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 73.65\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1345437162866195\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7641037.00it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 73.82\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 9 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.12305364587033789\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7029700.87it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 74.17\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 10 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.11188736759747069\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7827992.49it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 74.90\n",
            "New best dev UAS! Saving model.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "3KjN-JpmpD5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(80 * \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80 * \"=\")\n",
        "\n",
        "print(\"Restoring the best model weights found on the dev set\")\n",
        "parser.model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evaluation on test set\",)\n",
        "parser.model.eval()\n",
        "UAS, dependencies = parser.parse(test_set)\n",
        "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVFbwlu8E9sg",
        "outputId": "3c278ff8-ac42-4b82-d868-8cf425cae15f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "Final evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7934039.78it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- test UAS: 76.09\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation Study with Pos and Dep Features"
      ],
      "metadata": {
        "id": "MWWzLGtVTYwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "uOJIL1wxqA7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conll(filename):\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    with open(filename) as f:\n",
        "        i = 0\n",
        "        word, pos, head, dep = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            i = i+1\n",
        "            wa = line.strip().split('\\t')  # ['1', 'In', '_', 'ADP', 'IN', '_', '5', 'case', '_', '_']\n",
        "            # In <--------  5th guy  # case\n",
        "            \n",
        "            if len(wa) == 10:  # if all the columns are there\n",
        "                word.append(wa[1].lower())\n",
        "                pos.append(wa[4])\n",
        "                head.append(int(wa[6]))\n",
        "                dep.append(wa[7])\n",
        "            \n",
        "            # the row is not exactly 10, it means new sentence\n",
        "            elif len(word) > 0:  # if there is somethign inside the word\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
        "                word, pos, head, dep = [], [], [], [] # clear word, pos, head, dep\n",
        "        \n",
        "        if len(word) > 0:  # if there is somethign inside the word\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  # in the sentence level\n",
        "\n",
        "    return examples "
      ],
      "metadata": {
        "id": "wCLpzRFAY2n2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    print(\"1. Loading data\")\n",
        "    train_set = read_conll(\"/content/train.conll\")\n",
        "    dev_set   = read_conll(\"/content/dev.conll\")\n",
        "    test_set   = read_conll(\"/content/test.conll\")\n",
        "    \n",
        "    # making my dataset smaller because my mac cannot handle it\n",
        "    train_set = train_set[:1000]\n",
        "    dev_set   = dev_set[:500]\n",
        "    test_set  = test_set[:500]\n",
        "    \n",
        "    return train_set, dev_set, test_set"
      ],
      "metadata": {
        "id": "4PdhP4foZBtZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Load Function"
      ],
      "metadata": {
        "id": "2sfkj6RXsILB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, dev_set, test_set = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82bEMvGhZBo2",
        "outputId": "b590240b-e17f-4008-9b26-e7a4854360ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set), len(dev_set), len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S-i-IMdZBj8",
        "outputId": "32582854-954b-4f28-f71b-9f6fbeb5baa0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parser"
      ],
      "metadata": {
        "id": "YqqV7sIFsd8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_PREFIX = '<p>:' # indicating pos tags\n",
        "D_PREFIX = '<d>:' # indicating dependency tags\n",
        "UNK      = '<UNK>'\n",
        "NULL     = '<NULL>'\n",
        "ROOT     = '<ROOT>'\n",
        "\n",
        "class ModifiedParser(object):\n",
        "\n",
        "    def __init__(self, dataset, pos_feat = True, dep_feat = True):\n",
        "\n",
        "      self.n_features = 18\n",
        "      self.pos_feat = pos_feat\n",
        "      self.dep_feat = dep_feat\n",
        "\n",
        "      tok2id = dict()\n",
        "\n",
        "      # we are using \"unlabeled\" where we do not label with the dependency\n",
        "      # thus the number of dependency relation is 1\n",
        "      # trans = ['L', 'R', 'S']\n",
        "      # self.n_deprel = 1   # because we are not predicting the relations, we are only predicting S, L, R\n",
        "        \n",
        "      # creating a simple lookup table mapping action and id\n",
        "      # e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
        "      # e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
        "      # self.n_trans = len(trans)\n",
        "\n",
        "      if self.dep_feat == True:\n",
        "        \n",
        "        # setting the root dep\n",
        "        self.root_dep = 'root'\n",
        "                \n",
        "        # getting all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
        "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
        "                                               for w in ex['dep']\n",
        "                                               if w != self.root_dep]))\n",
        "        # print(all_dep)\n",
        "        # 1. putting dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
        "        # {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
        "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
        "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
        "\n",
        "        # we are using \"unlabeled\" where we do not label with the dependency\n",
        "        # thus the number of dependency relation is 1\n",
        "        trans = ['L', 'R', 'S']\n",
        "        self.n_deprel = 1   # because we are not predicting the relations, we are only predicting S, L, R\n",
        "        \n",
        "        # creating a simple lookup table mapping action and id\n",
        "        # e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
        "        # e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
        "        self.n_trans = len(trans)        \n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}  # using for easy coding\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "\n",
        "      if self.pos_feat == True: \n",
        "\n",
        "        # 2. putting pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  # also remember the pos tags of unknown\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "        \n",
        "        # now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
        "        \n",
        "      # 3. putting word into tok2id lookup table\n",
        "      tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "      tok2id[UNK]  = self.UNK = len(tok2id)\n",
        "      tok2id[NULL] = self.NULL = len(tok2id)\n",
        "      tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "        \n",
        "      # now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
        "        \n",
        "      # creating id2tok\n",
        "      self.tok2id = tok2id\n",
        "      self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "        \n",
        "\n",
        "      if self.pos_feat == True:\n",
        "            self.n_features = self.n_features + 18\n",
        "      if self.dep_feat == True:\n",
        "            self.n_features = self.n_features + 12\n",
        "\n",
        "      self.n_tokens = len(tok2id)\n",
        "\n",
        "                \n",
        "    # utility function, in case we want to convert token to id\n",
        "    # function to turn train set with words to train set with id instead using tok2id\n",
        "    def numericalize(self, examples):\n",
        "        numer_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "\n",
        "            if self.pos_feat == True:\n",
        "              \n",
        "              pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "\n",
        "            if self.dep_feat == True:\n",
        "\n",
        "              dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
        "                              else -1 for w in ex['dep']]\n",
        "\n",
        "            if self.pos_feat != True and self.dep_feat != True: \n",
        "              numer_examples.append({'word': word, 'head': head})\n",
        "\n",
        "            elif self.pos_feat == True and self.dep_feat != True:\n",
        "              numer_examples.append({'word': word, 'pos': pos, 'head': head})\n",
        "\n",
        "            elif self.pos_feat != True and self.dep_feat == True:\n",
        "              numer_examples.append({'word': word, 'head': head, 'dep': dep}) # 'dep': dep\n",
        "\n",
        "            else:\n",
        "              numer_examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})\n",
        "\n",
        "        return numer_examples\n",
        "            \n",
        "    # function to extract features to form a feature embedding matrix\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "             \n",
        "        # ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
        "        # ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
        "        # ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
        "        # ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
        "\n",
        "        # stack     :  [0]\n",
        "        # buffer    :  [1, 2, 3, 4, 5]\n",
        "        \n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0  # starting the stack with [ROOT]\n",
        "            \n",
        "        p_features = [] # pos features (2a, 2b, 2c) - 18\n",
        "        d_features = [] # dep features (3b, 3c) - 12\n",
        "        \n",
        "        # last 3 things on the stack as features\n",
        "        # if the stack is less than 3, then we simply append NULL from the left\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        \n",
        "        # next 3 things on the buffer as features\n",
        "        # if the buffer is less than 3, simply appending NULL\n",
        "        # the reason why NULL is appended on end because buffer is read left to right\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        \n",
        "        if self.pos_feat == True:\n",
        "          # corresponding pos tags\n",
        "          p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "          p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "             \n",
        "        # getting leftmost children based on the dependency arcs\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        # getting right most children based on the dependency arcs\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "        \n",
        "        # getting the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
        "        for i in range(2):\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1] # -1, -2 last two in the stack\n",
        "                \n",
        "                # the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
        "                lc = get_lc(k)  \n",
        "                rc = get_rc(k)\n",
        "                \n",
        "                # the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "\n",
        "                # leftmost of first word on stack, rightmost of first word, \n",
        "                # leftmost of the second word on stack, rightmost of second, \n",
        "                # leftmost of leftmost, rightmost of rightmost\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                if self.pos_feat == True: \n",
        "                  # corresponding pos\n",
        "                  p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                  p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                  p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                  p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                  p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                  p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "\n",
        "                if self.dep_feat == True:\n",
        "                  # corresponding dep\n",
        "                  d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
        "                  d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
        "                  d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
        "                  d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
        "                  d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
        "                  d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
        "                \n",
        "            else:\n",
        "                # attaching NULL when they don't exist\n",
        "                features += [self.NULL] * 6\n",
        "                p_features += [self.P_NULL] * 6\n",
        "                d_features += [self.D_NULL] * 6\n",
        "                \n",
        "        # features += p_features + d_features\n",
        "        if self.pos_feat == True and self.dep_feat == True:\n",
        "            features += p_features + d_features\n",
        "\n",
        "        elif self.pos_feat == True and self.dep_feat != True:\n",
        "            features += p_features\n",
        "\n",
        "        elif self.pos_feat != True and self.dep_feat == True:\n",
        "            features += d_features\n",
        "\n",
        "        else:\n",
        "            features = features\n",
        "        \n",
        "        assert len(features) == self.n_features  # asserting they are 18 + 18 + 12\n",
        "        \n",
        "        return features\n",
        "      \n",
        "    # generating training examples\n",
        "    # from the training sentences and their gold parse trees \n",
        "    def create_instances(self, examples):  # examples = word, pos, head, dep\n",
        "        all_instances = []\n",
        "        \n",
        "        for i, ex in enumerate(examples):\n",
        "            # Ms. Haag plays Elianti .\n",
        "            # e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
        "            # here 344 stands for ROOT\n",
        "            # Chaky - I cheated and take a look\n",
        "            n_words = len(ex['word']) - 1  #excluding the root\n",
        "            \n",
        "            # arcs = {(head, tail, dependency label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]  # [1, 2, 3, 4, 5]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            \n",
        "            # because that's the maximum number of shift, leftarcs, rightarcs we can have\n",
        "            # this will determine the sample size of each training example\n",
        "            # if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
        "            # but this for loop can be break if there is nothing left....\n",
        "            for i in range(n_words * 2):  # maximum times we can do either S, L, R\n",
        "                \n",
        "                # getting the gold transition based on the parse trees\n",
        "                # gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                \n",
        "                # if gold_t is None, no need to extract features.....\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                \n",
        "                # making sure when the model predicts, we inform the current state of stack and buffer, so\n",
        "                # the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
        "                legal_labels = self.legal_labels(stack, buf)                \n",
        "                assert legal_labels[gold_t] == 1\n",
        "                \n",
        "                # extracting all the 48 features \n",
        "                features = self.extract_features(stack, buf, arcs, ex)\n",
        "                instances.append((features, legal_labels, gold_t))\n",
        "                \n",
        "                # shift \n",
        "                if gold_t == 2:\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                # left arc \n",
        "                elif gold_t == 0:\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                # right arc\n",
        "                else:\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "                    \n",
        "            else:\n",
        "                all_instances += instances\n",
        "                \n",
        "        if self.dep_feat != True: # We only need the word and pos\n",
        "            all_instances = [[instance[0], instance[2]] for instance in all_instances]\n",
        "\n",
        "        return all_instances\n",
        "\n",
        "    # providing an one hot encoding of the labels\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel  # left arc but you cannot do ROOT <--- He\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel  # right arc because ROOT --> He\n",
        "        labels += [1] if len(buf) > 0 else [0]  #shift\n",
        "        return labels\n",
        "    \n",
        "    # a simple function to check punctuation POS tags\n",
        "    def punct(self, pos):\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    \n",
        "    # deciding whether to shift, leftarc, or rightarc, based on gold parse trees\n",
        "    # this is needed to create training examples which contain samples and ground truth\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        \n",
        "        # leaving if the stack is only 1, thus nothing to predict....\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "        \n",
        "        # predicting based on the last two words on the stack\n",
        "        # stack: [ROOT, he, has]\n",
        "        i0 = stack[-1] # has\n",
        "        i1 = stack[-2] # he\n",
        "        \n",
        "        # getting the head and dependency\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "\n",
        "        if self.dep_feat == True: \n",
        "          d0 = ex['dep'][i0]\n",
        "          d1 = ex['dep'][i1]\n",
        "        \n",
        "        # either shift, left arc or right arc\n",
        "        # \"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
        "        # if head of the second last word is the last word, then leftarc\n",
        "        if (i1 > 0) and (h1 == i0):\n",
        "            return 0  # action is left arc ---> gold_t\n",
        "        # if head of the last word is the second last word, then rightarc\n",
        "        # making sure nothing in the buffer has head with the last word on the stack\n",
        "        # otherwise, we lose the last word.....\n",
        "        elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "            return 1  # right arc\n",
        "        # otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
        "        else:\n",
        "            return None if len(buf) == 0 else 2  # shift         \n",
        "        \n",
        "        \n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        \n",
        "        for i, example in enumerate(dataset):\n",
        "            \n",
        "            # example['word']=[188, 186, 186, ..., 59]\n",
        "            # n_words=37\n",
        "            # sentence=[1, 2, 3, 4, 5,.., 37]\n",
        "            \n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]            \n",
        "            sentences.append(sentence)\n",
        "            \n",
        "            # mapping the object unique id to the i            \n",
        "            # The id is the object's memory address\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "            \n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "        \n",
        "        UAS = all_tokens = 0.0\n",
        "        with tqdm(total=len(dataset)) as prog:\n",
        "            for i, ex in enumerate(dataset):\n",
        "                head = [-1] * len(ex['word'])\n",
        "                for h, t, in dependencies[i]:\n",
        "                    head[t] = h\n",
        "                    \n",
        "                    if self.pos_feat == True and self.dep_feat != True:    \n",
        "                         for pred_h, gold_h, pos in \\\n",
        "                            zip(head[1:], ex['head'][1:], ex['pos'][1:]):\n",
        "                            assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                            pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                            if (not self.punct(pos_str)):\n",
        "                                UAS += 1 if pred_h == gold_h else 0\n",
        "                                all_tokens += 1\n",
        "\n",
        "                               \n",
        "                    elif self.pos_feat != True and self.dep_feat == True :   \n",
        "                         for pred_h, gold_h, pos in \\\n",
        "                            zip(head[1:], ex['head'][1:], ex['dep'][1:]):\n",
        "                            if (not self.punct(pos_str)):\n",
        "                                UAS += 1 if pred_h == gold_h else 0\n",
        "                                all_tokens += 1\n",
        "\n",
        "                    else: \n",
        "                         for pred_h, gold_h, gold_l, pos in \\\n",
        "                            zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
        "                            assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                            pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                            if (not self.punct(pos_str)):\n",
        "                                UAS += 1 if pred_h == gold_h else 0\n",
        "                                all_tokens += 1\n",
        "\n",
        "                prog.update(i + 1)\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies"
      ],
      "metadata": {
        "id": "Z-oA2tHe7vL9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        mb_x = torch.from_numpy(mb_x).long()\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "\n",
        "        pred = self.parser.model(mb_x)\n",
        "        pred = pred.detach().numpy()\n",
        "        \n",
        "        # we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
        "        # other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        \n",
        "        return pred"
      ],
      "metadata": {
        "id": "pvVFw28mFNdn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple function to create ids.....\n",
        "def build_dict(keys, offset=0):\n",
        "    # keys = ['P_PREFIX:IN', 'P_PREFIX:DT', 'P_PREFIX:NNP', 'P_PREFIX:CD', so on...]\n",
        "    # offset is needed because this tok2id has something already inside....\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    \n",
        "    # most_common = [('P_PREFIX:NN', 70), ('P_PREFIX:IN', 57), ... , ('P_PREFIX:JJR', 1)]\n",
        "    # we use most_common in case we only want some maximum pos tags....\n",
        "    mc = count.most_common()\n",
        "    \n",
        "    # {'P_PREFIX:NN': 31, 'P_PREFIX:IN': 32, .., 'P_PREFIX:JJR': 62} \n",
        "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
      ],
      "metadata": {
        "id": "x2asjd9w-4Bq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parser"
      ],
      "metadata": {
        "id": "gFFyfTprs5zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2. Building parser\")\n",
        "start = time.time()\n",
        "modified_parser = ModifiedParser(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utDhmGHO-345",
        "outputId": "e2e9c6aa-b7e4-4178-9bb6-c5f6b5dc8992"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Building parser\n",
            "took 0.04 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# before numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVJWAZDc-3qB",
        "outputId": "55e7c517-95bb-4b48-babd-39a7f25734fa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
            "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
            "Head:  [2, 3, 0, 3, 3]\n",
            "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = modified_parser.numericalize(train_set)\n",
        "dev_set = modified_parser.numericalize(dev_set)\n",
        "test_set = modified_parser.numericalize(test_set)"
      ],
      "metadata": {
        "id": "JsGaIoUxZzfr"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51mUJTN6ZzYl",
        "outputId": "9422a985-7597-4eb2-dd7c-39671f2d4e3d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
            "Pos:  [84, 42, 42, 55, 42, 46]\n",
            "Head:  [-1, 2, 3, 0, 3, 3]\n",
            "Dep:  [-1, 30, 8, 0, 20, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding"
      ],
      "metadata": {
        "id": "Aq2j0UudtHAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4. Loading pretrained embeddings...\",)\n",
        "start = time.time()\n",
        "word_vectors = {}\n",
        "for line in open(\"/content/en-cw.txt\").readlines():\n",
        "    we = line.strip().split() # we = word embeddings - first column: word;  the rest is embedding\n",
        "    word_vectors[we[0]] = [float(x) for x in we[1:]] # {word: [list of 50 numbers], nextword: [another list], so on...}\n",
        "    \n",
        "# creating an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
        "# we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
        "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (modified_parser.n_tokens, 50)), dtype='float32')\n",
        "\n",
        "for token in modified_parser.tok2id:\n",
        "        i = modified_parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token]\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
        "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkCPUWx6aWNu",
        "outputId": "30d5fab1-538f-4066-a9db-b0a5ba8a3468"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Loading pretrained embeddings...\n",
            "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
            "took 2.54 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "qiuREnQEtYv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5. Preprocessing training data...\",)\n",
        "start = time.time()\n",
        "train_examples = modified_parser.create_instances(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COU86_m1ZzOM",
        "outputId": "1204295f-aa94-4147-f734-27ef489f1090"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Preprocessing training data...\n",
            "took 1.70 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_examples[0]"
      ],
      "metadata": {
        "id": "MwCR1CAraX4a"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minibatch Loader"
      ],
      "metadata": {
        "id": "Z0w35pFLtsG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    data_size = len(data[0])\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)"
      ],
      "metadata": {
        "id": "0sfuudHpaXyP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Minibatch Loader"
      ],
      "metadata": {
        "id": "TWpyG466t2Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (train_x, train_y) in enumerate(minibatches(train_examples, 1024)):\n",
        "    # print(train_x.shape)  # batch size, features\n",
        "    # print(train_y.shape)        # one hot encoding of 3 actions - shift, la, ra"
      ],
      "metadata": {
        "id": "T1llyNIjt501"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "Ult9Yh4TuJKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedParserModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, n_features=48,\n",
        "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
        "\n",
        "        super(ModifiedParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        # t:  batch_size, n_features\n",
        "        batch_size = t.size()[0]\n",
        "                    \n",
        "        x = self.pretrained_embeddings(t)        \n",
        "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
        "        # x = (1024, 48 * 50)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t: (1024, 48)\n",
        "        embeddings = self.embedding_lookup(t)  \n",
        "    \n",
        "        # embeddings: (1024, 48 * 50)\n",
        "        hidden = self.embed_to_hidden(embeddings)\n",
        "    \n",
        "        # hidden: (1024, 200)\n",
        "        hidden_activations = F.relu(hidden)\n",
        "        # hidden_activations: (1024, 200)\n",
        "        thin_net = self.dropout(hidden_activations)\n",
        "        # thin_net: (1024, 200)\n",
        "        logits = self.hidden_to_logits(thin_net)\n",
        "        # logits: (1024, 3)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "HNiSztQQaXsF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just a class to get the average.....\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "fsGwl2YTzqhD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## writing train functions for POS features\n",
        "\n",
        "def pos_train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005, pos_feat = True,):\n",
        "    \n",
        "    best_dev_UAS = 0\n",
        "    \n",
        "    optimizer = optim.Adam(modified_parser.model.parameters(), lr=0.001)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = pos_train_for_epoch(\n",
        "            modified_parser, train_data, dev_data, optimizer, loss_func, batch_size, pos_feat)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(modified_parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def pos_train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size, pos_feat = True,):\n",
        "    \n",
        "    modified_parser.model.train()  # places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            \n",
        "            # train_x:  batch_size, n_features\n",
        "            # train_y:  batch_size, target(=3)\n",
        "            \n",
        "            optimizer.zero_grad() \n",
        "            loss = 0.\n",
        "            train_x = torch.from_numpy(train_x).long()  # long() for int so embedding works....\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  # getting the index with 1 because torch expects label to be single integer\n",
        "\n",
        "            # forward pass: computing predicted logits.\n",
        "            logits = modified_parser.model(train_x)\n",
        "            # computing loss\n",
        "            loss = loss_func(logits, train_y)\n",
        "            # computing gradients of the loss w.r.t model parameters.\n",
        "            loss.backward()\n",
        "            # taking step with optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    modified_parser.model.eval()  # places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "        \n",
        "    dev_UAS, _ = modified_parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS"
      ],
      "metadata": {
        "id": "GC50SW875C8I"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with POS Features"
      ],
      "metadata": {
        "id": "n-7Lg-c4vmQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating directory if it does not exist for saving the weights...\n",
        "## for POS features\n",
        "\n",
        "output_dir = \"pos_output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    \n",
        "print(80 * \"=\")\n",
        "print(\"TRAINING\")\n",
        "print(80 * \"=\")\n",
        "    \n",
        "model = ModifiedParserModel(embeddings_matrix)\n",
        "modified_parser.model = model\n",
        "\n",
        "start = time.time()\n",
        "pos_train(modified_parser, train_examples, dev_set, output_path,\n",
        "      batch_size=1024, n_epochs=10, lr=0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si7PgbDI5fVl",
        "outputId": "edf2a0e3-19fe-4d97-eb02-36b55f1860f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  8.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.6064962564657131\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 384398.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 32.67\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.308443493830661\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 379640.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 35.50\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.25316943041980267\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 387812.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 36.42\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.22043579816818237\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 378837.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 37.81\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.19283048839618763\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 377061.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 38.99\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 6 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.17605273239314556\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 392499.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 39.64\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1585300766552488\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 372877.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 40.30\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1471828481492897\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 388027.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 40.04\n",
            "\n",
            "Epoch 9 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.13203694686914483\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 387595.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 40.97\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 10 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  8.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.12197107992445429\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 355185.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 41.13\n",
            "New best dev UAS! Saving model.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "qS3znH2LvbFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(80 * \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80 * \"=\")\n",
        "\n",
        "print(\"Restoring the best model weights found on the dev set\")\n",
        "modified_parser.model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evaluation on test set\",)\n",
        "modified_parser.model.eval()\n",
        "UAS, dependencies = modified_parser.parse(test_set)\n",
        "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdI3iBlD5x6C",
        "outputId": "8eeeedf2-3034-4c14-e7b2-cf5fe46bd40a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "Final evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 399058.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- test UAS: 42.38\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## writing train functions for DEP Features\n",
        "\n",
        "def dep_train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005, dep_feat = True,):\n",
        "    \n",
        "    best_dev_UAS = 0\n",
        "    \n",
        "    optimizer = optim.Adam(modified_parser.model.parameters(), lr=0.001)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = dep_train_for_epoch(\n",
        "            modified_parser, train_data, dev_data, optimizer, loss_func, batch_size, dep_feat)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(modified_parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def dep_train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size, dep_feat = True,):\n",
        "    \n",
        "    modified_parser.model.train()  # places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            \n",
        "            # train_x:  batch_size, n_features\n",
        "            # train_y:  batch_size, target(=3)\n",
        "            \n",
        "            optimizer.zero_grad() \n",
        "            loss = 0.\n",
        "            train_x = torch.from_numpy(train_x).long()  # long() for int so embedding works....\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  # getting the index with 1 because torch expects label to be single integer\n",
        "\n",
        "            # forward pass: computing predicted logits.\n",
        "            logits = modified_parser.model(train_x)\n",
        "            # computing loss\n",
        "            loss = loss_func(logits, train_y)\n",
        "            # computing gradients of the loss w.r.t model parameters.\n",
        "            loss.backward()\n",
        "            # taking step with optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    modified_parser.model.eval()  # places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "        \n",
        "    dev_UAS, _ = modified_parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS"
      ],
      "metadata": {
        "id": "IDjFKhM8zqbD"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with DEP Features"
      ],
      "metadata": {
        "id": "Ymu0wZ-wwQMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating directory if it does not exist for saving the weights...\n",
        "## for DEP Features\n",
        "\n",
        "output_dir = \"dep_output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    \n",
        "print(80 * \"=\")\n",
        "print(\"TRAINING\")\n",
        "print(80 * \"=\")\n",
        "    \n",
        "model = ModifiedParserModel(embeddings_matrix)\n",
        "modified_parser.model = model\n",
        "\n",
        "start = time.time()\n",
        "dep_train(modified_parser, train_examples, dev_set, output_path,\n",
        "      batch_size=1024, n_epochs=10, lr=0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHOaTJ9KzqVD",
        "outputId": "2ca0741d-fe8b-4767-94fd-7434275e99e4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.6044616556415955\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 381705.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 32.66\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.3134731526176135\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 391957.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 35.79\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.25003557838499546\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 393257.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 36.27\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.21792984443406263\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 366418.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 37.70\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  8.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.19120179737607637\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 396492.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 38.06\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 6 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1732092263797919\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 395426.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 39.57\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:07<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.15763349117090306\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 398025.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 39.97\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.14271500644584498\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 391618.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 40.65\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 9 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.13063790928572416\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 390783.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 40.87\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 10 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.12204136326909065\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 387441.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 40.85\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "sFxl5FpqxN5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(80 * \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80 * \"=\")\n",
        "\n",
        "print(\"Restoring the best model weights found on the dev set\")\n",
        "modified_parser.model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evaluation on test set\",)\n",
        "modified_parser.model.eval()\n",
        "UAS, dependencies = modified_parser.parse(test_set)\n",
        "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oGALKxbzqOF",
        "outputId": "efd75374-2a4f-4820-a42e-b4320f6f460e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "Final evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 393100.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- test UAS: 41.77\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison Study with Embeddings"
      ],
      "metadata": {
        "id": "JI-zRGaSN9bV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "WoSONlnNOWSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conll(filename):\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    with open(filename) as f:\n",
        "        i = 0\n",
        "        word, pos, head, dep = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            i = i+1\n",
        "            wa = line.strip().split('\\t')  # ['1', 'In', '_', 'ADP', 'IN', '_', '5', 'case', '_', '_']\n",
        "            # In <--------  5th guy  # case\n",
        "            \n",
        "            if len(wa) == 10:  # if all the columns are there\n",
        "                word.append(wa[1].lower())\n",
        "                pos.append(wa[4])\n",
        "                head.append(int(wa[6]))\n",
        "                dep.append(wa[7])\n",
        "            \n",
        "            # the row is not exactly 10, it means new sentence\n",
        "            elif len(word) > 0:  # if there is somethign inside the word\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
        "                word, pos, head, dep = [], [], [], [] # clear word, pos, head, dep\n",
        "        \n",
        "        if len(word) > 0:  # if there is somethign inside the word\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  # in the sentence level\n",
        "\n",
        "    return examples  "
      ],
      "metadata": {
        "id": "6gDF3P1LOJYz"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    print(\"1. Loading data\")\n",
        "    train_set = read_conll(\"/content/train.conll\")\n",
        "    dev_set   = read_conll(\"/content/dev.conll\")\n",
        "    test_set   = read_conll(\"/content/test.conll\")\n",
        "    \n",
        "    # making my dataset smaller because my mac cannot handle it\n",
        "    train_set = train_set[:1000]\n",
        "    dev_set   = dev_set[:500]\n",
        "    test_set  = test_set[:500]\n",
        "    \n",
        "    return train_set, dev_set, test_set"
      ],
      "metadata": {
        "id": "gHdkMUovOaXy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing with Load Function"
      ],
      "metadata": {
        "id": "Q1IHviK8Oyfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, dev_set, test_set = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1xuQRmpOaSn",
        "outputId": "015cd037-aea3-4434-a5d9-0ba40e12e727"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set), len(dev_set), len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te8T5DmMOaNo",
        "outputId": "d3587026-8dc2-4a61-fffe-4e8b2b185586"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parser"
      ],
      "metadata": {
        "id": "jQZSchQGPTBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_PREFIX = '<p>:' # indicating pos tags\n",
        "D_PREFIX = '<d>:' # indicating dependency tags\n",
        "UNK      = '<UNK>'\n",
        "NULL     = '<NULL>'\n",
        "ROOT     = '<ROOT>'\n",
        "\n",
        "class Parser_Embedding(object):\n",
        "\n",
        "    def __init__(self, dataset, vocab_size, embed_size):\n",
        "        \n",
        "        # setting the root dep\n",
        "        self.root_dep = 'root'\n",
        "\n",
        "        #self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        self.embd = nn.Embedding(vocab_size, embd_size)\n",
        "        # if pre_embd_w is not None:\n",
        "            # print('pre embedding weight is set')\n",
        "            # self.embd.weight = nn.Parameter(pre_embd_w, requires_grad=True)\n",
        "\n",
        "        # self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
        "        # self.embedding_u = nn.Embedding(vocab_size, embed_size) # out embedding\n",
        "                \n",
        "        # getting all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
        "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
        "                                               for w in ex['dep']\n",
        "                                               if w != self.root_dep]))\n",
        "        \n",
        "        # print(all_dep)\n",
        "        # 1. putting dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
        "        # {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
        "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
        "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
        "        \n",
        "        # we are using \"unlabeled\" where we do not label with the dependency\n",
        "        # thus the number of dependency relation is 1\n",
        "        trans = ['L', 'R', 'S']\n",
        "        self.n_deprel = 1   # because we are not predicting the relations, we are only predicting S, L, R\n",
        "        \n",
        "        # creating a simple lookup table mapping action and id\n",
        "        # e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
        "        # e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
        "        self.n_trans = len(trans)\n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}  # using for easy coding\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "               \n",
        "        # 2. putting pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  # also remember the pos tags of unknown\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "        \n",
        "        # now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
        "        \n",
        "        # 3. putting word into tok2id lookup table\n",
        "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
        "        tok2id[NULL] = self.NULL = len(tok2id)\n",
        "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "        \n",
        "        # now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
        "        \n",
        "        # creating id2tok\n",
        "        self.tok2id = tok2id\n",
        "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "        \n",
        "        self.n_features = 18 + 18 + 12\n",
        "        self.n_tokens = len(tok2id)\n",
        "\n",
        "                \n",
        "    # utility function, in case we want to convert token to id\n",
        "    # function to turn train set with words to train set with id instead using tok2id\n",
        "    def numericalize(self, examples):\n",
        "        numer_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "            pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "            dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
        "                            else -1 for w in ex['dep']]\n",
        "            numer_examples.append({'word': word, 'pos': pos,\n",
        "                                 'head': head, 'dep': dep})\n",
        "        return numer_examples\n",
        "            \n",
        "    # function to extract features to form a feature embedding matrix\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "             \n",
        "        # ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
        "        # ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
        "        # ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
        "        # ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
        "\n",
        "        # stack     :  [0]\n",
        "        # buffer    :  [1, 2, 3, 4, 5]\n",
        "        \n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0  # starting the stack with [ROOT]\n",
        "            \n",
        "        p_features = [] # pos features (2a, 2b, 2c) - 18\n",
        "        d_features = [] # dep features (3b, 3c) - 12\n",
        "        \n",
        "        # last 3 things on the stack as features\n",
        "        # if the stack is less than 3, then we simply append NULL from the left\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        \n",
        "        # next 3 things on the buffer as features\n",
        "        # if the buffer is less than 3, simply appending NULL\n",
        "        # the reason why NULL is appended on end because buffer is read left to right\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        \n",
        "        # corresponding pos tags\n",
        "        p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "        p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "             \n",
        "        # getting leftmost children based on the dependency arcs\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        # getting right most children based on the dependency arcs\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "        \n",
        "        # getting the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
        "        for i in range(2):\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1] # -1, -2 last two in the stack\n",
        "                \n",
        "                # the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
        "                lc = get_lc(k)  \n",
        "                rc = get_rc(k)\n",
        "                \n",
        "                # the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "\n",
        "                # leftmost of first word on stack, rightmost of first word, \n",
        "                # leftmost of the second word on stack, rightmost of second, \n",
        "                # leftmost of leftmost, rightmost of rightmost\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                # corresponding pos\n",
        "                p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "\n",
        "                        \n",
        "                # corresponding dep\n",
        "                d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
        "                \n",
        "            else:\n",
        "                # attaching NULL when they don't exist\n",
        "                features += [self.NULL] * 6\n",
        "                p_features += [self.P_NULL] * 6\n",
        "                d_features += [self.D_NULL] * 6\n",
        "                \n",
        "        features += p_features + d_features\n",
        "        assert len(features) == self.n_features  # asserting they are 18 + 18 + 12\n",
        "        \n",
        "        return features\n",
        "      \n",
        "    # generating training examples\n",
        "    # from the training sentences and their gold parse trees \n",
        "    def create_instances(self, examples):  # examples = word, pos, head, dep\n",
        "        all_instances = []\n",
        "        \n",
        "        for i, ex in enumerate(examples):\n",
        "            # Ms. Haag plays Elianti .\n",
        "            # e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
        "            # here 344 stands for ROOT\n",
        "            # Chaky - I cheated and take a look\n",
        "            n_words = len(ex['word']) - 1  #excluding the root\n",
        "            \n",
        "            # arcs = {(head, tail, dependency label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]  # [1, 2, 3, 4, 5]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            \n",
        "            # because that's the maximum number of shift, leftarcs, rightarcs we can have\n",
        "            # this will determine the sample size of each training example\n",
        "            # if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
        "            # but this for loop can be break if there is nothing left....\n",
        "            for i in range(n_words * 2):  # maximum times we can do either S, L, R\n",
        "                \n",
        "                # getting the gold transition based on the parse trees\n",
        "                # gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                \n",
        "                # if gold_t is None, no need to extract features.....\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                \n",
        "                # making sure when the model predicts, we inform the current state of stack and buffer, so\n",
        "                # the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
        "                legal_labels = self.legal_labels(stack, buf)                \n",
        "                assert legal_labels[gold_t] == 1\n",
        "                \n",
        "                # extracting all the 48 features \n",
        "                features = self.extract_features(stack, buf, arcs, ex)\n",
        "                instances.append((features, legal_labels, gold_t))\n",
        "                \n",
        "                # shift \n",
        "                if gold_t == 2:\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                # left arc \n",
        "                elif gold_t == 0:\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                # right arc\n",
        "                else:\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "                    \n",
        "            else:\n",
        "                all_instances += instances\n",
        "\n",
        "        return all_instances\n",
        "\n",
        "    # providing an one hot encoding of the labels\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel  # left arc but you cannot do ROOT <--- He\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel  # right arc because ROOT --> He\n",
        "        labels += [1] if len(buf) > 0 else [0]  #shift\n",
        "        return labels\n",
        "    \n",
        "    # a simple function to check punctuation POS tags\n",
        "    def punct(self, pos):\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    \n",
        "    # deciding whether to shift, leftarc, or rightarc, based on gold parse trees\n",
        "    # this is needed to create training examples which contain samples and ground truth\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        \n",
        "        # leaving if the stack is only 1, thus nothing to predict....\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "        \n",
        "        # predicting based on the last two words on the stack\n",
        "        # stack: [ROOT, he, has]\n",
        "        i0 = stack[-1] # has\n",
        "        i1 = stack[-2] # he\n",
        "        \n",
        "        # getting the head and dependency\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "        d0 = ex['dep'][i0]\n",
        "        d1 = ex['dep'][i1]\n",
        "        \n",
        "        # either shift, left arc or right arc\n",
        "        # \"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
        "        # if head of the second last word is the last word, then leftarc\n",
        "        if (i1 > 0) and (h1 == i0):\n",
        "            return 0  # action is left arc ---> gold_t\n",
        "        # if head of the last word is the second last word, then rightarc\n",
        "        # making sure nothing in the buffer has head with the last word on the stack\n",
        "        # otherwise, we lose the last word.....\n",
        "        elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "            return 1  # right arc\n",
        "        # otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
        "        else:\n",
        "            return None if len(buf) == 0 else 2  # shift         \n",
        "        \n",
        "        \n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        \n",
        "        for i, example in enumerate(dataset):\n",
        "            \n",
        "            # example['word']=[188, 186, 186, ..., 59]\n",
        "            # n_words=37\n",
        "            # sentence=[1, 2, 3, 4, 5,.., 37]\n",
        "            \n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]            \n",
        "            sentences.append(sentence)\n",
        "            \n",
        "            # mapping the object unique id to the i            \n",
        "            # The id is the object's memory address\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "            \n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "        \n",
        "        UAS = all_tokens = 0.0\n",
        "        with tqdm(total=len(dataset)) as prog:\n",
        "            for i, ex in enumerate(dataset):\n",
        "                head = [-1] * len(ex['word'])\n",
        "                for h, t, in dependencies[i]:\n",
        "                    head[t] = h\n",
        "                for pred_h, gold_h, gold_l, pos in \\\n",
        "                        zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
        "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                        if (not self.punct(pos_str)):\n",
        "                            UAS += 1 if pred_h == gold_h else 0\n",
        "                            all_tokens += 1\n",
        "                prog.update(i + 1)\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies\n"
      ],
      "metadata": {
        "id": "-LGdeYv5PUgJ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        mb_x = torch.from_numpy(mb_x).long()\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "\n",
        "        pred = self.parser.model(mb_x)\n",
        "        pred = pred.detach().numpy()\n",
        "        \n",
        "        # we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
        "        # other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        \n",
        "        return pred"
      ],
      "metadata": {
        "id": "S8A2vK1kOaDX"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple function to create ids.....\n",
        "def build_dict(keys, offset=0):\n",
        "    # keys = ['P_PREFIX:IN', 'P_PREFIX:DT', 'P_PREFIX:NNP', 'P_PREFIX:CD', so on...]\n",
        "    # offset is needed because this tok2id has something already inside....\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    \n",
        "    # most_common = [('P_PREFIX:NN', 70), ('P_PREFIX:IN', 57), ... , ('P_PREFIX:JJR', 1)]\n",
        "    # we use most_common in case we only want some maximum pos tags....\n",
        "    mc = count.most_common()\n",
        "    \n",
        "    # {'P_PREFIX:NN': 31, 'P_PREFIX:IN': 32, .., 'P_PREFIX:JJR': 62} \n",
        "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
      ],
      "metadata": {
        "id": "47iP_F5NOZ9r"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parser"
      ],
      "metadata": {
        "id": "Ggj8ZwCAP4vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting only word out from the train_set\n",
        "corpus = [word['word'] for word in train_set]"
      ],
      "metadata": {
        "id": "DBT5Eri5VvlO"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the corpus\n",
        "# print(corpus[1])\n",
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6blquAhIV0hL",
        "outputId": "cd8f0abf-ee41-408b-bc84-facfcc5bcea5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "vocabs  = list(set(flatten(corpus)))"
      ],
      "metadata": {
        "id": "cNE1iCrYOaIQ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking vocab size\n",
        "voc_size = len(vocabs)"
      ],
      "metadata": {
        "id": "LopVQ7X4WDIa"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(voc_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SmqTrJ8WLL4",
        "outputId": "f3a277cf-4ee0-4d56-b9f6-4bfc70182b59"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2. Building parser\")\n",
        "start = time.time()\n",
        "embed_size = 2\n",
        "parser = Parser_Embedding(train_set, voc_size, embed_size)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDy4sqITPha_",
        "outputId": "766c3391-e6aa-4639-8b51-c9a27a4bf9f6"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Building parser\n",
            "took 0.04 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# before numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kti9niGgPhV6",
        "outputId": "eac1a633-f80f-4d3d-d2cb-5ec4cc0f3069"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
            "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
            "Head:  [2, 3, 0, 3, 3]\n",
            "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = parser.numericalize(train_set)\n",
        "dev_set = parser.numericalize(dev_set)\n",
        "test_set = parser.numericalize(test_set)"
      ],
      "metadata": {
        "id": "h56o0i84PhQO"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q1zM_tAPhLP",
        "outputId": "27581778-2e85-4ca3-aa33-9e77dd1c615f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
            "Pos:  [84, 42, 42, 55, 42, 46]\n",
            "Head:  [-1, 2, 3, 0, 3, 3]\n",
            "Dep:  [-1, 30, 8, 0, 20, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Embedding"
      ],
      "metadata": {
        "id": "O7jdKw2pRnQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4. Loading pretrained embeddings...\",)\n",
        "start = time.time()\n",
        "embeddings_index = {}\n",
        "for line in open(\"/content/en-cw.txt\").readlines():\n",
        "    we = line.strip().split() # we = word embeddings - first column: word;  the rest is embedding\n",
        "    word_vectors[we[0]] = [float(x) for x in we[1:]] # {word: [list of 50 numbers], nextword: [another list], so on...}\n",
        "    \n",
        "# creating an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
        "# we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
        "# embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 2)), dtype='float32')\n",
        "\n",
        "## center_embed = nn.Embedding(voc_size, embed_size)\n",
        "## out_embed = nn.Embedding(voc_size, embed_size)\n",
        "\n",
        "for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_index[token] = word_vectors\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_index[token.lower()] = word_vectors\n",
        "print('Found {} word vectors in glove.'.format(len(embeddings_index)))\n",
        "\n",
        "embedding_matrix = np.zeros((voc_size, embed_size))\n",
        "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFqkefZXPhF_",
        "outputId": "4652e024-9e85-4550-ae21-ffcb7a9257a5"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Loading pretrained embeddings...\n",
            "Found 4418 word vectors in glove.\n",
            "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
            "took 1.88 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "YraZESKt66Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5. Preprocessing training data...\",)\n",
        "start = time.time()\n",
        "train_examples = parser.create_instances(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov2InAgdPg_R",
        "outputId": "cb4b6dc7-9837-484f-f6a9-59143013692c"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Preprocessing training data...\n",
            "took 2.02 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_examples[0]"
      ],
      "metadata": {
        "id": "Fsd5cMKy-dsA"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minibatch Loader"
      ],
      "metadata": {
        "id": "Q2l0qCcZMENx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    data_size = len(data[0])\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)"
      ],
      "metadata": {
        "id": "MIjGH0AhPg1Z"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Minibatch Loader"
      ],
      "metadata": {
        "id": "DKqn778UMN4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (train_x, train_y) in enumerate(minibatches(train_examples, 1024)):\n",
        "#     print(train_x.shape)  #batch size, features\n",
        "#     print(train_y.shape)        #one hot encoding of 3 actions - shift, la, ra"
      ],
      "metadata": {
        "id": "Gg6r2ff_MJUA"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "2xcH2A05MZrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParserModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, n_features=48,\n",
        "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
        "\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        # t:  batch_size, n_features\n",
        "        batch_size = t.size()[0]\n",
        "                    \n",
        "        x = self.pretrained_embeddings(t)        \n",
        "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
        "        # x = (1024, 48 * 50)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t: (1024, 48)\n",
        "        embeddings = self.embedding_lookup(t)  \n",
        "    \n",
        "        # embeddings: (1024, 48 * 50)\n",
        "        hidden = self.embed_to_hidden(embeddings)\n",
        "    \n",
        "        # hidden: (1024, 200)\n",
        "        hidden_activations = F.relu(hidden)\n",
        "        # hidden_activations: (1024, 200)\n",
        "        thin_net = self.dropout(hidden_activations)\n",
        "        # thin_net: (1024, 200)\n",
        "        logits = self.hidden_to_logits(thin_net)\n",
        "        # logits: (1024, 3)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "wRz7fzrvMJPD"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just a class to get the average.....\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "H1VJZ4E4MJKY"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \n",
        "    best_dev_UAS = 0\n",
        "    \n",
        "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(\n",
        "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \n",
        "    parser.model.train()  # places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            \n",
        "            # train_x:  batch_size, n_features\n",
        "            # train_y:  batch_size, target(=3)\n",
        "            \n",
        "            optimizer.zero_grad() \n",
        "            loss = 0.\n",
        "            train_x = torch.from_numpy(train_x).long()  # long() for int so embedding works....\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  # getting the index with 1 because torch expects label to be single integer\n",
        "\n",
        "            # forward pass: computing predicted logits.\n",
        "            logits = parser.model(train_x)\n",
        "            # computing loss\n",
        "            loss = loss_func(logits, train_y)\n",
        "            # computing gradients of the loss w.r.t model parameters.\n",
        "            loss.backward()\n",
        "            # taking step with optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    parser.model.eval()  # places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "        \n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS"
      ],
      "metadata": {
        "id": "WHtd8neYMJE4"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "pjy9A1h2M5N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating directory if it does not exist for saving the weights...\n",
        "output_dir = \"glove_embed_output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    \n",
        "print(80 * \"=\")\n",
        "print(\"TRAINING\")\n",
        "print(80 * \"=\")\n",
        "    \n",
        "model = ParserModel(embeddings_matrix)\n",
        "parser.model = model\n",
        "\n",
        "start = time.time()\n",
        "train(parser, train_examples, dev_set, output_path,\n",
        "      batch_size=1024, n_epochs=10, lr=0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-nWHJ0zMI-P",
        "outputId": "bf57a1fd-490f-4f2b-a660-f6064f8d8a86"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.6008115739872059\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 8204153.73it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 55.83\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.3062425156434377\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7198462.24it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 64.15\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:07<00:00,  6.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.24813879964252314\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7703447.11it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 66.40\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.2144493522743384\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 3752591.74it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 68.97\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.18896177038550377\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7452322.59it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 69.34\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 6 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.17132752450803915\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7631490.98it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 71.03\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.15586361878861985\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7797203.35it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 72.98\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.14002978398154178\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7172809.61it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 72.78\n",
            "\n",
            "Epoch 9 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.13353691420828304\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7602335.33it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 74.30\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 10 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.11780394722397129\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7552714.01it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 74.40\n",
            "New best dev UAS! Saving model.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "jyr8D-hO32rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(80 * \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80 * \"=\")\n",
        "\n",
        "print(\"Restoring the best model weights found on the dev set\")\n",
        "parser.model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evaluation on test set\",)\n",
        "parser.model.eval()\n",
        "UAS, dependencies = parser.parse(test_set)\n",
        "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMPJHgu4MI1Q",
        "outputId": "eb6d4e80-3450-4234-aa6d-5c2b68d3eac5"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "Final evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7452851.21it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- test UAS: 75.66\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Embedding"
      ],
      "metadata": {
        "id": "1iN4W8tQ5rHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4. Loading pretrained embeddings...\",)\n",
        "start = time.time()\n",
        "embeddings_index = {}\n",
        "for line in open(\"/content/en-cw.txt\").readlines():\n",
        "    we = line.strip().split() # we = word embeddings - first column: word;  the rest is embedding\n",
        "    word_vectors[we[0]] = [float(x) for x in we[1:]] # {word: [list of 50 numbers], nextword: [another list], so on...}\n",
        "    \n",
        "# creating an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
        "# we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
        "# embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 2)), dtype='float32')\n",
        "\n",
        "## center_embed = nn.Embedding(voc_size, embed_size)\n",
        "## out_embed = nn.Embedding(voc_size, embed_size)\n",
        "\n",
        "for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_index[token] = word_vectors\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_index[token.lower()] = word_vectors\n",
        "print('Found {} word vectors in glove.'.format(len(embeddings_index)))\n",
        "\n",
        "embedding_matrix = nn.Embedding(voc_size, embed_size)  ## using nn.Embedding\n",
        "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEeOfrI45ppY",
        "outputId": "6b887974-6087-449b-f9bb-a96a91adfbce"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Loading pretrained embeddings...\n",
            "Found 4418 word vectors in glove.\n",
            "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
            "took 1.98 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "s-MLSQ117IdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5. Preprocessing training data...\",)\n",
        "start = time.time()\n",
        "train_examples = parser.create_instances(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7jhR4UI6v-h",
        "outputId": "4574d048-9e2b-45e8-e15d-d467a83e7cdd"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Preprocessing training data...\n",
            "took 1.95 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_examples[0]"
      ],
      "metadata": {
        "id": "3Sto4Qao6vsl"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minibatch Loader"
      ],
      "metadata": {
        "id": "vucouPvg7pDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    data_size = len(data[0])\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)"
      ],
      "metadata": {
        "id": "ayJIq_DY6vkK"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Minibatch Loader"
      ],
      "metadata": {
        "id": "YYV-tIxA7teO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (train_x, train_y) in enumerate(minibatches(train_examples, 1024)):\n",
        "#     print(train_x.shape)  #batch size, features\n",
        "#     print(train_y.shape)        #one hot encoding of 3 actions - shift, la, ra"
      ],
      "metadata": {
        "id": "UAqi6ABY6va3"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "tcCWaeiO8CSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParserModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, n_features=48,\n",
        "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
        "\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        # t:  batch_size, n_features\n",
        "        batch_size = t.size()[0]\n",
        "                    \n",
        "        x = self.pretrained_embeddings(t)        \n",
        "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
        "        # x = (1024, 48 * 50)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t: (1024, 48)\n",
        "        embeddings = self.embedding_lookup(t)  \n",
        "    \n",
        "        # embeddings: (1024, 48 * 50)\n",
        "        hidden = self.embed_to_hidden(embeddings)\n",
        "    \n",
        "        # hidden: (1024, 200)\n",
        "        hidden_activations = F.relu(hidden)\n",
        "        # hidden_activations: (1024, 200)\n",
        "        thin_net = self.dropout(hidden_activations)\n",
        "        # thin_net: (1024, 200)\n",
        "        logits = self.hidden_to_logits(thin_net)\n",
        "        # logits: (1024, 3)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ZMJEbhYn70LQ"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just a class to get the average.....\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "7wHhU03z7z_x"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \n",
        "    best_dev_UAS = 0\n",
        "    \n",
        "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(\n",
        "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \n",
        "    parser.model.train()  # places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            \n",
        "            # train_x:  batch_size, n_features\n",
        "            # train_y:  batch_size, target(=3)\n",
        "            \n",
        "            optimizer.zero_grad() \n",
        "            loss = 0.\n",
        "            train_x = torch.from_numpy(train_x).long()  # long() for int so embedding works....\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  # getting the index with 1 because torch expects label to be single integer\n",
        "\n",
        "            # forward pass: computing predicted logits.\n",
        "            logits = parser.model(train_x)\n",
        "            # computing loss\n",
        "            loss = loss_func(logits, train_y)\n",
        "            # computing gradients of the loss w.r.t model parameters.\n",
        "            loss.backward()\n",
        "            # taking step with optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    parser.model.eval()  # places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "        \n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS"
      ],
      "metadata": {
        "id": "YvX8oZCe8oEO"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "9TpGTwCz85al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating directory if it does not exist for saving the weights...\n",
        "output_dir = \"nn_embed_output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    \n",
        "print(80 * \"=\")\n",
        "print(\"TRAINING\")\n",
        "print(80 * \"=\")\n",
        "    \n",
        "model = ParserModel(embeddings_matrix)\n",
        "parser.model = model\n",
        "\n",
        "start = time.time()\n",
        "train(parser, train_examples, dev_set, output_path,\n",
        "      batch_size=1024, n_epochs=10, lr=0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faAO8W_x8cvP",
        "outputId": "13572649-e702-4781-89a2-b75bea8f22ab"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  8.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.5845959757765135\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7443980.28it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 57.36\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.30090800393372774\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7294922.88it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 63.84\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  8.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.24357639408359924\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 6728873.04it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 66.21\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.21169265607992807\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 6126588.41it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 68.77\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:06<00:00,  7.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.18666324267784754\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7741248.06it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 70.48\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 6 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.16970236940930286\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 6702089.41it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 70.70\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1520457724109292\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7454120.21it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 71.44\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.13815560626486936\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 5065926.48it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 73.64\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 9 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1241252653611203\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7546204.55it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 75.28\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 10 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 48/48 [00:05<00:00,  9.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1112862986822923\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7910027.64it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 74.42\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "vKpIf-ke8_QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(80 * \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80 * \"=\")\n",
        "\n",
        "print(\"Restoring the best model weights found on the dev set\")\n",
        "parser.model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evaluation on test set\",)\n",
        "parser.model.eval()\n",
        "UAS, dependencies = parser.parse(test_set)\n",
        "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO6HEgdI8cky",
        "outputId": "f7f10e51-869b-469f-ed23-7740624b277e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "Final evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7647154.55it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- test UAS: 75.93\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using SpaCy"
      ],
      "metadata": {
        "id": "1zouR4sXlB76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using spacy\n",
        "## testing sentences with spacy\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy # displacy is for visualization\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc_1 = nlp(\"Ms. Haag plays Elianti.\")\n",
        "options = {\"collapse_punct\": False}\n",
        "\n",
        "displacy.render(doc_1, options = options, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "YoFG8tGTlBoG",
        "outputId": "ec6f961d-5060-49ed-d57c-69eb03fa4902"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c2c7654caf79437d84abb9c6084801d2-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Ms.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Haag</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">plays</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Elianti</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c2c7654caf79437d84abb9c6084801d2-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c2c7654caf79437d84abb9c6084801d2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c2c7654caf79437d84abb9c6084801d2-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c2c7654caf79437d84abb9c6084801d2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c2c7654caf79437d84abb9c6084801d2-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c2c7654caf79437d84abb9c6084801d2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c2c7654caf79437d84abb9c6084801d2-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c2c7654caf79437d84abb9c6084801d2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc_2 = nlp(\"Mr. Lin does not like playing Elianti.\")\n",
        "options = {\"collapse_punct\": False}\n",
        "\n",
        "displacy.render(doc_2, options = options, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "M0JxeJ_jlBUq",
        "outputId": "a67a4b25-b4c3-4279-f65c-1c5c3a6b24d3"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9e898320301e446cb50693a2118c1e30-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Mr.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Lin</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">does</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">not</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">like</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">playing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Elianti</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9e898320301e446cb50693a2118c1e30-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9e898320301e446cb50693a2118c1e30-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9e898320301e446cb50693a2118c1e30-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 750.0,2.0 750.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9e898320301e446cb50693a2118c1e30-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9e898320301e446cb50693a2118c1e30-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9e898320301e446cb50693a2118c1e30-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9e898320301e446cb50693a2118c1e30-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9e898320301e446cb50693a2118c1e30-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9e898320301e446cb50693a2118c1e30-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9e898320301e446cb50693a2118c1e30-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9e898320301e446cb50693a2118c1e30-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9e898320301e446cb50693a2118c1e30-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9e898320301e446cb50693a2118c1e30-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9e898320301e446cb50693a2118c1e30-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc_3 = nlp(\"Jose does not like playing the piano for the family.\")\n",
        "options = {\"collapse_punct\": False}\n",
        "\n",
        "displacy.render(doc_3, options = options, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "2HWHw6oMlG3j",
        "outputId": "3ad99069-0b4f-4af8-ac8b-b80cf677597f"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4d7a55cb9a3a48fe90f97b52f678f139-0\" class=\"displacy\" width=\"1975\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Jose</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">does</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">not</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">like</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">playing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">piano</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">family</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-5\" stroke-width=\"2px\" d=\"M770,352.0 C770,177.0 1090.0,177.0 1090.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1090.0,354.0 L1098.0,342.0 1082.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-6\" stroke-width=\"2px\" d=\"M770,352.0 C770,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,354.0 L1278.0,342.0 1262.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-8\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,177.0 1615.0,177.0 1615.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1615.0,354.0 L1623.0,342.0 1607.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-9\" stroke-width=\"2px\" d=\"M595,352.0 C595,2.0 1800.0,2.0 1800.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4d7a55cb9a3a48fe90f97b52f678f139-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1800.0,354.0 L1808.0,342.0 1792.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc_4 = nlp(\"The girls initially assume that Mr. Laurence doesnt like anyone playing his piano because Lauries father ran off and married an Italian pianist.\")\n",
        "options = {\"collapse_punct\": False}\n",
        "\n",
        "displacy.render(doc_4, options = options, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "J4lIonw7lGnG",
        "outputId": "d0114468-ecf7-424b-8a7d-a5a296ed02f0"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"30c19561951b450789736d728cda8630-0\" class=\"displacy\" width=\"4600\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">girls</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">initially</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">assume</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">that</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">SCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Mr.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Laurence</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">does</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">nt</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">like</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">anyone</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">playing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">his</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">piano</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">because</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">SCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">Laurie</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">s</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">father</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">ran</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">off</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">married</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">an</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">Italian</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">pianist</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,439.5 200.0,439.5 200.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-1\" stroke-width=\"2px\" d=\"M245,527.0 C245,352.0 555.0,352.0 555.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,529.0 L237,517.0 253,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-2\" stroke-width=\"2px\" d=\"M420,527.0 C420,439.5 550.0,439.5 550.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,529.0 L412,517.0 428,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-3\" stroke-width=\"2px\" d=\"M770,527.0 C770,177.0 1615.0,177.0 1615.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,529.0 L762,517.0 778,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-4\" stroke-width=\"2px\" d=\"M945,527.0 C945,439.5 1075.0,439.5 1075.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,529.0 L937,517.0 953,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-5\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,264.5 1610.0,264.5 1610.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,529.0 L1112,517.0 1128,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-6\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,352.0 1605.0,352.0 1605.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,529.0 L1287,517.0 1303,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-7\" stroke-width=\"2px\" d=\"M1470,527.0 C1470,439.5 1600.0,439.5 1600.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,529.0 L1462,517.0 1478,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-8\" stroke-width=\"2px\" d=\"M595,527.0 C595,89.5 1620.0,89.5 1620.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1620.0,529.0 L1628.0,517.0 1612.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-9\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,439.5 1775.0,439.5 1775.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1775.0,529.0 L1783.0,517.0 1767.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-10\" stroke-width=\"2px\" d=\"M1820,527.0 C1820,439.5 1950.0,439.5 1950.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1950.0,529.0 L1958.0,517.0 1942.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-11\" stroke-width=\"2px\" d=\"M2170,527.0 C2170,439.5 2300.0,439.5 2300.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2170,529.0 L2162,517.0 2178,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-12\" stroke-width=\"2px\" d=\"M1995,527.0 C1995,352.0 2305.0,352.0 2305.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2305.0,529.0 L2313.0,517.0 2297.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-13\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,264.5 3185.0,264.5 3185.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-14\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,352.0 3005.0,352.0 3005.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2695,529.0 L2687,517.0 2703,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-15\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,439.5 2825.0,439.5 2825.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2825.0,529.0 L2833.0,517.0 2817.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-16\" stroke-width=\"2px\" d=\"M3045,527.0 C3045,439.5 3175.0,439.5 3175.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3045,529.0 L3037,517.0 3053,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-17\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,177.0 3190.0,177.0 3190.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3190.0,529.0 L3198.0,517.0 3182.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-18\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,439.5 3350.0,439.5 3350.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3350.0,529.0 L3358.0,517.0 3342.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-19\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,352.0 3530.0,352.0 3530.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3530.0,529.0 L3538.0,517.0 3522.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-20\" stroke-width=\"2px\" d=\"M3220,527.0 C3220,264.5 3710.0,264.5 3710.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3710.0,529.0 L3718.0,517.0 3702.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-21\" stroke-width=\"2px\" d=\"M3920,527.0 C3920,352.0 4230.0,352.0 4230.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3920,529.0 L3912,517.0 3928,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-22\" stroke-width=\"2px\" d=\"M4095,527.0 C4095,439.5 4225.0,439.5 4225.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4095,529.0 L4087,517.0 4103,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-23\" stroke-width=\"2px\" d=\"M3745,527.0 C3745,264.5 4235.0,264.5 4235.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4235.0,529.0 L4243.0,517.0 4227.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-30c19561951b450789736d728cda8630-0-24\" stroke-width=\"2px\" d=\"M595,527.0 C595,2.0 4425.0,2.0 4425.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-30c19561951b450789736d728cda8630-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M4425.0,529.0 L4433.0,517.0 4417.0,517.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report\n",
        "\n",
        "### Ablation Study with POS and DEP Features\n",
        "\n",
        "The results from the ablation study by removing DEP featues and POS features are summarized in the following table:\n",
        "\n",
        "|        Features           | Train Accuracy | Test Accuracy |\n",
        "|         :----:            |     :---:      |    :-------:  |\n",
        "| 18 word + 18 pos + 12 dep |    74.90%      |    76.09%     |\n",
        "| 18 word + 18 pos          |    41.13%      |    42.38%     |\n",
        "| 18 word + 12 dep          |    40.85%      |    41.77%     |\n",
        "\n",
        "The result shows that the accuracy is nearly 40% when we remove either DEP features or POS features, indicating that both features are equally important. We could obtain the highest accuracy with we used all the features. \n",
        "\n",
        "### Comparison Study with GloVe Embedding and nn.Embedding\n",
        "\n",
        "The results from the comparison study using GloVe Embedding and nn.Embedding are summarized in the following table:\n",
        "\n",
        "\n",
        "|        Embeddings         | Train Accuracy | Test Accuracy |\n",
        "|         :----:            |     :---:      |    :-------:  |\n",
        "| Word Embedding (Default)  |    74.90%      |    76.09%     |\n",
        "|   GloVe Embedding         |    74.40%      |    75.66%     |\n",
        "|   nn.Embedding            |    74.42%      |    75.93%     |\n",
        "\n",
        "The result shows that the accuracy is a bit low and does not change much with GloVe Embedding and nn.Embedding. \n",
        "\n",
        "Lastly, we tried to compare 2-3 sentences using SpaCy to check if we get the same dependency."
      ],
      "metadata": {
        "id": "4FpyMIvf95mI"
      }
    }
  ]
}