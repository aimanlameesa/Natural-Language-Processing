{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK3xwZM0LQH_",
        "outputId": "f63aa424-4fb5-446a-f641-e4a6bbae4105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTUKzT7o_a0p",
        "outputId": "4d92ff0e-1994-4261-85a8-e5241c4e7486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-26 13:52:01.630023: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-26 13:52:02.715988: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 13:52:02.716129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 13:52:02.716149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.22.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (23.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Pq7mXZ28gvQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext, datasets, math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# making this program comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-GIkPpLIQv",
        "outputId": "cc5c740a-6d0e-467a-b9e6-a2d8183a0bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Loading Data"
      ],
      "metadata": {
        "id": "y3TiQflZLvBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
        "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
        "\n",
        "# using \"github jupyter code to text\" dataset\n",
        "train = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"train\")\n",
        "test = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"test\")\n",
        "print(train, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py7mQJ0wL05O",
        "outputId": "1584d48c-eabd-455a-d308-05bd38b2df0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 47452\n",
            "}) Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 11864\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['content'][10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TARhPe2L1Mz",
        "outputId": "71b6976e-a6c9-456f-d84a-6ded7b5a2172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import pylearn2.utils\n",
            "import pylearn2.config\n",
            "import theano\n",
            "import neukrill_net.dense_dataset\n",
            "import neukrill_net.utils\n",
            "import numpy as np\n",
            "%matplotlib inline\n",
            "import matplotlib.pyplot as plt\n",
            "import holoviews as hl\n",
            "%load_ext holoviews.ipython\n",
            "import sklearn.metrics\n",
            "\n",
            "cd ..\n",
            "\n",
            "settings = neukrill_net.utils.Settings(\"settings.json\")\n",
            "run_settings = neukrill_net.utils.load_run_settings(\n",
            "    \"run_settings/replicate_8aug.json\", settings, force=True)\n",
            "\n",
            "model = pylearn2.utils.serial.load(run_settings['alt_picklepath'])\n",
            "\n",
            "c = 'train_objective'\n",
            "channel = model.monitor.channels[c]\n",
            "\n",
            "\"\"\"\n",
            "Explanation: The following are the results we've got from online augmentation so far. Some bugs have been fixed by Scott since then so these might be redundant. If they're not redundant then they are very bad.\n",
            "Loading the pickle\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "plt.title(c)\n",
            "plt.plot(channel.example_record,channel.val_record)\n",
            "\n",
            "c = 'train_y_nll'\n",
            "channel = model.monitor.channels[c]\n",
            "plt.title(c)\n",
            "plt.plot(channel.example_record,channel.val_record)\n",
            "\n",
            "def plot_monitor(c = 'valid_y_nll'):\n",
            "    channel = model.monitor.channels[c]\n",
            "    plt.title(c)\n",
            "    plt.plot(channel.example_record,channel.val_record)\n",
            "    return None\n",
            "plot_monitor()\n",
            "\n",
            "plot_monitor(c=\"valid_objective\")\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Replicating 8aug\n",
            "The DensePNGDataset run with 8 augmentations got us most of the way to our best score in one go. If we can replicate that results with online augmentation then we can be pretty confident that online augmentation is a good idea. Unfortunately, it looks like we can't:\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "%run check_test_score.py run_settings/replicate_8aug.json\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Would actually like to know what kind of score this model gets on the check_test_score script.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "run_settings = neukrill_net.utils.load_run_settings(\n",
            "    \"run_settings/online_manyaug.json\", settings, force=True)\n",
            "\n",
            "model = pylearn2.utils.serial.load(run_settings['alt_picklepath'])\n",
            "\n",
            "plot_monitor(c=\"valid_objective\")\n",
            "\n",
            "\"\"\"\n",
            "Explanation: So we can guess that the log loss score we're seeing is in fact correct. There are definitely some bugs in the ListDataset code.\n",
            "Many Augmentations\n",
            "We want to be able to use online augmentations to run large combinations of different augmentations on the images. This model had almost everything turned on, a little:\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "settings = neukrill_net.utils.Settings(\"settings.json\")\n",
            "run_settings = neukrill_net.utils.load_run_settings(\n",
            "    \"run_settings/alexnet_based_onlineaug.json\", settings, force=True)\n",
            "\n",
            "model = pylearn2.utils.serial.load(run_settings['pickle abspath'])\n",
            "\n",
            "plot_monitor(c=\"train_y_nll\")\n",
            "\n",
            "plot_monitor(c=\"valid_y_nll\")\n",
            "\n",
            "plot_monitor(c=\"train_objective\")\n",
            "\n",
            "plot_monitor(c=\"valid_objective\")\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Looks like it's completely incapable of learning.\n",
            "These problems suggest that the augmentation might be garbling the images; making them useless for learning from. Or worse, garbling the order so each image doesn't correspond to its label.\n",
            "Transformer Results\n",
            "We also have results from a network trained using a Transformer dataset, which is how online augmentation is supposed to be supported in Pylearn2.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test['content'][10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMAM3Czh6h29",
        "outputId": "338e0599-bc46-4d76-d376-133d5a3cec93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "%matplotlib inline\n",
            "\n",
            "from sklearn.preprocessing import Imputer\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.cross_validation import train_test_split as tts\n",
            "\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.svm import SVC \n",
            "\n",
            "from imblearn.over_sampling import SMOTE\n",
            "from imblearn.pipeline import Pipeline\n",
            "from sklearn.metrics import roc_curve, auc\n",
            "\n",
            "from __future__ import division\n",
            "import warnings\n",
            "warnings.filterwarnings(\"ignore\")\n",
            "\n",
            "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
            "secom = pd.read_table(url, header=None, delim_whitespace=True)\n",
            "\n",
            "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
            "y = pd.read_table(url, header=None, usecols=[0], squeeze=True, delim_whitespace=True)\n",
            "\n",
            "print 'The dataset has {} observations/rows and {} variables/columns.' \\\n",
            "       .format(secom.shape[0], secom.shape[1])\n",
            "print 'The ratio of majority class to minority class is {}:1.' \\\n",
            "      .format(int(y[y == -1].size/y[y == 1].size))\n",
            "\n",
            "\"\"\"\n",
            "Explanation: SVM classification/SMOTE oversampling for an imbalanced data set\n",
            "Date created: Oct 14, 2016 \n",
            "Last modified: Nov 16, 2016\n",
            "Tags: SVM, SMOTE, ROC/AUC, oversampling, imbalanced data set, semiconductor data \n",
            "About: Rebalance imbalanced semicondutor manufacturing dataset by oversampling the minority class using SMOTE. Classify using SVM. Assess the value of oversampling using ROC/AUC. \n",
            "<h3>I. Introduction</h3>\n",
            "\n",
            "The SECOM dataset in the  UCI Machine Learning Repository is semicondutor manufacturing data. There are 1567 records, 590 anonymized features and 104 fails. This makes it an imbalanced dataset with a 14:1 ratio of pass to fails. The process yield has a simple pass/fail response (encoded -1/1).\n",
            "<h4>Objective</h4>\n",
            "We consider some of the different approaches to classify imbalanced data. In the previous example we looked at one-class SVM.\n",
            "Another strategy is to rebalance the dataset by oversampling the minority class and/or undersampling the majority class. This is done to improve the sensitivity (i.e the true positive rate) of the minority class. For this exercise, we will look at: \n",
            "- rebalancing the dataset using SMOTE (which oversamples the minority class) \n",
            "- ROC curves for different oversampling ratios\n",
            "<h4>Methodology</h4>\n",
            "The sklearn imblearn toolbox has many methods for oversamplng/undersampling. We will use the SMOTE (Synthetic Minority Over-sampling Technique) method introduced in 2002 by Chawla et al. <a href=\"#ref1\">[1]</a>, <a href=\"#ref2\">[2]</a>. With SMOTE, synthetic examples are interpolated along the line segments joining some/all of the <i>k</i> minority class nearest neighbors.\n",
            "In the experiment, the  oversampling rate is varied between 10-70%, in 10% increments. The percentage represents the final minority class fraction after oversampling: if the majority class has 1000 data points (and the minority class 50), at 10% the minority class will have 100 data points after oversampling (not 5 or 50+5 = 55). \n",
            "The rebalanced data is classified using an SVM. The imblearn toolbox has a pipeline method which will be used to chain all the steps. The SMOTE+SVM method is evaluated by the area under the Receiver Operating Characteristic curve (AUC).\n",
            "<h4>Preprocessing</h4>\n",
            "The data represents measurements from a large number of processes or sensors and many of the records are missing. In addition some measurements are identical/constant and so not useful for prediction. We will remove those columns with high missing count or constant values.\n",
            "The Random Forest variable importance is used to rank the variables in terms of their importance. For the random forest, we will impute the remaining missing values with the median for the column. \n",
            "We will additionally scale the data that is applied to the SVM. We will use the <i>sklearn preprocessing</i> module for both imputing and scaling.\n",
            "These are the same steps used for the one-class SVM and a more detailed explanation can be seen there.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# dropping columns which have large number of missing entries \n",
            "\n",
            "m = map(lambda x: sum(secom[x].isnull()), xrange(secom.shape[1]))\n",
            "m_200thresh = filter(lambda i: (m[i] > 200), xrange(secom.shape[1]))\n",
            "secom_drop_200thresh = secom.dropna(subset=[m_200thresh], axis=1)\n",
            "dropthese = [x for x in secom_drop_200thresh.columns.values if \\\n",
            "             secom_drop_200thresh[x].std() == 0]\n",
            "secom_drop_200thresh.drop(dropthese, axis=1, inplace=True)\n",
            "\n",
            "print 'The SECOM data set now has {} variables.'\\\n",
            "      .format(secom_drop_200thresh.shape[1])\n",
            "    \n",
            "\n",
            "# imputing missing values for the random forest\n",
            "\n",
            "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
            "secom_imp = pd.DataFrame(imp.fit_transform(secom_drop_200thresh))\n",
            "\n",
            "# use Random Forest to assess variable importance\n",
            "\n",
            "rf = RandomForestClassifier(n_estimators=100, random_state=7)\n",
            "rf.fit(secom_imp, y)\n",
            "\n",
            "# sorting features according to their rank\n",
            "\n",
            "importance = rf.feature_importances_\n",
            "ranked_indices = np.argsort(importance)[::-1]\n",
            "\n",
            "\n",
            "\"\"\"\n",
            "Explanation: <h3>II. Preprocessing </h3>\n",
            "\n",
            "We process the missing values first, dropping columns which have a large number of missing values and imputing values for those that have only a few missing values.\n",
            "The Random Forest variable importance is used to rank the variables in terms of their importance. The one-class SVM exercise has a more detailed version of these steps.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# split data into train and holdout sets\n",
            "# stratify the sample used for modeling to preserve the class proportions\n",
            "\n",
            "\n",
            "X_train, X_holdout, y_train, y_holdout = tts(secom_imp[ranked_indices[:40]], y, \\\n",
            "                                             test_size=0.2, stratify=y, random_state=5)\n",
            "\n",
            "\n",
            "print 'Train data: The majority/minority class have {} and {} elements respectively.'\\\n",
            "      .format(y_train[y_train == -1].size, y_train[y_train == 1].size)\n",
            "print 'The maj/min class ratio is: {0:2.0f}' \\\n",
            "      .format(round(y_train[y_train == -1].size/y_train[y_train == 1].size))\n",
            "print 'Holdout data: The majority/minority class have {} and {} elements respectively.'\\\n",
            "       .format(y_holdout[y_holdout == -1].size, y_holdout[y_holdout == 1].size)\n",
            "print 'The maj/min class ratio for the holdout set is: {0:2.0f}' \\\n",
            "      .format(round(y_holdout[y_holdout == -1].size/y_holdout[y_holdout == 1].size))\n",
            "    \n",
            "\n",
            "# scaling the split data. The holdout data uses scaling parameters \n",
            "# computed from the training data\n",
            "\n",
            "standard_scaler = StandardScaler()\n",
            "X_train_scaled  = pd.DataFrame(standard_scaler.fit_transform(X_train), \\\n",
            "                              index=X_train.index)\n",
            "X_holdout_scaled = pd.DataFrame(standard_scaler.transform(X_holdout))\n",
            "# Note: we convert to a DataFrame because the plot functions \n",
            "# we will use need DataFrame inputs.\n",
            "\n",
            "\"\"\"\n",
            "Explanation: <h3>III. SVM Classification </h3>\n",
            "\n",
            "<h4> Preprocessing </h4>\n",
            "\n",
            "The SVM is sensitive to feature scale so the first step is to center and normalize the data. The train and test sets are scaled separately using the mean and variance computed from the training data. This is done to estimate the ability of the model to generalize.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# oversampling\n",
            "\n",
            "ratio = 0.5\n",
            "\n",
            "smote = SMOTE(ratio = ratio, kind='regular')\n",
            "smox, smoy = smote.fit_sample(X_train_scaled, y_train)\n",
            "\n",
            "print 'Before resampling: \\n\\\n",
            "The majority/minority class have {} and {} elements respectively.'\\\n",
            ".format(y_train[y == -1].size, y_train[y == 1].size)\n",
            "print 'After oversampling at {}%: \\n\\\n",
            "The majority/minority class have {} and {} elements respectively.'\\\n",
            ".format(ratio, smoy[smoy == -1].size, smoy[smoy == 1].size)\n",
            "\n",
            "\n",
            "# plotting minority class distribution after SMOTE\n",
            "# column 4 displayed\n",
            "\n",
            "from IPython.html.widgets import interact\n",
            "@interact(ratio=[0.1,1.0])\n",
            "\n",
            "def plot_dist(ratio):\n",
            "    sns.set(style=\"white\", font_scale=1.3) \n",
            "    fig, ax = plt.subplots(figsize=(7,5))\n",
            "\n",
            "    smote = SMOTE(ratio = ratio, kind='regular')\n",
            "    smox, smoy = smote.fit_sample(X_train_scaled, y_train)\n",
            "    smox_df = pd.DataFrame(smox)\n",
            "\n",
            "    ax = sns.distplot(smox_df[4][smoy == 1], color='b',  \\\n",
            "                  kde=False, label='after')         \n",
            "    ax = sns.distplot(X_train_scaled[4][y_train == 1], color='r', \\\n",
            "                  kde=False, label='before')\n",
            "    ax.set_ylim([0, 130])\n",
            "    ax.set(xlabel='')\n",
            "    ax.legend(title='Ratio = {}'.format(ratio))\n",
            "    plt.title('Minority class distribution before and after oversampling')\n",
            "\n",
            "    plt.show()\n",
            "\n",
            "\n",
            "# classification results\n",
            "\n",
            "from sklearn.metrics import confusion_matrix, matthews_corrcoef,\\\n",
            "classification_report, roc_auc_score, accuracy_score\n",
            "\n",
            "# manually selected parameters\n",
            "clf = SVC(C = 2, gamma = .0008)\n",
            "clf.fit(smox, smoy)\n",
            "y_predicted = clf.predict(X_holdout_scaled)\n",
            "\n",
            "\n",
            "print 'The accuracy is: {0:4.2} \\n' \\\n",
            ".format(accuracy_score(y_holdout, y_predicted))\n",
            "\n",
            "print 'The confusion matrix: '\n",
            "cm = confusion_matrix(y_holdout, y_predicted)\n",
            "print cm\n",
            "\n",
            "print '\\nThe True Negative rate is: {0:4.2}' \\\n",
            ".format(float(cm[1][1])/np.sum(cm[1]))\n",
            "\n",
            "print '\\nThe Matthews correlation coefficient: {0:4.2f} \\n' \\\n",
            ".format(matthews_corrcoef(y_holdout, y_predicted))\n",
            "\n",
            "print(classification_report(y_holdout, y_predicted))\n",
            "print 'The AUC is: {0:4.2}'\\\n",
            ".format(roc_auc_score(y_holdout, y_predicted))\n",
            "\n",
            "\n",
            "\"\"\"\n",
            "Explanation: <h4> Finding parameters </h4>\n",
            "\n",
            "The usual way to select parameters is via grid-search and cross-validation (CV). The scoring is based on the accuracy. When the classes are imbalanced, the true positive of the majority class dominates. Often, there is a high cost associated with the misclassification of the minority class, and in those cases alternative scoring measures such as the F1 and $F_{\\beta}$ scores or the Matthews Correlation Coefficient (which uses all four values of the confusion matrix) are used. \n",
            "In CV experiments on this data, the majority class still dominates so that for the best CV F1-scores, the True Negative Rate (TNR - the rate at which the minority class is correctly classified) is zero.\n",
            "Instead of automating the selection of hyperparameters, I have manually selected <i>C</i> and $\\gamma$ values for which the precision/recall/F1 values as well as the TNR are high.\n",
            "An example is shown below.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# oversampling, classification and computing ROC values\n",
            "\n",
            "fpr = dict()\n",
            "tpr = dict()\n",
            "roc_auc = dict()\n",
            "\n",
            "ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
            "C =     [3, 3, 3, 2, 2, 2, 2]\n",
            "gamma = [.02, .009, .009, .005, .0008, .0009, .0007]\n",
            "\n",
            "    \n",
            "estimators = [('smt', SMOTE(random_state=42)), \n",
            "              ('clf', SVC(probability=True, random_state=42))]\n",
            "pipe = Pipeline(estimators)\n",
            "\n",
            "print pipe\n",
            "\n",
            "for i, ratio, C, gamma in zip(range(7), ratio, C, gamma):\n",
            "\n",
            "    pipe.set_params(smt__ratio = ratio, clf__C = C, clf__gamma = gamma)\n",
            "    probas_ = pipe.fit(X_train_scaled, y_train).predict_proba(X_holdout_scaled)\n",
            "    fpr[i], tpr[i], _ = roc_curve(y_holdout, probas_[:,1])\n",
            "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
            "    \n",
            "\n",
            "# plotting the ROC curves\n",
            "\n",
            "def plot_roc(fpr, tpr, roc_auc):\n",
            "    colors = ['darkorange', 'deeppink', 'red', 'aqua', 'cornflowerblue','navy', 'blue']\n",
            "\n",
            "    plt.figure(figsize=(10,8.5))\n",
            "    for i, color in zip(range(7), colors):\n",
            "        plt.plot(fpr[i], tpr[i], color=color, lw=2, linestyle=':',\n",
            "                 label='{0} (area = {1:0.2f})'\n",
            "                 ''.format((i+1)/10, roc_auc[i]))\n",
            "\n",
            "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
            "    plt.xlim([0.0, 1.0])\n",
            "    plt.ylim([0.0, 1.05])\n",
            "    plt.xlabel('False Positive Rate')\n",
            "    plt.ylabel('True Positive Rate')\n",
            "    plt.title('ROC curves: SMOTE oversampled minority class', fontsize=14)\n",
            "    plt.legend(title='Class ratio after oversampling', loc=\"lower right\")\n",
            "    plt.show()\n",
            "    plt.savefig('ROC_oversampling.png')\n",
            "\n",
            "\n",
            "plot_roc(fpr, tpr, roc_auc)\n",
            "\n",
            "\"\"\"\n",
            "Explanation: For these manually selected parameters, the TNR is 0.38, the Matthews correlation coefficient is 0.21 and the precision/recall/F1 is in the 0.86 - 0.90 range. Selecting the best CV score (usually in the 0.90 range), on the other hand, would have given a TNR of 0 for all the scoring metrics I looked at.\n",
            "<h4>The Pipeline -- Oversampling, classification and ROC computations </h4>\n",
            "\n",
            "The imblearn package includes a pipeline module which allows one to chain transformers, resamplers and estimators. We compute the ROC curves for each of the oversampling ratios and corresponding hyperparameters C and gamma and for this we use the pipeline to oversample with SMOTE and classify with the SVM.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing"
      ],
      "metadata": {
        "id": "Tvz1_P-lMWf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = [split for text in train['content'] for split in text.split('\\n') if split != \"\"]\n",
        "test_split = [split for text in test['content'] for split in text.split('\\n') if split != \"\"]"
      ],
      "metadata": {
        "id": "TMObTZrf9Gf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the number of sentences\n",
        "len(train_split), len(test_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz6Me8pU9LlV",
        "outputId": "1d5469bd-5882-4154-fc4e-03468fd6d964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11367363, 2875424)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "def preprocessing(sentence):\n",
        "    \n",
        "    # removing the html tags and non-English words\n",
        "    sentence = re.sub(\"<[^>]*>\", \"\", sentence) \n",
        "    sentence = re.sub(\"[^\\x00-\\x7F]+\", \"\", sentence) \n",
        "    stopwords = list(STOP_WORDS)\n",
        "    doc = nlp(sentence)\n",
        "    cleaned_tokens = []\n",
        "    \n",
        "    # removing extra spaces or duplicate symbols.......\n",
        "    for token in doc: \n",
        "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
        "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
        "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
        "                \n",
        "    return \" \".join(cleaned_tokens)"
      ],
      "metadata": {
        "id": "AMIgPyCU_HkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing"
      ],
      "metadata": {
        "id": "1IpdyFtRMYqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter: \n",
        "        text = preprocessing(text)\n",
        "        yield tokenizer(text)\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "train_size = 100000\n",
        "tokenized_dataset_train = yield_tokens(train_split[:train_size])\n",
        "tokenized_dataset_test = yield_tokens(test_split[:20000])"
      ],
      "metadata": {
        "id": "lG5vasRiNdOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset_train"
      ],
      "metadata": {
        "id": "Ea1R8WRM91O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numericalizing"
      ],
      "metadata": {
        "id": "s1AvnpIBMt9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numericalization\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_split[:train_size]), min_freq=3)\n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 1)            \n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print(len(vocab))                         \n",
        "print(vocab.get_itos()[:10])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpNFUqocMhKU",
        "outputId": "e1564920-d1c3-4f66-b82e-657be380a7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11256\n",
            "['<unk>', '<eos>', '#', 'explanation', '=', 'end', \"'\", 'import', 'property', 'value']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preparing the BatchLoader"
      ],
      "metadata": {
        "id": "2WHNAtdXNkZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data"
      ],
      "metadata": {
        "id": "ik0Px6paNryv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:       \n",
        "        # appending eos so we know it ends....so model learn how to end...                             \n",
        "        tokens = example.append('<eos>') # end of sentence\n",
        "        #numericalization     \n",
        "        tokens = [vocab[token] for token in example] \n",
        "        data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size # getting the int number of batches...\n",
        "    data = data[:num_batches * batch_size]    # making the batch evenly, and cut out any remaining                     \n",
        "    data = data.view(batch_size, num_batches)        \n",
        "    return data # [batch size, bunch of tokens]"
      ],
      "metadata": {
        "id": "3DCverR-lx7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_data = get_data(tokenized_dataset_train, vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset_test, vocab, batch_size)\n",
        "# test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
      ],
      "metadata": {
        "id": "l8Hse5pONvS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# saving train and valid data\n",
        "object_data = train_data\n",
        "file_train = open('train_data.pkl', 'wb') \n",
        "pickle.dump(object_data, file_train)\n",
        "\n",
        "object_data = valid_data\n",
        "file_val = open('valid_data.pkl', 'wb') \n",
        "pickle.dump(object_data, file_val)\n",
        "\n",
        "# saving vocab\n",
        "object_data = vocab\n",
        "file_vocab = open('vocab.pkl', 'wb') \n",
        "pickle.dump(object_data, file_vocab)"
      ],
      "metadata": {
        "id": "ENLZjolOOU6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modeling"
      ],
      "metadata": {
        "id": "p0RQo5QBOBWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "                \n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hid_dim = hid_dim\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
        "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
        "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        return hidden, cell\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell\n",
        "\n",
        "        \n",
        "    def forward(self, src, hidden):\n",
        "        # src: [batch size, seq len]\n",
        "        embedding = self.dropout(self.embedding(src))\n",
        "        # embedding: [batch size, seq len, emb_dim]\n",
        "        output, hidden = self.lstm(embedding, hidden)      \n",
        "        # output: [batch size, seq len, hid_dim]\n",
        "        # hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
        "        output = self.dropout(output) \n",
        "        prediction = self.fc(output)\n",
        "        # prediction: [batch size, seq_len, vocab size]\n",
        "        return prediction, hidden"
      ],
      "metadata": {
        "id": "SNX9cr8KODjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training"
      ],
      "metadata": {
        "id": "V9jtPv_aOkdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "emb_dim = 1024                # 400 in the paper\n",
        "hid_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                # 3 in the paper\n",
        "dropout_rate = 0.65              \n",
        "lr = 1e-3             "
      ],
      "metadata": {
        "id": "L9G0o9lROEH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ],
      "metadata": {
        "id": "o0EKjDjROujO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901cf5af-7717-43f4-9410-097821c8554b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 39,857,144 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    # data # [batch size, bunch of tokens]\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  # target simply is ahead of src by 1            \n",
        "    return src, target"
      ],
      "metadata": {
        "id": "6Gj2ri6pOuZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    # data # [batch size, bunch of tokens]\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]  # we need to -1 because we start at 0\n",
        "    num_batches = data.shape[-1]\n",
        "    \n",
        "    # reseting the hidden every epoch\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # hidden does not need to be in the computational graph for efficiency\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) # src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)               \n",
        "\n",
        "        # need to reshape because criterion expects pred to be 2d and target to be 1d\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  # prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ],
      "metadata": {
        "id": "QZ1hGVuLOuQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ],
      "metadata": {
        "id": "4WcJkcS3OuKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "seq_len  = 50 # <----decoding length\n",
        "clip    = 0.25\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion, \n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ],
      "metadata": {
        "id": "vgOUPuMHPTqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4d93f7-cbad-4cf9-e32e-b5d887e66a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 456.970\n",
            "\tValid Perplexity: 165.043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 198.653\n",
            "\tValid Perplexity: 112.127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 134.969\n",
            "\tValid Perplexity: 96.082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 109.710\n",
            "\tValid Perplexity: 87.714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 96.292\n",
            "\tValid Perplexity: 83.172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 86.227\n",
            "\tValid Perplexity: 80.299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 76.877\n",
            "\tValid Perplexity: 76.779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 68.712\n",
            "\tValid Perplexity: 75.400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 62.000\n",
            "\tValid Perplexity: 71.913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 56.064\n",
            "\tValid Perplexity: 69.938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 51.153\n",
            "\tValid Perplexity: 68.752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 46.286\n",
            "\tValid Perplexity: 68.851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 42.117\n",
            "\tValid Perplexity: 66.972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 39.090\n",
            "\tValid Perplexity: 65.684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 37.000\n",
            "\tValid Perplexity: 64.615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 35.094\n",
            "\tValid Perplexity: 64.174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 33.241\n",
            "\tValid Perplexity: 63.934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 31.547\n",
            "\tValid Perplexity: 63.344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 29.968\n",
            "\tValid Perplexity: 63.059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 28.463\n",
            "\tValid Perplexity: 62.661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 27.088\n",
            "\tValid Perplexity: 63.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 26.010\n",
            "\tValid Perplexity: 62.735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.651\n",
            "\tValid Perplexity: 62.071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.078\n",
            "\tValid Perplexity: 61.825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 24.657\n",
            "\tValid Perplexity: 61.899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 24.941\n",
            "\tValid Perplexity: 61.018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 24.846\n",
            "\tValid Perplexity: 61.015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.024\n",
            "\tValid Perplexity: 60.914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 24.953\n",
            "\tValid Perplexity: 60.839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 24.780\n",
            "\tValid Perplexity: 60.892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.046\n",
            "\tValid Perplexity: 60.816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.088\n",
            "\tValid Perplexity: 60.849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.160\n",
            "\tValid Perplexity: 60.878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.195\n",
            "\tValid Perplexity: 60.880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.265\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.269\n",
            "\tValid Perplexity: 60.880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.279\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.316\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.252\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.263\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.220\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.180\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.277\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.308\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.355\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.265\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.151\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.283\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.254\n",
            "\tValid Perplexity: 60.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.266\n",
            "\tValid Perplexity: 60.881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Real-world inference"
      ],
      "metadata": {
        "id": "juzLHplaYj4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            \n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "            \n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Ued57WHHYgwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'import numpy'\n",
        "max_seq_len = 30\n",
        "seed = 0\n",
        "\n",
        "#smaller the temperature, more diverse tokens but comes \n",
        "#with a tradeoff of less-make-sense sentence\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
        "                          vocab, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ],
      "metadata": {
        "id": "QeKRwjJFYuXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b08655-f3b8-49e4-91e9-4ce4e7204049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "import numpy np\n",
            "\n",
            "0.7\n",
            "import numpy np\n",
            "\n",
            "0.75\n",
            "import numpy np\n",
            "\n",
            "0.8\n",
            "import numpy np\n",
            "\n",
            "1.0\n",
            "import numpy np\n",
            "\n"
          ]
        }
      ]
    }
  ]
}