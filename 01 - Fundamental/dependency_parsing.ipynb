{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x1tnXPG-TLE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm  #gimmick for progressbar when you train\n",
        "import pickle # saving and loading models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing Function"
      ],
      "metadata": {
        "id": "zNobZ5iQP23I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['He', 'has', 'good', 'control', '.']\n",
        "something = sentence.pop(0)"
      ],
      "metadata": {
        "id": "n2BNBDsh-rx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "something"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "72WVv25C-w9O",
        "outputId": "61fdb180-5c60-47f0-ffc4-4f49022ef386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOomWBJB-0BB",
        "outputId": "707048eb-6447-4288-87d8-a15c79bc5ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['has', 'good', 'control', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basically, it takes the current state of the buffer, stack, dependencies\n",
        "# tells us how SHIFT, LA, RA changes these three objects\n",
        "\n",
        "class Parsing(object):\n",
        "    \n",
        "    # init stack, buffer, dep\n",
        "    def __init__(self, sentence):  \n",
        "        self.sentence = sentence      # ['The', 'cat', 'sat]  #conll format which is already in the tokenized form\n",
        "        self.stack    = ['ROOT']\n",
        "        self.buffer   = sentence[:]  # in the beginning, everything is inside the buffer\n",
        "        self.dep      = []           # maintains a list of tuples of dep\n",
        "    \n",
        "    # parse function that tells me how shift, la, ra changes these three objects\n",
        "    def parse_step(self, transition):     # transition could be either S, LA, RA\n",
        "        if transition == 'S':\n",
        "            # getting the top guy in the buffer and put in stack\n",
        "            head = self.buffer.pop(0)\n",
        "            self.stack.append(head)\n",
        "        elif transition == 'LA':  # stack = [ROOT, He, has] ==> append to dep (has, he) and then He is gone from the stack [ROOT, has]\n",
        "            dependent = self.stack.pop(-2)  # He\n",
        "            self.dep.append((self.stack[-1], dependent))  # (has, he)\n",
        "        elif transition == 'RA':\n",
        "            dependent = self.stack.pop()  # stack = [ROOT, has, control] ==> dep (has, control), control will be gone fromt he stack [ROOT, has]\n",
        "            self.dep.append((self.stack[-1], dependent))\n",
        "        else:\n",
        "            print(f\"Bad transition: {transition}\")\n",
        "    \n",
        "    # given some series of transition, it gonna for-loop the parse function\n",
        "    def parse(self, transitions):\n",
        "        for t in transitions:\n",
        "            self.parse_step(t)\n",
        "        return self.dep\n",
        "    \n",
        "    # checking whether things are finished - no need to do anymore functions....\n",
        "    def is_completed(self):\n",
        "        return (len(self.buffer) == 0) and (len(self.stack) == 1)  # so buffer is empty and ROOT is the only guy in stack"
      ],
      "metadata": {
        "id": "94D3aQ6q-2Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parse Step"
      ],
      "metadata": {
        "id": "VohyeO_AQ-Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a instance of Parsing\n",
        "parsing = Parsing(['He', 'has', 'good', 'control', '.'])"
      ],
      "metadata": {
        "id": "ULSkk9G0BOrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRRebgdPRITY",
        "outputId": "af933fb8-33c0-4b39-a725-4422dc43cbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT'], ['He', 'has', 'good', 'control', '.'], [])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trying to do a shift\n",
        "parsing.parse_step(\"S\")\n",
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riYqxx8PRIEm",
        "outputId": "d9205765-6f46-4bac-85cd-704f7cbe1d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT', 'He'], ['has', 'good', 'control', '.'], [])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doing shift, and then left arc\n",
        "parsing.parse_step(\"S\")\n",
        "parsing.parse_step(\"LA\")\n",
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dERzS4hUyES",
        "outputId": "44befe06-6aa3-4a00-f617-b968808b3315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT', 'has'], ['good', 'control', '.'], [('has', 'He')])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parse"
      ],
      "metadata": {
        "id": "vOdIMqhbV4e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsing = Parsing(['He', 'has', 'good', 'control', '.'])\n",
        "\n",
        "# using the parse function, to tell it to do S, S, L, S, S, L, R\n",
        "# double checking whether we have three dep (has, he), (control, good), (has, control)\n",
        "\n",
        "parsing = Parsing([\"He\", \"has\", \"good\", \"control\",\".\"])\n",
        "parsing.parse([\"S\",\"S\", \"LA\", \"S\", \"S\", \"LA\", \"RA\"])\n",
        "parsing.stack, parsing.buffer, parsing.dep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjOvvrHqUx4g",
        "outputId": "01d7a8d9-92f5-4c1e-d470-1823a8ef597e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ROOT', 'has'],\n",
              " ['.'],\n",
              " [('has', 'He'), ('control', 'good'), ('has', 'control')])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minibatch Parsing"
      ],
      "metadata": {
        "id": "4AtGFN4JWpx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    dep = []  # all the resulting dep\n",
        "    \n",
        "    # init Parsing instance for each sentence in the batch\n",
        "    partial_parses = [Parsing(sentence) for sentence in sentences]  # in tokenized form\n",
        "    # Parsing(['The', 'cat', 'sat']), Parsing(['Chaky', 'is', 'mad'])\n",
        "    \n",
        "    unfinished_parses = partial_parses[:]\n",
        "    \n",
        "    # while we still have sentence\n",
        "    while unfinished_parses:  # if there are still a Parsing object\n",
        "    \n",
        "        # taking a certain batch of sentence\n",
        "        minibatch = unfinished_parses[:batch_size] # number of Parsing object\n",
        "        \n",
        "        # creating a dummy model to tell us what's the next transition for each sentence\n",
        "        transitions = model.predict(minibatch) \n",
        "        # transitions = [S, S, .....]\n",
        "        # minibatch   = [Parsing(sentence1), Parsing(sentence2)]\n",
        "        \n",
        "                \n",
        "        # for transition predicted this dummy model\n",
        "        for transition, partial_parse in zip(transitions, minibatch):\n",
        "            # parse step\n",
        "            # transition: S\n",
        "            # partial_parse: Parsing(sentence)\n",
        "            partial_parse.parse_step(transition)\n",
        "            \n",
        "        # removing any sentence is finish\n",
        "        unfinished_parses[:] = [p for p in unfinished_parses if not p.is_completed()]\n",
        "    \n",
        "    dep = [parse.dep for parse in partial_parses]\n",
        "    \n",
        "    return dep"
      ],
      "metadata": {
        "id": "wrXMHYQNU6SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyModel(object):\n",
        "    def predict(self, partial_parses):\n",
        "        # partial_parses: list of Parsing instances\n",
        "        # first shifting everything onto the stack, and then just doing RA if the first word\n",
        "        # of the sentence is \"right\", otherwise, is \"left\"\n",
        "        return [(\"RA\" if pp.stack[1] == \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]"
      ],
      "metadata": {
        "id": "wDPCTWLZU6OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "             [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "             [\"left\", \"arcs\", \"only\"],\n",
        "             [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "\n",
        "minibatch_parse(sentences, DummyModel(), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQqLKxyrW_9r",
        "outputId": "0e15771f-8923-4ccf-af60-2f48887ec23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
              " [('only', 'again'), ('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
              " [('only', 'arcs'), ('only', 'left'), ('only', 'ROOT')],\n",
              " [('again', 'only'), ('again', 'arcs'), ('again', 'left'), ('again', 'ROOT')]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "_LrJpeRKXa3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conll(filename):\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    with open(filename) as f:\n",
        "        i = 0\n",
        "        word, pos, head, dep = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            i = i+1\n",
        "            wa = line.strip().split('\\t')  # ['1', 'In', '_', 'ADP', 'IN', '_', '5', 'case', '_', '_']\n",
        "            # In <--------  5th guy  # case\n",
        "            \n",
        "            if len(wa) == 10:  # if all the columns are there\n",
        "                word.append(wa[1].lower())\n",
        "                pos.append(wa[4])\n",
        "                head.append(int(wa[6]))\n",
        "                dep.append(wa[7])\n",
        "            \n",
        "            # the row is not exactly 10, it means new sentence\n",
        "            elif len(word) > 0:  # if there is somethign inside the word\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
        "                word, pos, head, dep = [], [], [], [] # clear word, pos, head, dep\n",
        "        \n",
        "        if len(word) > 0:  # if there is somethign inside the word\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  # in the sentence level\n",
        "\n",
        "    return examples  "
      ],
      "metadata": {
        "id": "Q0iFWeTFW_0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    print(\"1. Loading data\")\n",
        "    train_set = read_conll(\"/content/train.conll\")\n",
        "    dev_set   = read_conll(\"/content/dev.conll\")\n",
        "    test_set   = read_conll(\"/content/test.conll\")\n",
        "    \n",
        "    # making my dataset smaller because my mac cannot handle it\n",
        "    train_set = train_set[:1000]\n",
        "    dev_set   = dev_set[:500]\n",
        "    test_set  = test_set[:500]\n",
        "    \n",
        "    return train_set, dev_set, test_set"
      ],
      "metadata": {
        "id": "Wvpepi3tI71l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Load Function"
      ],
      "metadata": {
        "id": "lhwNVhjXZCAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, dev_set, test_set = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Ipa75cPeGi",
        "outputId": "e8341669-25d3-49fc-dd06-b91aa98de002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set), len(dev_set), len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwqXoq71PgV7",
        "outputId": "06af7f73-7787-4fa5-e5da-004631b0c78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we eventually gonna make the dependency\n",
        "# so maybe we can cheat a little bit, and see the answer\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy #displacy is for visualization\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Ms. Haag plays Elianti .\")\n",
        "options = {\"collapse_punct\": False}\n",
        "\n",
        "displacy.render(doc, options = options, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "g9oOQlK_VUD-",
        "outputId": "f71f14d8-04de-46c6-b48e-f2147577b7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"22e1435819404ee0a9030199b278486e-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Ms.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Haag</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">plays</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Elianti</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PUNCT</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-22e1435819404ee0a9030199b278486e-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-22e1435819404ee0a9030199b278486e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-22e1435819404ee0a9030199b278486e-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-22e1435819404ee0a9030199b278486e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-22e1435819404ee0a9030199b278486e-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-22e1435819404ee0a9030199b278486e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-22e1435819404ee0a9030199b278486e-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-22e1435819404ee0a9030199b278486e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parser"
      ],
      "metadata": {
        "id": "_ThTL4R1bCcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_PREFIX = '<p>:' # indicating pos tags\n",
        "D_PREFIX = '<d>:' # indicating dependency tags\n",
        "UNK      = '<UNK>'\n",
        "NULL     = '<NULL>'\n",
        "ROOT     = '<ROOT>'\n",
        "\n",
        "class Parser(object):\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        \n",
        "        # setting the root dep\n",
        "        self.root_dep = 'root'\n",
        "                \n",
        "        # getting all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
        "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
        "                                               for w in ex['dep']\n",
        "                                               if w != self.root_dep]))\n",
        "        \n",
        "        # 1. putting dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
        "        # {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
        "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
        "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
        "        \n",
        "        # we are using \"unlabeled\" where we do not label with the dependency\n",
        "        # thus the number of dependency relation is 1\n",
        "        trans = ['L', 'R', 'S']\n",
        "        self.n_deprel = 1   # because we are not predicting the relations, we are only predicting S, L, R\n",
        "        \n",
        "        # creating a simple lookup table mapping action and id\n",
        "        # e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
        "        # e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
        "        self.n_trans = len(trans)\n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}  # using for easy coding\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "               \n",
        "        # 2. putting pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  # also remember the pos tags of unknown\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "        \n",
        "        # now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
        "        \n",
        "        # 3. putting word into tok2id lookup table\n",
        "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
        "        tok2id[NULL] = self.NULL = len(tok2id)\n",
        "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "        \n",
        "        # now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
        "        \n",
        "        # creating id2tok\n",
        "        self.tok2id = tok2id\n",
        "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "        \n",
        "        self.n_features = 18 + 18 + 12\n",
        "        self.n_tokens = len(tok2id)\n",
        "\n",
        "                \n",
        "    # utility function, in case we want to convert token to id\n",
        "    # function to turn train set with words to train set with id instead using tok2id\n",
        "    def numericalize(self, examples):\n",
        "        numer_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "            pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "            dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
        "                            else -1 for w in ex['dep']]\n",
        "            numer_examples.append({'word': word, 'pos': pos,\n",
        "                                 'head': head, 'dep': dep})\n",
        "        return numer_examples\n",
        "            \n",
        "    # function to extract features to form a feature embedding matrix\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "             \n",
        "        # ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
        "        # ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
        "        # ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
        "        # ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
        "\n",
        "        # stack     :  [0]\n",
        "        # buffer    :  [1, 2, 3, 4, 5]\n",
        "        \n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0  # starting the stack with [ROOT]\n",
        "            \n",
        "        p_features = [] # pos features (2a, 2b, 2c) - 18\n",
        "        d_features = [] # dep features (3b, 3c) - 12\n",
        "        \n",
        "        # last 3 things on the stack as features\n",
        "        # if the stack is less than 3, then we simply append NULL from the left\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        \n",
        "        # next 3 things on the buffer as features\n",
        "        # if the buffer is less than 3, simply appending NULL\n",
        "        # the reason why NULL is appended on end because buffer is read left to right\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        \n",
        "        # corresponding pos tags\n",
        "        p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "        p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "             \n",
        "        # getting leftmost children based on the dependency arcs\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        # getting right most children based on the dependency arcs\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "        \n",
        "        # getting the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
        "        for i in range(2):\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1] # -1, -2 last two in the stack\n",
        "                \n",
        "                # the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
        "                lc = get_lc(k)  \n",
        "                rc = get_rc(k)\n",
        "                \n",
        "                # the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "\n",
        "                # leftmost of first word on stack, rightmost of first word, \n",
        "                # leftmost of the second word on stack, rightmost of second, \n",
        "                # leftmost of leftmost, rightmost of rightmost\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                # corresponding pos\n",
        "                p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "\n",
        "                        \n",
        "                # corresponding dep\n",
        "                d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
        "                d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
        "                \n",
        "            else:\n",
        "                # attaching NULL when they don't exist\n",
        "                features += [self.NULL] * 6\n",
        "                p_features += [self.P_NULL] * 6\n",
        "                d_features += [self.D_NULL] * 6\n",
        "                \n",
        "        features += p_features + d_features\n",
        "        assert len(features) == self.n_features  # asserting they are 18 + 18 + 12\n",
        "        \n",
        "        return features\n",
        "      \n",
        "    # generating training examples\n",
        "    # from the training sentences and their gold parse trees \n",
        "    def create_instances(self, examples):  # examples = word, pos, head, dep\n",
        "        all_instances = []\n",
        "        \n",
        "        for i, ex in enumerate(examples):\n",
        "            # Ms. Haag plays Elianti .\n",
        "            # e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
        "            # here 344 stands for ROOT\n",
        "            # Chaky - I cheated and take a look\n",
        "            n_words = len(ex['word']) - 1  #excluding the root\n",
        "            \n",
        "            # arcs = {(head, tail, dependency label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]  # [1, 2, 3, 4, 5]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            \n",
        "            # because that's the maximum number of shift, leftarcs, rightarcs we can have\n",
        "            # this will determine the sample size of each training example\n",
        "            # if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
        "            # but this for loop can be break if there is nothing left....\n",
        "            for i in range(n_words * 2):  # maximum times we can do either S, L, R\n",
        "                \n",
        "                # getting the gold transition based on the parse trees\n",
        "                # gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                \n",
        "                # if gold_t is None, no need to extract features.....\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                \n",
        "                # making sure when the model predicts, we inform the current state of stack and buffer, so\n",
        "                # the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
        "                legal_labels = self.legal_labels(stack, buf)                \n",
        "                assert legal_labels[gold_t] == 1\n",
        "                \n",
        "                # extracting all the 48 features \n",
        "                features = self.extract_features(stack, buf, arcs, ex)\n",
        "                instances.append((features, legal_labels, gold_t))\n",
        "                \n",
        "                # shift \n",
        "                if gold_t == 2:\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                # left arc \n",
        "                elif gold_t == 0:\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                # right arc\n",
        "                else:\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "                    \n",
        "            else:\n",
        "                all_instances += instances\n",
        "\n",
        "        return all_instances\n",
        "\n",
        "    # providing an one hot encoding of the labels\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel  # left arc but you cannot do ROOT <--- He\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel  # right arc because ROOT --> He\n",
        "        labels += [1] if len(buf) > 0 else [0]  #shift\n",
        "        return labels\n",
        "    \n",
        "    # a simple function to check punctuation POS tags\n",
        "    def punct(self, pos):\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    \n",
        "    # deciding whether to shift, leftarc, or rightarc, based on gold parse trees\n",
        "    # this is needed to create training examples which contain samples and ground truth\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        \n",
        "        # leaving if the stack is only 1, thus nothing to predict....\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "        \n",
        "        # predicting based on the last two words on the stack\n",
        "        # stack: [ROOT, he, has]\n",
        "        i0 = stack[-1] # has\n",
        "        i1 = stack[-2] # he\n",
        "        \n",
        "        # getting the head and dependency\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "        d0 = ex['dep'][i0]\n",
        "        d1 = ex['dep'][i1]\n",
        "        \n",
        "        # either shift, left arc or right arc\n",
        "        # \"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
        "        # if head of the second last word is the last word, then leftarc\n",
        "        if (i1 > 0) and (h1 == i0):\n",
        "            return 0  # action is left arc ---> gold_t\n",
        "        # if head of the last word is the second last word, then rightarc\n",
        "        # making sure nothing in the buffer has head with the last word on the stack\n",
        "        # otherwise, we lose the last word.....\n",
        "        elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "            return 1  # right arc\n",
        "        # otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
        "        else:\n",
        "            return None if len(buf) == 0 else 2  # shift         \n",
        "        \n",
        "        \n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        \n",
        "        for i, example in enumerate(dataset):\n",
        "            \n",
        "            # example['word']=[188, 186, 186, ..., 59]\n",
        "            # n_words=37\n",
        "            # sentence=[1, 2, 3, 4, 5,.., 37]\n",
        "            \n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]            \n",
        "            sentences.append(sentence)\n",
        "            \n",
        "            # mapping the object unique id to the i            \n",
        "            # The id is the object's memory address\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "            \n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "        \n",
        "        UAS = all_tokens = 0.0\n",
        "        with tqdm(total=len(dataset)) as prog:\n",
        "            for i, ex in enumerate(dataset):\n",
        "                head = [-1] * len(ex['word'])\n",
        "                for h, t, in dependencies[i]:\n",
        "                    head[t] = h\n",
        "                for pred_h, gold_h, gold_l, pos in \\\n",
        "                        zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
        "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                        if (not self.punct(pos_str)):\n",
        "                            UAS += 1 if pred_h == gold_h else 0\n",
        "                            all_tokens += 1\n",
        "                prog.update(i + 1)\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies"
      ],
      "metadata": {
        "id": "7kgABE4UVVIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        mb_x = torch.from_numpy(mb_x).long()\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "\n",
        "        pred = self.parser.model(mb_x)\n",
        "        pred = pred.detach().numpy()\n",
        "        \n",
        "        # we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
        "        # other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        \n",
        "        return pred"
      ],
      "metadata": {
        "id": "mv_IbBmdq7b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple function to create ids.....\n",
        "def build_dict(keys, offset=0):\n",
        "    # keys = ['P_PREFIX:IN', 'P_PREFIX:DT', 'P_PREFIX:NNP', 'P_PREFIX:CD', so on...]\n",
        "    # offset is needed because this tok2id has something already inside....\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    \n",
        "    # most_common = [('P_PREFIX:NN', 70), ('P_PREFIX:IN', 57), ... , ('P_PREFIX:JJR', 1)]\n",
        "    # we use most_common in case we only want some maximum pos tags....\n",
        "    mc = count.most_common()\n",
        "    \n",
        "    # {'P_PREFIX:NN': 31, 'P_PREFIX:IN': 32, .., 'P_PREFIX:JJR': 62} \n",
        "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
      ],
      "metadata": {
        "id": "sslH0ztMrA5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Parser"
      ],
      "metadata": {
        "id": "Ob1S1RrLddYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2. Building parser\")\n",
        "start = time.time()\n",
        "parser = Parser(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7fFOx40rFs6",
        "outputId": "f39ba77d-3cb0-482c-c6a3-490b92297665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Building parser\n",
            "took 0.04 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# before numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyfSLed31CeT",
        "outputId": "aa01aad9-bde2-4194-8006-f5e983777443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
            "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
            "Head:  [2, 3, 0, 3, 3]\n",
            "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = parser.numericalize(train_set)\n",
        "dev_set = parser.numericalize(dev_set)\n",
        "test_set = parser.numericalize(test_set)"
      ],
      "metadata": {
        "id": "1JjdQPMn49jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after numericalization\n",
        "print(\"Word: \", train_set[1][\"word\"])\n",
        "print(\"Pos: \",  train_set[1][\"pos\"])\n",
        "print(\"Head: \", train_set[1][\"head\"])\n",
        "print(\"Dep: \",  train_set[1][\"dep\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj48GALk5CbD",
        "outputId": "9a1a74cf-a416-4471-9548-4fc8aa9dd18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
            "Pos:  [84, 42, 42, 55, 42, 46]\n",
            "Head:  [-1, 2, 3, 0, 3, 3]\n",
            "Dep:  [-1, 1, 29, 0, 22, 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding"
      ],
      "metadata": {
        "id": "b9FCaWA4dviw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4. Loading pretrained embeddings...\",)\n",
        "start = time.time()\n",
        "word_vectors = {}\n",
        "for line in open(\"/content/en-cw.txt\").readlines():\n",
        "    we = line.strip().split() # we = word embeddings - first column: word;  the rest is embedding\n",
        "    word_vectors[we[0]] = [float(x) for x in we[1:]] # {word: [list of 50 numbers], nextword: [another list], so on...}\n",
        "    \n",
        "# creating an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
        "# we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
        "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
        "\n",
        "for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token]\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
        "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHwrDBoY1LtK",
        "outputId": "4379d478-ec47-4c5a-aaf9-61d6183e83b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. Loading pretrained embeddings...\n",
            "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
            "took 0.52 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "WROj1Q4mebjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5. Preprocessing training data...\",)\n",
        "start = time.time()\n",
        "train_examples = parser.create_instances(train_set)\n",
        "print(\"took {:.2f} seconds\".format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd7xfseQ1MJB",
        "outputId": "cacb9fa8-5110-4318-8854-7724b9b27ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5. Preprocessing training data...\n",
            "took 1.57 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ThsK97ekhR",
        "outputId": "11b73879-9e4c-4a68-a1bc-9f32c3fc9576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([5155,\n",
              "  5155,\n",
              "  5156,\n",
              "  91,\n",
              "  113,\n",
              "  806,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  5155,\n",
              "  83,\n",
              "  83,\n",
              "  84,\n",
              "  40,\n",
              "  41,\n",
              "  42,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  83,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38,\n",
              "  38],\n",
              " [0, 0, 1],\n",
              " 2)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minibatch Loader"
      ],
      "metadata": {
        "id": "sY5PmcZWepzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    data_size = len(data[0])\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)"
      ],
      "metadata": {
        "id": "nudGhfoy3GhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing your Minibatch Loader"
      ],
      "metadata": {
        "id": "YOQ6br-5ewzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, (train_x, train_y) in enumerate(minibatches(train_examples, 1024)):\n",
        "#     print(train_x.shape)  #batch size, features\n",
        "#     print(train_y.shape)        #one hot encoding of 3 actions - shift, la, ra"
      ],
      "metadata": {
        "id": "4-s-6mbF3Gsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "4WP41fdKe8dB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParserModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, n_features=48,\n",
        "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
        "\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        # t:  batch_size, n_features\n",
        "        batch_size = t.size()[0]\n",
        "                    \n",
        "        x = self.pretrained_embeddings(t)        \n",
        "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
        "        # x = (1024, 48 * 50)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t: (1024, 48)\n",
        "        embeddings = self.embedding_lookup(t)  \n",
        "    \n",
        "        # embeddings: (1024, 48 * 50)\n",
        "        hidden = self.embed_to_hidden(embeddings)\n",
        "    \n",
        "        # hidden: (1024, 200)\n",
        "        hidden_activations = F.relu(hidden)\n",
        "        # hidden_activations: (1024, 200)\n",
        "        thin_net = self.dropout(hidden_activations)\n",
        "        # thin_net: (1024, 200)\n",
        "        logits = self.hidden_to_logits(thin_net)\n",
        "        # logits: (1024, 3)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "-9BQqgbj5xDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just a class to get the average.....\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "pRr0n-Ti9ntq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \n",
        "    best_dev_UAS = 0\n",
        "    \n",
        "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(\n",
        "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \n",
        "    parser.model.train()  # places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            \n",
        "            # train_x:  batch_size, n_features\n",
        "            # train_y:  batch_size, target(=3)\n",
        "            \n",
        "            optimizer.zero_grad() \n",
        "            loss = 0.\n",
        "            train_x = torch.from_numpy(train_x).long()  # long() for int so embedding works....\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  # getting the index with 1 because torch expects label to be single integer\n",
        "\n",
        "            # forward pass: computing predicted logits.\n",
        "            logits = parser.model(train_x)\n",
        "            # computing loss\n",
        "            loss = loss_func(logits, train_y)\n",
        "            # computing gradients of the loss w.r.t model parameters.\n",
        "            loss.backward()\n",
        "            # taking step with optimizer.\n",
        "            optimizer.step()\n",
        "\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    parser.model.eval()  # places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "        \n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS"
      ],
      "metadata": {
        "id": "uz2vHZxR9pGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Slt1OdUYgAB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating directory if it does not exist for saving the weights...\n",
        "output_dir = \"output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "output_path = output_dir + \"model.weights\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    \n",
        "print(80 * \"=\")\n",
        "print(\"TRAINING\")\n",
        "print(80 * \"=\")\n",
        "    \n",
        "model = ParserModel(embeddings_matrix)\n",
        "parser.model = model\n",
        "\n",
        "start = time.time()\n",
        "train(parser, train_examples, dev_set, output_path,\n",
        "      batch_size=1024, n_epochs=10, lr=0.0005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-3RdF5U91cs",
        "outputId": "4222e1c5-8b8a-4152-88bc-6a6e8ad7d13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.5954099521040916\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7096649.50it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 51.67\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.3325179858754079\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7334644.48it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 60.22\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.26925493994106847\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 8150439.47it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 63.79\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.23330723649511734\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7228374.53it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 65.91\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.20574142194042602\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 6100761.54it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 69.32\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 6 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:07<00:00,  6.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1865379111841321\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7756335.10it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 70.40\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1702693896368146\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7473951.49it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 70.65\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1507034107732276\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 6641423.21it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 72.88\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 9 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.1377916500593225\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 8274971.66it/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 71.95\n",
            "\n",
            "Epoch 10 out of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48/48 [00:06<00:00,  7.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train Loss: 0.12668690411373973\n",
            "Evaluating on dev set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7372627.55it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- dev UAS: 74.71\n",
            "New best dev UAS! Saving model.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(80 * \"=\")\n",
        "print(\"TESTING\")\n",
        "print(80 * \"=\")\n",
        "\n",
        "print(\"Restoring the best model weights found on the dev set\")\n",
        "parser.model.load_state_dict(torch.load(output_path))\n",
        "print(\"Final evaluation on test set\",)\n",
        "parser.model.eval()\n",
        "UAS, dependencies = parser.parse(test_set)\n",
        "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVFbwlu8E9sg",
        "outputId": "86360872-9a55-4ae2-f09e-d4638653311f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTING\n",
            "================================================================================\n",
            "Restoring the best model weights found on the dev set\n",
            "Final evaluation on test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125250it [00:00, 7294011.30it/s]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- test UAS: 76.23\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}